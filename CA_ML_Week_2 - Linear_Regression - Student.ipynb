{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Week 2 - Linear Regression\n",
    "\n",
    "---\n",
    "\n",
    "[Top](#ML-Week-2---Linear-Regression) | [Previous section](#ML-Week-2---Linear-Regression) | [Next section](#Part-0:-Quick-review) | [Bottom](#Thank-you)\n",
    "\n",
    "This notebook has the following sections:\n",
    "\n",
    "* [Part 0: Quick review!](#Part-0:-Quick-review)\n",
    "* [Part 1: Introduction to regression](#Part-1:-Introduction-to-regression)\n",
    "* [Part 2: Solving the linear regression problem](#Part-2:-Solving-the-linear-regression-problem)\n",
    "* [Part 3: Improving the regression: adding more features](#Part-3:-Improving-the-regression:-adding-more-features)\n",
    "* [Part 4: Testing the model and more](#Part-4:-Testing-the-model-and-more)\n",
    "\n",
    "\n",
    "## Part 0: Quick review\n",
    "---\n",
    "\n",
    "[Top](#ML-Week-2---Linear-Regression) | [Previous section](#ML-Week-2---Linear-Regression) | [Next section](#Part-1:-Introduction-to-regression) | [Bottom](#Thank-you)\n",
    "\n",
    "Let's see who took a look at the **median** and **correlation** lessons.\n",
    "\n",
    "Run the following code to load-in a dataset based off of the [capital bike sharing dataset UCI machine](https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset) library. We'll also import the pandas library so we can load the file as a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Import bike sharing data\n",
    "bike_sharing_data = pd.read_csv('data/ml_week_2_bike_sharing.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also import our visualisation libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import in matplotlib and seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Exercise \n",
    "\n",
    "The following code creates a [boxplot](https://towardsdatascience.com/understanding-boxplots-5e2df7bcbd51) of the **temp_degrees_c** column in our dataset. Answer the following questions on a piece of paper, or using Python...\n",
    "\n",
    "1. Approximately, what is the median?\n",
    "2. Approximately, what is the 25th percentile?\n",
    "3. Approximately, what is the 75th percentile?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup a figure\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Create boxplot\n",
    "sns.boxplot(bike_sharing_data['temp_degrees_c'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import numpy as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import numpy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Exercise\n",
    "\n",
    "The following code creates a [**heatmap**](https://seaborn.pydata.org/generated/seaborn.heatmap.html) of the correlation matrix in our dataset. Please answer the following on a sheet of paper, or using Python.\n",
    "\n",
    "1. What features positively correlate?\n",
    "2. What features negatively correlate?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup a figure\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Create correlation matdix\n",
    "corr_matrix = bike_sharing_data.corr()\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "sns.heatmap(corr_matrix, vmax=1.0, vmin=-1.0, linewidths=.5, cmap=cmap, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Introduction to regression\n",
    "\n",
    "---\n",
    "[Top](#ML-Week-2---Linear-Regression) | [Previous section](#Part-0:-Quick-review) | [Next section](#Part-2:-Solving-the-linear-regression-problem) | [Bottom](#Thank-you)\n",
    "\n",
    "Let's take a quick look at our dataset. The dataset has the following columns...\n",
    "\n",
    "| column | description |\n",
    "| :----- | :--- |\n",
    "| dteday | A specific date |\n",
    "| temp_degrees_c | The temperature in degrees celsius |\n",
    "| windspeed | A normalised windspeed for that specific day. <br>The data has been normalised such that the min=0, and max=1 |\n",
    "| cnt | The count of capital bikes used on that day |\n",
    "\n",
    "This data, as mentioned is from the [capital bikeshare system](https://www.capitalbikeshare.com/) in Washington D.C.\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"https://momentummag.com/wp-content/uploads/2016/04/sdf-1.jpg\" width=\"500\">\n",
    "\n",
    "---\n",
    "\n",
    "### Thought exercise\n",
    "\n",
    "Look at the data and do some research on the capital bike system. Try to answer the following questions. Feel free to partner up.\n",
    "\n",
    "1. What is the capital bikeshare system?\n",
    "2. What _business questions_ might we be able to answer from the data?\n",
    "\n",
    "---\n",
    "\n",
    "### What is regression?\n",
    "\n",
    "From [The Hundred-Page Machine Learning Book](http://themlbook.com/):\n",
    "\n",
    "> **Regression** is a problem of predicting a real-valued label (often called a **target**) given an unlabeled example.\n",
    "\n",
    "Put into some of the terminology of the last lesson, a regression problem tries to develop a method to predict a **continuous variable** within our dataset.\n",
    "\n",
    "Today, we're going to build a regression algorithm to predict the **count of bikes** used based upon the weather and windspeed patterns.\n",
    "\n",
    "---\n",
    "\n",
    "### Thought exercise\n",
    "\n",
    "* Why would we do this?? \n",
    "* What would capital bikeshare be able to do with this information??\n",
    "* Have you done regression before? What other problems might it be useful to do regression? \n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Problem setup\n",
    "\n",
    "#### Side-step...linear equations\n",
    "\n",
    "In math, the following function is called a **linear equation**.\n",
    "\n",
    "$$ f(x) = mx + b $$\n",
    "\n",
    "* $x$ is an **input** to the equation\n",
    "* $f(x)$ is an **output** to the equation\n",
    "* $m$ is called the **slope** of a line\n",
    "* $b$ is called the **y-intercept**\n",
    "\n",
    "Let's run the following code, which plots the following linear equation.\n",
    "\n",
    "$$f(x) = 3x + 2$$\n",
    "\n",
    "In this equation:\n",
    "\n",
    "* $m = 3$\n",
    "* $b = 2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a numpy array. Each value represents an input, x\n",
    "x = np.array([-2, -1, 0, 1, 2])\n",
    "\n",
    "# Compute 3 * x + 2\n",
    "f_x = 3*x + 2\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 4))\n",
    "with sns.axes_style(\"whitegrid\"):\n",
    "    sns.lineplot(x=x, y=f_x)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.title('Plot of f(x)=3x + 2')\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Exercise\n",
    "\n",
    "* When we change a value of $x$ by 1, how does the value of $f(x)$ change?\n",
    "* When $x=0$, what is the value of $f(x)$?\n",
    "* How do these values relate to the equation $f(x) = 3x + 2$?\n",
    "---\n",
    "\n",
    "We call $m$ the **slope** and $b$ the **y-intercept**. Hopefully these are familiar terms. Since there is a single input, $x$, we call this a **univariate** (one variable) linear model.\n",
    "\n",
    "### What does this have to do with our dataset?\n",
    "\n",
    "In our problem, we want to help capital bikeshare predict bike usage based upon the weather for a given day. We have many columns in our dataset, but let's pretend we have just the daily temperature, in degrees celsius, and the count of bike users for a specific day. So, in other words, in our dataset we have...\n",
    "\n",
    "* A vector, $x$ of temperatures for each day, in degrees celsius\n",
    "* An output, $y$, of counts that represent the number of bikes used per day\n",
    "\n",
    "To do be able to estimate bike usage, we can make a **linear equation**, of the form $y=mx + b$, that allows us to _predict_ the number of bikes used in a certain day, given a likely daily temperature $x$.\n",
    "\n",
    "#### How do we do this?\n",
    "\n",
    "Let's start with some basics. The following code plots the count of bikes using on a specific day vs. the daily temperature. It does this three different plots (each plot has the same data). It then plots a different line on each of these plots, by varying the values of $m$ and $b$.\n",
    "\n",
    "1. **Left-hand plot**: $m = 350$, $b = 1000$\n",
    "2. **Center plot**: $m = 150$, $b = 2000$\n",
    "3. **Right-hand plot**: $m = 100$, $b = 1000$\n",
    "\n",
    "\n",
    "Which plot develops the best line to fit our data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Develop lines\n",
    "line_1 = 350 * bike_sharing_data['temp_degrees_c'] + 1000 # y = 350*x + 1000\n",
    "line_2 = 150 * bike_sharing_data['temp_degrees_c'] + 2000 # y = 150*x + 2000\n",
    "line_3 = 100 * bike_sharing_data['temp_degrees_c'] + 1000 # y = 100*x + 1000\n",
    "\n",
    "# Function to plot lines\n",
    "def plot_scatter_w_lines(x, y, line, xlabel, ylabel, title, ylim):\n",
    "    sns.regplot(x=x, y=y, fit_reg=None)\n",
    "    sns.regplot(x=x, y=line, marker='')\n",
    "    plt.ylim(0,ylim)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "\n",
    "# Plot each line\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot data and each line\n",
    "plt.subplot(1, 3, 1)\n",
    "plot_scatter_w_lines(\n",
    "    bike_sharing_data['temp_degrees_c'], bike_sharing_data['cnt'], line_1, \n",
    "    'Temperature (Degrees C)', 'Count of Bikes', 'y = 350x + 1000', 9000\n",
    ")\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plot_scatter_w_lines(\n",
    "    bike_sharing_data['temp_degrees_c'], bike_sharing_data['cnt'], line_2, \n",
    "    'Temperature (Degrees C)', '', 'y = 150x + 2000', 9000\n",
    ")\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plot_scatter_w_lines(\n",
    "    bike_sharing_data['temp_degrees_c'], bike_sharing_data['cnt'], line_3, \n",
    "    'Temperature (Degrees C)', '', 'y = 100x + 1000', 9000\n",
    ")\n",
    "\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Solving the linear regression problem\n",
    "\n",
    "[Top](#ML-Week-2---Linear-Regression) | [Previous section](#Part-1:-Introduction-to-regression) | [Next section](#Part-3:-Improving-the-regression:-adding-more-features) | [Bottom](#Thank-you)\n",
    "\n",
    "Based upon the above lines, you can start to get a feel about what slope and y-intercept best fit our data. So let's reframe the linear regression as the following...\n",
    "\n",
    "> **Linear regression** predicts a continuous variable from a dataset by **finding the best $m$ and $b$**, where $m$ and $b$ are the slope and y-intercept of the equation $f(x) = mx + b$.\n",
    "\n",
    "We will call $m$ and $b$ the **parameters** of our model.\n",
    "\n",
    "### Error functions\n",
    "\n",
    "To be able to have our computer find the best $m$ and $b$, we need to solidify our definition of what makes one combination of $m$ and $b$ better than another combination.\n",
    "\n",
    "To do this, let's define our first **error function** or **mean squared error (MSE)**. If we have a line, the MSE helps define how _far away_ our line is from a set of data points.\n",
    "\n",
    "We'll walk through an example using the code below. The code will graph a line based upon a set of data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is our data\n",
    "x = np.array([0, 1, 2, 3, 4, 5])\n",
    "y = np.array([-3, 1, 0, 5, 10, 8])\n",
    "f_x = 2 * x - 2\n",
    "\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.regplot(x=x, y=y, fit_reg=None)\n",
    "sns.regplot(x=x, y=f_x, marker='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a small visual to describe the process of finding the mean squared error, based upon...\n",
    "\n",
    "* The data points given (in blue)\n",
    "* A specific line (in orange)\n",
    "\n",
    "---\n",
    "\n",
    "<img src=\"img/MSE_Diagram.png\" width=\"900\">\n",
    "\n",
    "---\n",
    "\n",
    "### Exercise\n",
    "\n",
    "The MSE can be found using the following four steps...\n",
    "\n",
    "1. Find the difference between the line `f_x` and the true points `y`\n",
    "2. Square these differences\n",
    "3. Sum the result\n",
    "4. Divide by the total amount of data points (you can usee the `len(my_vec)` function to find the length of an array)\n",
    "\n",
    "Complete the following code to manually find the MSE. The comments should help describe the process step-by-step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERT YOUR CODE HERE\n",
    "\n",
    "# Print the variables f_x and y that can be used to find the MSE\n",
    "print(f_x)\n",
    "print(y)\n",
    "\n",
    "# Find the difference between f_x and y\n",
    "\n",
    "\n",
    "# Square these differences\n",
    "\n",
    "\n",
    "# Sum the result\n",
    "\n",
    "\n",
    "# Divide by the total amount of data points\n",
    "mse_ans = 0\n",
    "\n",
    "print(mse_ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarise these steps, the MSE is written by this formula...\n",
    "\n",
    "\n",
    "$$ \\frac{1}{N}\\sum_{i=1}^{N}[f(x_i) - y_i]^2 $$\n",
    "\n",
    "\n",
    "We can easily find the MSE using the [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html) library. The following code re-plots our three different lines described above, with MSE values.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import in the sklearn MSE\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Plot with MSE\n",
    "# Plot each line\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot data and each line\n",
    "plt.subplot(1, 3, 1)\n",
    "mse_1 = round(mean_squared_error(bike_sharing_data['cnt'], line_1), 2)\n",
    "plot_scatter_w_lines(\n",
    "    bike_sharing_data['temp_degrees_c'], bike_sharing_data['cnt'], line_1, \n",
    "    'Temperature (Degrees C)', 'Count of Bikes', 'y = 350x + 1000, MSE: ' + str(mse_1), 9000\n",
    ")\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "mse_2 = round(mean_squared_error(bike_sharing_data['cnt'], line_2), 2)\n",
    "plot_scatter_w_lines(\n",
    "    bike_sharing_data['temp_degrees_c'], bike_sharing_data['cnt'], line_2, \n",
    "    'Temperature (Degrees C)', '', 'y = 150x + 2000, MSE: ' + str(mse_2), 9000\n",
    ")\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "mse_3 = round(mean_squared_error(bike_sharing_data['cnt'], line_3), 2)\n",
    "plot_scatter_w_lines(\n",
    "    bike_sharing_data['temp_degrees_c'], bike_sharing_data['cnt'], line_3, \n",
    "    'Temperature (Degrees C)', '', 'y = 100x + 1000, MSE: ' + str(mse_3), 9000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see, the better the line fits our data, the **lower the MSE**! So, when we run linear regression, the goal is to **find the values of $m$ and $b$ that achieve the minimum MSE** for our data.\n",
    "\n",
    "### Running linear regression in Python\n",
    "\n",
    "Finding the best $m$ and $b$ in Python is pretty easy. We'll use the [sklearn \"Linear Regression\"](https://scikit-learn.org/stable/modules/generates/sklearn.linear_model.LinearRegression.html) module to do this.\n",
    "\n",
    "The following code will run our first **learning algorithm**. It will do this within the following steps...\n",
    "\n",
    "1. We will first create an sklearn LinearRegression object called `lr`\n",
    "2. We will then _fit_ a line to our data, using the `lr.fit(X, y)` method. This will find the optimal $m$ and $b$ at the minimum MSE\n",
    "3. We will then _re-predict_ our data from the linear equation that was created.\n",
    "\n",
    "We can then assess how good our line was using the MSE metric. Also note, since we only have one-column of data we're using, we call this process **univariate linear regression**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import LinearRegression from sklearn.linear_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# 1. Create an object\n",
    "lr = LinearRegression()\n",
    "\n",
    "# 2. Fit the data\n",
    "lr.fit(X=bike_sharing_data[['temp_degrees_c']], y=bike_sharing_data['cnt'])\n",
    "\n",
    "# 3. Repredict the data\n",
    "predicted_data = lr.predict(X=bike_sharing_data[['temp_degrees_c']])\n",
    "\n",
    "# MSE\n",
    "mse = mean_squared_error(bike_sharing_data['cnt'], predicted_data)\n",
    "\n",
    "# Plot line\n",
    "plt.figure(figsize=(10, 5))\n",
    "plot_scatter_w_lines(\n",
    "    bike_sharing_data['temp_degrees_c'], bike_sharing_data['cnt'], predicted_data, \n",
    "    'Temperature (Degrees C)', '', 'y = %.2fx + %.2f, MSE: %.2f' % (lr.coef_[0], lr.intercept_, mse), 9000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: How does sklearn find the optimal equation?\n",
    "\n",
    "sklearn does not just magically find an optimal $m$ and $b$ for a set of data. Instead, it uses the MSE to develop a function, and then finds a minimum of that function. Let's recall our example with the small `f_x` and `y` dataset (taking just the first three values of each vector for simplicity)...\n",
    "\n",
    "* $x = [0, 1, 2]$\n",
    "* $y = [-3, 1, 0]$\n",
    "\n",
    "\n",
    "We can plug these values into our MSE equation to get the following, pretending we do not know what the values of $m$ and $b$ are...\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "MSE = \\frac{1}{N}\\sum_{i=1}^{N}[f(x_i) - y_i]^2  \\\\\n",
    "= \\frac{1}{3}[(f(x_1) - y_1)^2 + (f(x_2) - y_2)^2 + (f(x_3) - y_3)^2]\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "Let's plug values in. We'll plug each $y_i$ in from our vector, and for each $f(x_i)$ we'll sub in the formula $f(x_i) = mx_i + b$ with each appropriate $x_i$ value.\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "= \\frac{1}{3}[(m*0 + b - -3)^2 + (m*1 + b - 1)^2 + (m*2 + b - 0)^2]\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "and simplifying, we get the following **quadratic equation** (remember from the last lesson that _quadratic equations_ had a highest power of $2$ on any variable).\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "MSE = 5m^2 + 3b^2 + 6mb - 2m + 4b + 10\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "For convenience sake, let's assume $b = 0$ in our final graph. The function is now...\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "MSE = 5m^2 - 2m + 10\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "Let's graph this function. What shape do we get?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create m and mse\n",
    "m = np.array(range(-10, 10))\n",
    "mse = 5*np.power(m, 2) - 2*m + 10\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.regplot(x=m, y=mse, marker='.', order=2)\n",
    "plt.xlabel('m')\n",
    "plt.ylabel('MSE')\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returning to our brief lesson on optimisation from last week, the minimum of the MSE function occurs where the **gradient = 0**. In this function, the gradient = 0 at $m = \\frac{1}{5} = 0.2$, so the best $m$ to fit our data (given $b=0$) is $m=0.2$.\n",
    "\n",
    "Here's a gif that shows minimisation down a quadratic function. The **cost** on the y-axis in the left-hand graph should look familiar...it's our MSE!\n",
    "\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*KQVi812_aERFRolz_5G3rA.gif\" width=\"800\">\n",
    "\n",
    "#### What happens when our function is more complex?\n",
    "\n",
    "The MSE function we looked at within the last image had a pretty obvious minimum. But, what happens when the MSE is a more complex function? Our real MSE had two inputs, $m$ _and_ $b$.\n",
    "\n",
    "We can graph this below, using a 3D plot (since we have two inputs, $m$ and $b$, for every $MSE$ output)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Create figure\n",
    "fig = plt.figure(figsize=(15, 6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# For each set of style and range settings, plot n random points in the box\n",
    "m = np.arange(-10, 10, step=1)\n",
    "b = np.arange(-10, 10, step=1)\n",
    "m, b = np.meshgrid(m, b)\n",
    "mse = 5*np.power(m, 2) + 3*np.power(b, 2) + 6*m*b - 2*m + 4*b + 10\n",
    "surf = ax.plot_surface(m, b, mse, linewidth=0, cmap=cm.coolwarm, antialiased=False)\n",
    "\n",
    "ax.set_xlabel('m')\n",
    "ax.set_ylabel('b')\n",
    "ax.set_zlabel('MSE')\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this function is a little bit more complex, and even visually, it's not super clear where the minimum of the function is. Thus, computers typically use optimisation methods that _approximate_ the gradient. One such technique is called **gradient descent**.\n",
    "\n",
    "#### Gradient descent\n",
    "\n",
    "Gradient descent in an **optimisation** algorithm that iteratively tries to find a minimum of a function. [This blogpost](https://towardsdatascience.com/gradient-descent-in-a-nutshell-eaf8c18212f0) does a great job of explaining that gradient descent is like climbing down a hill. It has two steps...\n",
    "\n",
    "1. A **direction update** step, where we approximate which direction we should move down the hill\n",
    "2. A **parameter update** step, where we move using a given **step size** within the direction we chose\n",
    "\n",
    "The **step size** parameter is _really important_ in gradient descent, and is often represented by the greek letter $\\alpha$. Here's a visual of what gradient descent looks like, moving down a parabola, with a big and small value of $\\alpha$:\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/0*QwE8M4MupSdqA3M4.png\" width=\"500\">\n",
    "\n",
    "Tldr;\n",
    "\n",
    "* Large $\\alpha$ trains the algorithm faster by taking _larger steps_ in the direction of the minimum\n",
    "* Small $\\alpha$ values train the algorithm slower\n",
    "\n",
    "It seems like we would always choose large $\\alpha$ then, right? Well, it turns out there's an issue with this. It's possible if our $\\alpha$ is too big, and we have multiple local minimums, we might completely miss the value we want to achieve, and climb down the wrong hill!\n",
    "\n",
    "<br>\n",
    "\n",
    "![](img/optimums.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "If you want a mathemtical discussion of gradient descent, see more [here](http://mccormickml.com/2014/03/04/gradient-descent-derivation/). You can directly derive gradient descent algorithms from the partial derivaties of the MSE function.\n",
    "\n",
    "#### Last note\n",
    "\n",
    "It turns out that for linear regression, using the MSE, you do not need to use gradient descent. The techniques typically used come from the field of Linear Algebra. See more [here](https://docs.scipy.org/doc/scipy-0.19.0/reference/generated/scipy.linalg.lstsq.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Improving the regression: adding more features\n",
    "\n",
    "---\n",
    "[Top](#ML-Week-2---Linear-Regression) | [Previous section](#Part-2:-Solving-the-linear-regression-problem) | [Next section](#Part-4:-Testing-the-model-and-more) | [Bottom](#Thank-you)\n",
    "\n",
    "### Using more features\n",
    "\n",
    "Thus far, we have done regression with **one input variable**. We have another variable in our dataset that describes the **windspeed**. Let's graph our scatterplot data with this additional variable, and see how it affects the count of bikes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a figure\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Graph count vs. temp_degrees_c\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.scatterplot(x=bike_sharing_data['temp_degrees_c'], y=bike_sharing_data['cnt'])\n",
    "plt.xlabel('Temperature (Degrees C)')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Graph count vs. windspeed\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.scatterplot(x=bike_sharing_data['windspeed'], y=bike_sharing_data['cnt'])\n",
    "plt.xlabel('Windspeed')\n",
    "plt.ylabel('')\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the count of bikes used vary with windspeed? Hopefully, the relationship makes logical sense.\n",
    "\n",
    "---\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Go back to the [linear regression](#Running-linear-regression-in-Python) code. \n",
    "\n",
    "\n",
    "In the cell below, perform the following:\n",
    "\n",
    "* Copy and paste the code to build a linear regression model. **DO NOT PASTE** the plotting section.\n",
    "* Make a linear regression model called `lr_2`\n",
    "* Instead of using just the `temp_degrees_c`, add the `windspeed` column to the `fit()` method.\n",
    "\n",
    "**Answer the following:** Does the MSE increase or decrease when adding the additional column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERT YOUR CODE HERE\n",
    "\n",
    "\n",
    "# 1. Create an object\n",
    "\n",
    "# 2. Fit the data\n",
    "\n",
    "# 3. Repredict the data\n",
    "\n",
    "# 4. Calculate the MSE using the mean_squared_error() function\n",
    "\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is called a **multivariate** regression problem. Notice how we used **two features**. This expanded our linear equation to look like the following...\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "$$f(\\textbf{x}) = m_{temp}x_{temp} + m_{wind}x_{wind} + b$$\n",
    "\n",
    "<br>\n",
    "\n",
    "Thus, we have one $m_i$ where $i$ represents **each feature** in our dataset. Notice how I used a bolded $\\textbf{x}$ to signify that we are now inputting a _vector_ into our model. Where $\\textbf{x} = [x_{temp}, x_{wind}]$\n",
    "\n",
    "### Creating more features\n",
    "\n",
    "Though we have **two columns**, we're not necessarily done. It's possible to create more features off of our dataset. \n",
    "\n",
    "___\n",
    "\n",
    "### Thought exercise\n",
    "\n",
    "Look at the relationship between temperature and the count of bicycles. Is a line the best relationship for our model? Think about when people are likely to use bicycles.\n",
    "\n",
    "___\n",
    "\n",
    "Let's create a new feature, where we **square** the temperature. We can then run our model again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the new column\n",
    "bike_sharing_data['temp_degrees_c_2'] = bike_sharing_data['temp_degrees_c'].copy()**2\n",
    "\n",
    "# 1. Create an object\n",
    "lr_3 = LinearRegression()\n",
    "\n",
    "# 2. Fit the data\n",
    "lr_3.fit(X=bike_sharing_data[['temp_degrees_c', 'windspeed', 'temp_degrees_c_2']], y=bike_sharing_data['cnt'])\n",
    "\n",
    "# 3. Repredict the data\n",
    "predicted_data = lr_3.predict(X=bike_sharing_data[['temp_degrees_c', 'windspeed', 'temp_degrees_c_2']])\n",
    "\n",
    "# MSE\n",
    "mse = mean_squared_error(bike_sharing_data['cnt'], predicted_data)\n",
    "\n",
    "# Print MSE\n",
    "print('m_temp = %.2f, m_temp_2 = %.2f, m_wind = %.2f, b=%.2f'\n",
    "      % (lr_3.coef_[0], lr_3.coef_[1], lr_3.coef_[2], lr_3.intercept_))\n",
    "print('MSE: %.2f' % mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have **three features** in our model, and **four parameters** trained. The model looks liks this:\n",
    "\n",
    "<br>\n",
    "\n",
    "$$f(\\textbf{x}) = m_{temp}x_{temp} + m_{temp^2}x_{temp}^2 + m_{wind}x_{wind} + b$$\n",
    "\n",
    "<br>\n",
    "\n",
    "We could keep adding parameters in our model. We'll generalise our linear equation equation to look like the following...\n",
    "\n",
    "<br>\n",
    "\n",
    "$$f(\\textbf{x}) = \\sum_{j=1}^{L}m_{j}x_{j} + b$$\n",
    "\n",
    "<br>\n",
    "\n",
    "This says we have a total of $L$ features in our model, and each feature has a slope $m_j$. Our equation sums through $L$ features and adds the intercept, $b$ (**thoughts**...what does $b$ represent?).\n",
    "\n",
    "#### Generalising our mdoel\n",
    "\n",
    "The following function adds more and more parameters to our model, by adding additional powers to one or more of our two original features. It then plots the fit of temperature vs. count, and windspeed vs. count. Run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression(data, temp_pow, wind_pow):\n",
    "    \"\"\"\n",
    "    Run linear regression and plot the result.\n",
    "    \n",
    "    :input data: <pd.DataFrame>, the dataframe with data\n",
    "    :input temp_pow: <int>, the max power of the temperature\n",
    "    :input wind_pow: <int>, the max power of the windspeed\n",
    "    \"\"\"\n",
    "    # Create data\n",
    "    temp_data = data.copy()\n",
    "    \n",
    "    if temp_pow > 1:\n",
    "        for i in range(2, temp_pow + 1):\n",
    "            temp_data['temp_degrees_c_' + str(i)] = temp_data['temp_degrees_c'] ** i\n",
    "    if wind_pow > 1:\n",
    "        for i in range(2, wind_pow + 1):\n",
    "            temp_data['windspeed_' + str(i)] = temp_data['windspeed'] ** i\n",
    "            \n",
    "    # Run linear regression\n",
    "    cols = [c for c in temp_data.columns if c not in ['cnt', 'dteday']]\n",
    "    linreg = LinearRegression()\n",
    "    linreg.fit(X=temp_data[cols], y=temp_data['cnt'])\n",
    "\n",
    "    # Predict\n",
    "    pred = linreg.predict(X=temp_data[cols])\n",
    "    \n",
    "    # Plot\n",
    "    fig = plt.figure(figsize=(15, 5))\n",
    "    # Graph count vs. temp_degrees_c\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.regplot(x=temp_data['temp_degrees_c'], y=temp_data['cnt'], fit_reg=False)\n",
    "    sns.regplot(x=temp_data['temp_degrees_c'], y=pred, marker='', ci=False, order=temp_pow)\n",
    "    plt.xlabel('Temperature (Degrees C)')\n",
    "    plt.ylabel('Count')\n",
    "\n",
    "    # Graph count vs. windspeed\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.regplot(x=temp_data['windspeed'], y=temp_data['cnt'], fit_reg=False)\n",
    "    sns.regplot(x=temp_data['windspeed'], y=pred, marker='', ci=False, order=wind_pow)\n",
    "    plt.xlabel('Windspeed')\n",
    "    plt.ylabel('')\n",
    "    \n",
    "    # Title\n",
    "    mse = mean_squared_error(temp_data['cnt'], pred)\n",
    "    fig.suptitle('Regression with MSE=%.2f' % mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example of running this code for the equation we have already developed. Remember the equation was:\n",
    "\n",
    "<br>\n",
    "\n",
    "$$f(\\textbf{x}) = m_{temp}x_{temp} + m_{temp^2}x_{temp}^2 + m_{wind}x_{wind} + b$$\n",
    "\n",
    "<br>\n",
    "\n",
    "Here, `temp_pow = 2`, and `wind_pow = 1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the code\n",
    "linear_regression(data=bike_sharing_data, temp_pow=2, wind_pow=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "In the cell below, play around with creating different features by changing `temp_pow` and `windspeed`. How does the MSE change with higher powers?\n",
    "\n",
    "**Note:** Normally we only raise the powers if it makes sense from our qualitative understanding of the problem. More on this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE THE CODE BELOW\n",
    "linear_regression(data=bike_sharing_data, temp_pow=2, wind_pow=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Testing the model and more\n",
    "\n",
    "---\n",
    "[Top](#ML-Week-2---Linear-Regression) | [Previous section](#Part-3:-Improving-the-regression:-adding-more-features) | [Next section](#Thank-you) | [Bottom](#Thank-you)\n",
    "\n",
    "The entire process we have been doing today is a form of **supervised learning**.\n",
    "\n",
    "> **Supervised learning** trains a model based upon an inputted set of features _and_ a label or variable that the model is trying to predict.\n",
    "\n",
    "![](https://qph.fs.quoracdn.net/main-qimg-33a660216575e3754b12b1718c0e052c)\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "We've sort of left a step out though. Usually after we train a model, we save the model and then use it to provide a prediction for _new_ data. Capital bikeshare wouldn't be interested in using the model on past data, but would really want it to predict what is likely to happen in the _future_, once it knows a projected temperature/windspeed for a day. \n",
    "\n",
    "### Test sets\n",
    "\n",
    "Since we don't have future data available, what we normally do is split our dataset into **two datasets**.\n",
    "\n",
    "* A **training set**: the set of data we use to _fit_ a model\n",
    "* A **test set**: the set of data we use to _test_ the model's performance\n",
    "\n",
    "The **test set** is supposed to represent how our model might perform on _future_ data, and is not involved in training a model. The sklearn library has a module which splits the data into training and test sets, called [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html). Usually we leave out about 20-30% of our data for testing.\n",
    "\n",
    "Run the following code to create a function called `linear_regressin_w_test()`. It is very similar to our original `linear_regression()` function, except it also...\n",
    "\n",
    "* Splits our data into train and test sets\n",
    "* Only trains using the training set\n",
    "* Prints the MSE of the model on the test set within our graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def linear_regression_w_test(data, temp_pow, wind_pow):\n",
    "    \"\"\"\n",
    "    Run linear regression and plot the result.\n",
    "    \n",
    "    :input data: <pd.DataFrame>, the dataframe with data\n",
    "    :input temp_pow: <int>, the max power of the temperature\n",
    "    :input wind_pow: <int>, the max power of the windspeed\n",
    "    \"\"\"\n",
    "    # Create data\n",
    "    temp_data = data.copy()\n",
    "    \n",
    "    if temp_pow > 1:\n",
    "        for i in range(2, temp_pow + 1):\n",
    "            temp_data['temp_degrees_c_' + str(i)] = temp_data['temp_degrees_c'] ** i\n",
    "    if wind_pow > 1:\n",
    "        for i in range(2, wind_pow + 1):\n",
    "            temp_data['windspeed_' + str(i)] = temp_data['windspeed'] ** i\n",
    "            \n",
    "    # Split into train and test sets\n",
    "    train, test = train_test_split(temp_data, random_state=42)\n",
    "            \n",
    "    # Run linear regression\n",
    "    cols = [c for c in temp_data.columns if c not in ['cnt', 'dteday']]\n",
    "    linreg = LinearRegression()\n",
    "    linreg.fit(X=train[cols], y=train['cnt'])\n",
    "\n",
    "    # Predict\n",
    "    pred = linreg.predict(X=test[cols])\n",
    "    \n",
    "    # Plot\n",
    "    fig = plt.figure(figsize=(15, 5))\n",
    "    # Graph count vs. temp_degrees_c\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.regplot(x=test['temp_degrees_c'], y=test['cnt'], fit_reg=False)\n",
    "    sns.regplot(x=test['temp_degrees_c'], y=pred, marker='', ci=False, order=temp_pow)\n",
    "    plt.xlabel('Temperature (Degrees C)')\n",
    "    plt.ylabel('Count')\n",
    "\n",
    "    # Graph count vs. windspeed\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.regplot(x=test['windspeed'], y=test['cnt'], fit_reg=False)\n",
    "    sns.regplot(x=test['windspeed'], y=pred, marker='', ci=False, order=wind_pow)\n",
    "    plt.xlabel('Windspeed')\n",
    "    plt.ylabel('')\n",
    "    \n",
    "    # Title\n",
    "    mse = mean_squared_error(test['cnt'], pred)\n",
    "    fig.suptitle('Regression with MSE=%.2f' % mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Exercise\n",
    "\n",
    "\n",
    "The following code can be run in the exact same way as our `linear_regression` function, with the same parameters. Change `temp_pow` and `wind_pow` as you did previously to create new features. The only difference is it calculates the **MSE of the test set** in relation to the line created from the training set.\n",
    "\n",
    "How does the MSE change when you create features of higher order? Is it different than it changed previously?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE THE CODE BELOW\n",
    "linear_regression_w_test(data=bike_sharing_data, temp_pow=3, wind_pow=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you create higher order features, you **overfit** the model. This is a common problem in machine learning, and we'll talk about it next week.\n",
    "\n",
    "---\n",
    "\n",
    "### Exercise\n",
    "\n",
    "The original dataset can be loaded in the cell below. It has a lot more features, which you can find research on the [its UCI ML page](https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the full dataset\n",
    "full_bike_sharing_data = pd.read_csv('data/day.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell has a template for a linear regression model. You can change the `cols` variable to add more features. Play around with linear regression! Create more features as you'd like.\n",
    "\n",
    "If you want to plot the result, you'll have to change the `plot_col` column. Currently the code is plotting the `temp` variable. If you also add higher order features, you can change the `order` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test sets\n",
    "train_data, test_data = train_test_split(full_bike_sharing_data, random_state=42)\n",
    "\n",
    "# CHANGE COLS TO FIT MODEL WITH HERE\n",
    "cols = ['temp', 'windspeed']\n",
    "full_lr = LinearRegression()\n",
    "full_lr.fit(X=train_data[cols], y=train_data['cnt'])\n",
    "\n",
    "# Predict\n",
    "pred = full_lr.predict(X=test_data[cols])\n",
    "\n",
    "# CHANGE COL TO PLOT AND ORDER HERE\n",
    "plot_col = 'windspeed'\n",
    "order = 1\n",
    "\n",
    "# Plot\n",
    "fig = plt.figure(figsize=(15, 5))\n",
    "sns.regplot(x=test_data['temp'], y=test_data['cnt'], fit_reg=False)\n",
    "sns.regplot(x=test_data['temp'], y=pred, marker='', ci=False, order=order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thank you\n",
    "\n",
    "[Top](#ML-Week-2---Linear-Regression) | [Previous section](#Part-4:-Testing-the-model-and-more) | [Next section](#Thank-you) | [Bottom](#Thank-you)\n",
    "\n",
    "That concludes our week 2 lesson. Hopefully you enjoyed :)\n",
    "\n",
    "### Downloading the notebook\n",
    "\n",
    "If you would like to retain your work, please follow the following directions:\n",
    "* On the top of this screen, in the header menu, click \"File\", then \"Download as\" and then \"Notebook\".\n",
    "* You will need to download [Python 3.7 with Anaconda](https://www.anaconda.com/distribution/#download-section) to use this in the future"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
