{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Week 5 - Artificial Neural Networks\n",
    "---\n",
    "[Top](#ML-Week-5---Artificial-Neural-Networks) | [Previous](#ML-Week-5---Artificial-Neural-Networks) | [Next](#Part-0:-Imports-and-Setup) | [Bottom](#Cheers)\n",
    "\n",
    "Today's lesson will be on Artificial Neural Networks (ANNs). I'll be trying to cover an extremely broad and complicated topic with the following parts:\n",
    "\n",
    "* [Part 0: Imports and Setup](#Part-0:-Imports-and-Setup)\n",
    "* [Part 1: Biological Intuition](#Part-1:-Biological-Intuition-(Optional))\n",
    "* [Part 2: Neural Network Basics](#Part-2:-Neural-Network-Basics)\n",
    "* [Part 3: Perceptrons](#Part-3:-Perceptrons)\n",
    "* [Part 4: From Logistic Regression to ANNs](#Part-4:-From-Logistic-Regression-to-ANNs)\n",
    "* [Part 5: Deeper Networks](#Part-5:-Deeper-Networks)\n",
    "* [Part 6: Intro to Computer Vision](#Part-6:-Intro-to-Computer-Vision)\n",
    "* [Part 7: Bonus Content](#Part-7:-Bonus-Content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Imports and Setup\n",
    "\n",
    "---\n",
    "[Top](#ML-Week-5---Artificial-Neural-Networks) | [Previous](#ML-Week-5---Artificial-Neural-Networks) | [Next](#Part-1:-Biological-Intuition-(Optional)) | [Bottom](#Cheers)\n",
    "\n",
    "Before we can jump into learning there's quite a few things to import and some code to setup visualising our data. Here's a helpful table on these libraries in case you want to dig into them a little bit more.\n",
    "\n",
    "| Library | Description |\n",
    "|- | - |\n",
    "| [matplotlib](https://matplotlib.org/) | Plotting Python functions and code |\n",
    "| [seaborn](https://seaborn.pydata.org/) | Statistical visualisations in Python |\n",
    "| [pandas](https://pandas.pydata.org/) | Python library for working with tabular data |\n",
    "| [numpy](https://www.numpy.org/) | Mathemtical manipulations in Python |\n",
    "| [warnings](https://docs.python.org/3/library/warnings.html) | Regulates warnings printed in Python |\n",
    "| [Datetime](https://docs.python.org/3/library/datetime.html) | Working with date objects in Python |\n",
    "| [sklearn](https://scikit-learn.org/) | Python library for prepping, building and assessing many machine learning models |\n",
    "| [keras](https://keras.io/) | Interface for building neural networks in Python |\n",
    "\n",
    "### Imports and Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "\n",
    "# Tabular data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 200)\n",
    "\n",
    "# Dates\n",
    "from datetime import datetime\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.datasets import make_classification, make_moons, make_circles\n",
    "from sklearn.metrics import confusion_matrix, classification_report, mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, KFold\n",
    "\n",
    "# Neural networks\n",
    "!pip install keras \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization, Activation\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import keras.backend as K\n",
    "from keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions\n",
    "\n",
    "This cell will house a bunch of utility functions used throughout the notebook. I wouldn't waste too much time trying to understand what's going on here as most of it is just for visualisation or creating quick datasets. A lot of these functions and code come from [this blogpost](https://towardsdatascience.com/applied-deep-learning-part-1-artificial-neural-networks-d7834f67a4f6), which actually does a really good job explaining the basics of neural networks.\n",
    "\n",
    "We'll be providing a little bit more of a high-level explanation, and move to a more fun example later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(func, X, y, figsize=(9, 6)):\n",
    "    \"\"\"\n",
    "    Plots decision boundaries for a classifier\n",
    "    \n",
    "    :param func: <function> Prediction function\n",
    "    :param X: <2d-array like> input data\n",
    "    :param y: <1d-array like> target data\n",
    "    :param figsize: <tuple> size of figure\n",
    "    \"\"\"\n",
    "    amin, bmin = X.min(axis=0) - 0.1\n",
    "    amax, bmax = X.max(axis=0) + 0.1\n",
    "    hticks = np.linspace(amin, amax, 101)\n",
    "    vticks = np.linspace(bmin, bmax, 101)\n",
    "    \n",
    "    aa, bb = np.meshgrid(hticks, vticks)\n",
    "    ab = np.c_[aa.ravel(), bb.ravel()]\n",
    "    c = func(ab)\n",
    "    cc = c.reshape(aa.shape)\n",
    "\n",
    "    cm = plt.cm.RdBu\n",
    "    cm_bright = ListedColormap(['#cc2529', '#396ab1'])\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    contour = plt.contourf(aa, bb, cc, cmap=cm, alpha=0.8)\n",
    "    \n",
    "    ax_c = fig.colorbar(contour)\n",
    "    ax_c.set_label(\"$P(y = 1)$\")\n",
    "    ax_c.set_ticks([0, 0.25, 0.5, 0.75, 1])\n",
    "    \n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cm_bright)\n",
    "    plt.xlim(amin, amax)\n",
    "    plt.ylim(bmin, bmax)\n",
    "    \n",
    "    \n",
    "def plot_data(X, y, figsize=None):\n",
    "    \"\"\"\n",
    "    Plots 2-dimensional data\n",
    "    \n",
    "    :param X: <2d-array like> input data\n",
    "    :param y: <1d-array like> target data\n",
    "    :param figsize: <tuple> size of figure\n",
    "    \"\"\"\n",
    "    if not figsize:\n",
    "        figsize = (8, 6)\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.plot(X[y==0, 0], X[y==0, 1], 'or', alpha=0.5, label=0)\n",
    "    plt.plot(X[y==1, 0], X[y==1, 1], 'ob', alpha=0.5, label=1)\n",
    "    plt.xlim((min(X[:, 0])-0.1, max(X[:, 0])+0.1))\n",
    "    plt.ylim((min(X[:, 1])-0.1, max(X[:, 1])+0.1))\n",
    "    plt.legend()\n",
    "\n",
    "    \n",
    "def plot_loss_accuracy(history):\n",
    "    \"\"\"\n",
    "    Plot training history (loss and accuracy)\n",
    "    \n",
    "    :param history: <object>, model fit output with history and loss\n",
    "    \"\"\"\n",
    "    historydf = pd.DataFrame(history.history, index=history.epoch)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    historydf.plot(ylim=(0, max(1, historydf.values.max())))\n",
    "    loss = history.history['loss'][-1]\n",
    "    acc = history.history['acc'][-1]\n",
    "    plt.title('Loss: %.3f, Accuracy: %.3f' % (loss, acc))\n",
    "\n",
    "\n",
    "#Nice function to show a confusion matrix\n",
    "def plot_confusion_matrix(model, X, y):\n",
    "    \"\"\"\n",
    "    Plot confusion matrix\n",
    "    \n",
    "    :param model: <ML model>, model fit\n",
    "    :param X: <2d-array like> input data\n",
    "    :param y: <1d-array like> target data\n",
    "    \"\"\"\n",
    "    y_pred = model.predict_classes(X, verbose=0)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(pd.DataFrame(confusion_matrix(y, y_pred)), annot=True, fmt='d', cmap='YlGnBu', alpha=0.8, vmin=0)\n",
    "\n",
    "\n",
    "# Simple sinewave generator function\n",
    "def make_sine_wave():\n",
    "    \"\"\"\n",
    "    Makes a basic sine wave\n",
    "    \"\"\"\n",
    "    c = 3\n",
    "    num = 2400\n",
    "    step = num/(c*4)\n",
    "    np.random.seed(0)\n",
    "    x0 = np.linspace(-c*np.pi, c*np.pi, num)\n",
    "    x1 = np.sin(x0)\n",
    "    noise = np.random.normal(0, 0.1, num) + 0.1\n",
    "    noise = np.sign(x1) * np.abs(noise)\n",
    "    x1  = x1 + noise\n",
    "    x0 = x0 + (np.asarray(range(num)) / step) * 0.3\n",
    "    X = np.column_stack((x0, x1))\n",
    "    y = np.asarray([int((i/step)%2 > 1) for i in range(len(x0))])\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Biological Intuition (Optional)\n",
    "\n",
    "---\n",
    "[Top](#ML-Week-5---Artificial-Neural-Networks) | [Previous](#Part-0:-Imports-and-Setup) | [Next](#Part-2:-Neural-Network-Basics) | [Bottom](#Cheers)\n",
    "\n",
    "When first learning about neural networks it is common to start with some sort of biological intuition. I personally don't find it useful since it's far more complex than an ANN but none the less here is a picture showing a biological neural synapse. \n",
    "\n",
    "<img src=\"img/bnn.png\" width=\"500\">\n",
    "\n",
    "Verterbrates' brains consist of neurons and those neurons communicate via exchanging electrical signals. The signals flow through axons that connect to other cells. This process is incredibly complex and not well understood but it is the basis of how organic creatures learn. If you want to learn more about it a good start is the [wikipedia page on synapses](https://en.wikipedia.org/wiki/Synapse) or this [khan academy video](https://www.youtube.com/watch?v=Tbq-KZaXiL4).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Neural Network Basics ##\n",
    "\n",
    "---\n",
    "[Top](#ML-Week-5---Artificial-Neural-Networks) | [Previous](#Part-1:-Biological-Intuition-(Optional)) | [Next](#Part-3:-Perceptrons) | [Bottom](#Cheers)\n",
    "\n",
    "### What is a neural network? ###\n",
    "\n",
    "A neural network is just a supervised learning architecture that allows us to approximate any mathematical function. It consists of building blocks we call neurons or perceptrons. These building blocks interconnect to each other and form distinct layers. A shallow neural network consists of only 3 basic layers:\n",
    "\n",
    "* Input layer\n",
    "* Hidden layer\n",
    "* Output layer\n",
    "\n",
    "Where the input layer takes whatever data we want to process, then passes it to the hidden layer which does the computation and then forwards it to the output layer that produces some sort of prediction. A deep neural network merely has more hidden layers and we will discuss the advantages of that layer.\n",
    "\n",
    "### What does a neural network look like? ###\n",
    "\n",
    "A neural network looks like a graph, the graph consists of three types of nodes in layers which are all interconnected like so: \n",
    "\n",
    "<img src=\"img/ann_tom.png\" width=\"661\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Glossary ##\n",
    "\n",
    "Before diving any deeper into neural networks let's first get a basic understanding of some important terminology. Even if you understand nothing about neural networks by the end of today, if you at least start to grasp the terminology it will help in further study!\n",
    "            \n",
    "<div class=\"row\">\n",
    "<h3 align=\"left\">Inputs</h3>\n",
    "<img align=\"left\" src=\"img/glossary/inputs.png\" >\n",
    "<p style=\"text-align:left;\">\n",
    "This is the data we feed into the net. It could be images, audio recordings or stock prices. Before we feed it into the network we may have to preprocess it into a form that is more readable by the network.\n",
    "</p>\n",
    "</div>\n",
    "\n",
    "<div class=\"row\">\n",
    "<h3 align=\"left\">Outputs</h3>\n",
    "<img align=\"left\" src=\"img/glossary/outputs.png\" >\n",
    "<p style=\"text-align:left;\">\n",
    "This is the data we get back from the net. It will be formulated from all the output neurons. An example might be a classification of an image being either a dog or a cat.\n",
    "</p>\n",
    "</div>\n",
    "\n",
    "<div class=\"row\">\n",
    "<h3 align=\"left\">Neurons/Perceptrons</h3>\n",
    "<img align=\"left\" src=\"img/glossary/neuron.png\" >\n",
    "<p>\n",
    "The building block of a neural network. Take a series of inputs and produces an output.\n",
    "</p>\n",
    "</div>\n",
    "\n",
    "<div class=\"row\">\n",
    "<h3 align=\"left\">Activation Function</h3>\n",
    "<img align=\"left\" src=\"img/glossary/activationfunction.png\" >\n",
    "<p>\n",
    "All neuron outputs are fed through an activation function. This function helps to introduce non-linearity into the network and can also help keep output values manageable. \n",
    "</p>\n",
    "</div>\n",
    "\n",
    "<div class=\"row\">\n",
    "<h3 align=\"left\">Weight Space</h3>\n",
    "<img align=\"left\" src=\"img/glossary/weightspace.png\" >\n",
    "<p>\n",
    "Every connection between neurons has a weight. Weights, biases and the activation function define a neuron output. The goal of our network is to create an optimal weightspace at every layer that effectively learns to solve our input problem. </p>\n",
    "</div>\n",
    "\n",
    "<div class=\"row\">\n",
    "<h3 align=\"left\">Forward Pass</h3>\n",
    "<img align=\"left\" src=\"img/glossary/forwardpass.png\" >\n",
    "<p>\n",
    "A forward pass is when we send an input signal all the way to the output. Each perceptron is fires and passes its output to the next layer. We use a forward pass to produce our final outputs.\n",
    "</p>\n",
    "</div>\n",
    "\n",
    "<div class=\"row\">\n",
    "<h3 align=\"left\">Error</h3>\n",
    "<img align=\"left\" src=\"img/glossary/error.png\" >\n",
    "<p>\n",
    "We use the error between the predicted outputs and the known outputs to evalute our model. As we train the network we aim to minimise this error\n",
    " </p>\n",
    "</div>\n",
    "\n",
    "<div class=\"row\">\n",
    "<h3 align=\"left\">Backpropagation</h3>\n",
    "<img align=\"left\" src=\"img/glossary/backpropagation.png\" >\n",
    "<p>\n",
    "During the training process we employ the backpropagation algorithm. This algorithm moves from the output layer to the input layer and updates all the neuron parameters. As time goes on the network loss will decrease. This is quite a complicated mathematical concept and requires calculus. More informational on the mathematical aspect called gradient descent can be found <a href=\"https://en.wikipedia.org/wiki/Gradient_descent\">on wikipedia</a> .\n",
    "\n",
    "</p>\n",
    "</div>\n",
    "\n",
    "<div class=\"row\">\n",
    "<h3 align=\"left\">Hyperparamaters</h3>\n",
    "<img align=\"left\" src=\"img/glossary/hypers.png\" >\n",
    "<p>\n",
    "Hyperparamaters are configuration numbers that manage how the network is built. Examples include number of layers, number of neurons, how many iterations of training we do and what activation functions we use.</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Perceptrons\n",
    "\n",
    "---\n",
    "[Top](#ML-Week-5---Artificial-Neural-Networks) | [Previous](#Part-2:-Neural-Network-Basics) | [Next](#Part-4:-From-Logistic-Regression-to-ANNs) | [Bottom](#Cheers)\n",
    "\n",
    "Perceptrons are the basic building block of our neural networks. Their purpose is simple, transform an input into an output.\n",
    "\n",
    "<img src=\"img/perceptron.png\">\n",
    "\n",
    "The process can be summed up in english as  \n",
    "\n",
    "1. multiplying all inputs by the weight matrix and summing them together\n",
    "2. add 1 times some bias weight so we can shift the function\n",
    "3. feed this result through the activation function\n",
    "4. produce the output\n",
    "\n",
    "and for those of us who are more maths and visually inclined as\n",
    "\n",
    "<img src=\"img/matrix.png\">\n",
    "\n",
    "This process is actually the same as logistic regression, with multiple features and training example. We did logistic regression a couple of weeks ago."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: From Logistic Regression to ANNs\n",
    "\n",
    "---\n",
    "[Top](#ML-Week-5---Artificial-Neural-Networks) | [Previous](#Part-3:-Perceptrons) | [Next](#Part-5:-Deeper-Networks) | [Bottom](#Cheers)\n",
    "\n",
    "Last week you learned about how you can use logistic regression to solve linearly separable binary classification problems. Below we have a recap example showing how effective this is. We first make a scatter plot of two classes of points and then observe the class discriminator. Let's pretend we have data from two species of baby snakes that we have caught to research. Each snake has two distinguishing features...\n",
    "\n",
    "1. The length of the snake (in cm)\n",
    "2. The number of stripes on the snake's body\n",
    "\n",
    "The goal of the classifier would be to predict the species of snake based upon the length and the number of stripes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the data\n",
    "X, y = make_classification(n_samples=1000, n_features=2, n_redundant=0, \n",
    "                           n_informative=2, random_state=7, n_clusters_per_class=1)\n",
    "\n",
    "# Modify the data\n",
    "X=(X+2)*4 + 4\n",
    "\n",
    "plot_data(X, y)\n",
    "plt.title(\"Caught Baby Snakes\")\n",
    "plt.ylabel(\"Length (cm)\")\n",
    "plt.xlabel(\"Number of Stripes\")\n",
    "plt.gca().legend(('Tiger Snake','Eastern Brown Snake'))\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X, y)\n",
    "print('LR coefficients:', lr.coef_)\n",
    "print('LR intercept:', lr.intercept_)\n",
    "\n",
    "plot_data(X, y)\n",
    "\n",
    "limits = np.array([-2, 40])\n",
    "boundary = -(lr.coef_[0][0] * limits + lr.intercept_[0]) / lr.coef_[0][1]\n",
    "plt.title(\"Caught Baby Snakes\")\n",
    "plt.ylabel(\"Length (cm)\")\n",
    "plt.xlabel(\"Number of Stripes\")\n",
    "plt.gca().legend(('Tiger Snake','Eastern Brown Snake'))\n",
    "plt.plot(limits, boundary, \"g-\", linewidth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mini Exercise\n",
    "\n",
    "This data is called **linearly separable**. Why do think it's called that?\n",
    "\n",
    "#### Mini Exercise (Beastmode)\n",
    "\n",
    "Tweak the above code to produce a dataset which isnt linearly separable and observe the results.\n",
    "\n",
    "### Neural Network Replacement\n",
    "\n",
    "We can solve the same problem that logistic regression solves using a neural network. We use Keras to construct a network with a single dense layer with one neuron in it. The neuron can process an array of data size 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a sequential network\n",
    "model = Sequential()\n",
    "\n",
    "# Add a dense layer\n",
    "model.add(Dense(units=1, input_shape=(2,), activation='sigmoid'))\n",
    "\n",
    "# Compile the network and define our optimisation algorithm, loss function and metrics\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model for 50 iterations\n",
    "history = model.fit(x=X, y=y, verbose=0, epochs=50)\n",
    "\n",
    "# Visualise the training\n",
    "plot_loss_accuracy(history)\n",
    "\n",
    "# Visualise predidcted boundary\n",
    "plot_decision_boundary(lambda x: model.predict(x), X, y)\n",
    "plt.title(\"Caught Baby Snakes\")\n",
    "plt.ylabel(\"Length (cm)\")\n",
    "plt.xlabel(\"Number of Stripes\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember our old steps in fitting a model? Something that looked like...\n",
    "\n",
    "\n",
    "```python\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X, y)\n",
    "```\n",
    "\n",
    "\n",
    "This code is doing the same thing using keras, which is a different library than sklearn. The above code has been replaced by...\n",
    "\n",
    "\n",
    "```python\n",
    "# Add a dense layer\n",
    "model.add(Dense(units=1, input_shape=(2,), activation='sigmoid'))\n",
    "\n",
    "# Compile the network and define our optimisation algorithm, loss function and metrics\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model for 50 iterations\n",
    "history = model.fit(x=X, y=y, verbose=0, epochs=50)\n",
    "```\n",
    "\n",
    "It's a little bit more complicated, but the syntax from keras will actually let us make a more complicated model. More on this later...\n",
    "\n",
    "### Ponder this\n",
    "\n",
    "1. Why are we able to learn this classifier so quickly?\n",
    "\n",
    "2. Why are we able to learn at all given we only have 1 neuron?\n",
    "\n",
    "3. What happens to the decision boundary if we run the same code multiple times?\n",
    "\n",
    "4. What happens when we vary the number of epochs? Do higher epochs matter and can we get away with a lot less?\n",
    "\n",
    "Try to experiment with some of the other hyperparameters and read up about the layers [on the keras website](https://keras.io/layers/core/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slightly More Complex Problems\n",
    "\n",
    "As we saw the simple neural network is fantastic at solving the linearly separable classification problem but how does it fair on a more complex mathematical description?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_circles(n_samples=1000, noise=0.05, factor=0.3, random_state=0)\n",
    "plot_data(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try to train a model using a single layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same model as before\n",
    "model = Sequential()\n",
    "model.add(Dense(1, input_shape=(2,), activation='sigmoid'))\n",
    "\n",
    "model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train model for 100 epochs\n",
    "history = model.fit(X, y, verbose=0, epochs=100)\n",
    "plot_loss_accuracy(history)\n",
    "\n",
    "plot_decision_boundary(lambda x: model.predict(x), X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see it is completely useless. The network still tries to find a linear boundary. Clearly our network is not complex enough to learn how to solve this problem!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Deeper Networks\n",
    "\n",
    "---\n",
    "[Top](#ML-Week-5---Artificial-Neural-Networks) | [Previous](#Part-4:-From-Logistic-Regression-to-ANNs) | [Next](#Part-6:-Intro-to-Computer-Vision) | [Bottom](#Cheers)\n",
    "\n",
    "A **deep** network adds **hidden layers** to the model. The deeper the network, the _more layers_. This branch of machine learning is often referred to as \"Deep Learning\". As we saw before our shallow networks weren't able to solve problems like separating points which aren't linearly separable. \n",
    "\n",
    "### Exercise \n",
    "\n",
    "Try and solve the previous circle problem by adding another layer (or layers) to the network. Then try changing some of the hyper paramaters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same model as before\n",
    "model = Sequential()\n",
    "\n",
    "# ADD MORE LAYERS HERE. Here's a clue:\n",
    "# model.add(Dense(10, input_shape=(2,), activation='sigmoid'))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(1, input_shape=(2,), activation='sigmoid'))\n",
    "\n",
    "model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train model for 100 epochs\n",
    "history = model.fit(X, y, verbose=0, epochs=100)\n",
    "plot_loss_accuracy(history)\n",
    "\n",
    "plot_decision_boundary(lambda x: model.predict(x), X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we've solved one non linear problem but it still seems relatively trivial.\n",
    "\n",
    "### Optional: More complex models (sine curve)\n",
    "\n",
    "Lets construct an even more complex problem. A [sine curve](https://en.wikipedia.org/wiki/Sine_wave) shows a periodic oscillation, for example, the oscillating voltage along a single phase AC power line. Can we use a neural network to classify different sections of a sine curve? The red class denotes the power up part of the cycle and the blue class represents the power down part with respect to time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sine wave\n",
    "X, y = make_sine_wave()\n",
    "\n",
    "plot_data(X, y, figsize=(10, 8))\n",
    "plt.xlabel(\"Time in milliseconds\")\n",
    "plt.ylabel(\"Voltage (V)\")\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now create a model to recognise these oscillation bends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct standard sequential network\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_shape=(2,), activation='tanh'))\n",
    "model.add(Dense(64, activation='tanh'))\n",
    "model.add(Dense(64, activation='tanh'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile model\n",
    "model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fit\n",
    "history = model.fit(X, y, verbose=0, epochs=50)\n",
    "\n",
    "# Plot decision boundary\n",
    "plot_decision_boundary(lambda x: model.predict(x), X, y, figsize=(12, 9))\n",
    "\n",
    "# Plot accuracy\n",
    "plot_loss_accuracy(history)\n",
    "\n",
    "# Plot metrics and confusion matrix\n",
    "y_pred = model.predict_classes(X, verbose=0)\n",
    "print(classification_report(y, y_pred))\n",
    "plot_confusion_matrix(model, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap and extra resources\n",
    "\n",
    "So far we've used nets to very accurately solve some mathematical problems. You may be wondering about some of the intricacies of how the network works.\n",
    "\n",
    "Some main things to look into in your own time are \n",
    "\n",
    " * [Why are activations useful, what is relu, and why relu is it so powerful?](https://towardsdatascience.com/activation-functions-and-its-types-which-is-better-a9a5310cc8f)\n",
    " * [Why do we use the adam optimiser?](https://towardsdatascience.com/adam-latest-trends-in-deep-learning-optimization-6be9a291375c), if you're feeling really smart you might try the [paper](https://arxiv.org/pdf/1412.6980.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Intro to Computer Vision\n",
    "\n",
    "---\n",
    "[Top](#ML-Week-5---Artificial-Neural-Networks) | [Previous](#Part-5:-Deeper-Networks) | [Next](#Part-7:-Bonus-Content) | [Bottom](#Cheers)\n",
    "\n",
    "One of the strongest applications of deep learning is in computer vision. To illustrate one usecase we'll the employ the [MNIST fashion dataset](https://github.com/zalandoresearch/fashion-mnist) to try and classify what kind an item of clothing is when fed into the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "#Load the prebaked fashion mnist dataset\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets have a little investigation into the prepackaged data. A useful numpy matrix variable is \"shape\", let's use this to tell us how much data we are working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The shape of train image dataset is {}\".format(train_images.shape))\n",
    "print(\"There are {} labels\".format(len(train_labels)))\n",
    "print(\"The shape of test image dataset is {}\".format(test_images.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(train_images[0])\n",
    "plt.colorbar()\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need to do some pre-processing on our data to make it easier to use wtihin our neural network. The following will scale the data from 0-255 to 0-1 so its easier for the network to process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "plt.figure()\n",
    "plt.imshow(train_images[0])\n",
    "plt.colorbar()\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualise some of the data to see what we're working with. As you can see we have black and white images with a resolution of 28 by 28. There are 10 different classes within this network, specifying the type of clothing within the picture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(train_images[i], cmap=plt.cm.binary)\n",
    "    plt.xlabel(class_names[train_labels[i]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to before, we make a simple neural network and use the adam optimiser to train for 5 epochs to classify the type of clothing based upon the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "#Compile model\n",
    "model.compile(optimizer='adam', \n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "#Train the model for 5 epochs\n",
    "model.fit(train_images, train_labels, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how well our model did on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualise the output qualitatively with some class using the following functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image(i, predictions_array, true_label, img):\n",
    "    \"\"\"\n",
    "    Plot an image\n",
    "    \n",
    "    :param i: <int>, the index of the image\n",
    "    :param predictions_array: list<int>, predictions for the images\n",
    "    :param true_label: list<int> the actual image labels\n",
    "    :param img: <np.array>, the actual image\n",
    "    \"\"\"\n",
    "    predictions_array, true_label, img = predictions_array[i], true_label[i], img[i]\n",
    "    plt.grid(False)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    plt.imshow(img, cmap=plt.cm.binary)\n",
    "\n",
    "    predicted_label = np.argmax(predictions_array)\n",
    "    if predicted_label == true_label:\n",
    "        color = 'blue'\n",
    "    else:\n",
    "        color = 'red'\n",
    "\n",
    "    plt.xlabel(\"{} {:2.0f}% ({})\".format(class_names[predicted_label],\n",
    "                                100*np.max(predictions_array),\n",
    "                                class_names[true_label]),\n",
    "                                color=color)\n",
    "\n",
    "def plot_value_array(i, predictions_array, true_label):\n",
    "    \"\"\"\n",
    "    Plot a bar chart of likely classes.\n",
    "    \n",
    "    :param i: <int>, the index of the image\n",
    "    :param predictions_array: list<int>, predictions for the images\n",
    "    :param true_label: list<int> the actual image labels\n",
    "    \"\"\"\n",
    "    predictions_array, true_label = predictions_array[i], true_label[i]\n",
    "    plt.grid(False)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    thisplot = plt.bar(range(10), predictions_array, color=\"#777777\")\n",
    "    plt.ylim([0, 1]) \n",
    "    predicted_label = np.argmax(predictions_array)\n",
    "\n",
    "    thisplot[predicted_label].set_color('red')\n",
    "    thisplot[true_label].set_color('blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try to predict an image's clothing type from the test dataset, and visualise the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab an image from the test dataset\n",
    "test_image_num = 1\n",
    "\n",
    "img = test_images[test_image_num]\n",
    "print(img.shape)\n",
    "# Add the image to a batch where it's the only member.\n",
    "img = (np.expand_dims(img,0))\n",
    "\n",
    "# Make predictions on all images\n",
    "predictions = model.predict(test_images)\n",
    "\n",
    "plt.figure(figsize=(6,3))\n",
    "plot_image(test_image_num, predictions, test_labels, test_images)\n",
    "plt.show()\n",
    "\n",
    "print(img.shape)\n",
    "predictions_single = model.predict(img)\n",
    "\n",
    "# Print the raw prediction matrix\n",
    "print(predictions_single)\n",
    "\n",
    "# Display all the class predictions\n",
    "plot_value_array(0, predictions_single, test_labels)\n",
    "_ = plt.xticks(range(10), class_names, rotation=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Bonus Content\n",
    "\n",
    "---\n",
    "[Top](#ML-Week-5---Artificial-Neural-Networks) | [Previous](#Part-6:-Intro-to-Computer-Vision) | [Next](#Cheers) | [Bottom](#Cheers)\n",
    "\n",
    "Here's some more about computer vision processes.\n",
    "\n",
    "<img src=\"img/vision.png\" width=\"661\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cheers\n",
    "\n",
    "---\n",
    "[Top](#ML-Week-5---Artificial-Neural-Networks) | [Previous](#Part-7:-Bonus-Content) | [Next](#Cheers) | [Bottom](#Cheers)\n",
    "\n",
    "Any questions hit me up my AI relevant experience is particularly in:\n",
    "* Deep neural networks and their varieties\n",
    "* Computer vision, classical and deep\n",
    "* Robotics and self driving vehicles\n",
    "\n",
    "**Please complete the week 5 survey here**: [bit.ly/ml_week_5_survey](https://docs.google.com/forms/d/e/1FAIpQLSeW4SELSovBB0iIo0gi42pFQEDSJxZB46gJN4NaSCNuVVeJLw/viewform?usp=sf_link)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
