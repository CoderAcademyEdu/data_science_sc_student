{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Python - Lesson 4\n",
    "\n",
    "This lesson will cover the following topics:\n",
    "\n",
    "* Libraries\n",
    "* Numpy\n",
    "* Pandas\n",
    "\n",
    "It is assumed you have basic knowledge of the following:\n",
    "\n",
    "* Data types\n",
    "* Variables\n",
    "* Arithmetic\n",
    "* Conditional statements/boolean operators ('==', 'and', 'or', etc.)\n",
    "* Lists/dictionaries\n",
    "\n",
    "It is recommended you have knowledge of the following:\n",
    "\n",
    "* Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "When we begin a session of Python, there are certain libraries and functionality that are available to use, as programmers, and there are some functionality that we have to import in. For instance, we can automatically use the `sum()` function to summarise a Python list.\n",
    "\n",
    "Run the cell below which will sum the numbers in the list, `my_numbers_list`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create my_numbers_list\n",
    "my_numbers_list = [1, 2, 3, 4]\n",
    "\n",
    "# Print the sum\n",
    "print(sum(my_numbers_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often times, there is additional functionality that we do not have available to use at the _beginning_ of the Python session, but we can make available by `import`'ing in the additional functionality.\n",
    "\n",
    "We can do this using an `import` statement, which will `import` a library of code. Food for thought...why would Python not have all libraries _always_ available?\n",
    "\n",
    "Run the following code which will complete the following steps...\n",
    "\n",
    "* Import the `math` library\n",
    "* Take the square root of the number `4`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import\n",
    "import math\n",
    "\n",
    "# Take the square root\n",
    "math.sqrt(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will notice that we have to call the function using `math.sqrt()` over simply `sqrt()`. This is because we are saying that the `sqrt()` function is a part of the `math` library.\n",
    "\n",
    "We can import a specific function in the `math` library by using the `from math import sqrt` statement. Now we can take the square root of a number using the `sqrt()` function directly, instead of needing to call `math.sqrt()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import\n",
    "from math import sqrt\n",
    "\n",
    "# Take the square root\n",
    "sqrt(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** You can import _all_ functions from the `math` library using the `from math import *` statement. Another thought exercise...what is the advantage of using `import math` vs `from math import *`?\n",
    "\n",
    "Here are all the functions in the math library: https://docs.python.org/3/library/math.html\n",
    "\n",
    "Here are some other examples of using a variety of functions from the math library, after calling the `from math import *` statement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import\n",
    "from math import *\n",
    "\n",
    "# Take the exponent of 0, i.e. e^0 = 1\n",
    "print(exp(0))\n",
    "\n",
    "# Take the natural log of 1, which is 0\n",
    "print(log(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NumPy\n",
    "\n",
    "NumPy is one of the most popular mathematical libraries in Python. You can take a look at the NumPy documentation to see the variety of different scientific computing capabilties within NumPy: https://docs.scipy.org/doc/numpy/reference/routines.html\n",
    "\n",
    "We are going to demonstrate some of the basic functionality of NumPy, namely...\n",
    "\n",
    "* Arrays in NumPy\n",
    "* Matrices in NumPy\n",
    "\n",
    "We can import numpy using the following `import` statement. Note that we are using the `as` to give numpy a \"nickname\". Many Python libraries have common nicknames, and `numpy` is typically given the nickname `np`.\n",
    "\n",
    "Run the following cell to improt `numpy` and give it the nickname `np`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arrays\n",
    "\n",
    "Numpy arrays are similar to one-dimensional lists. The difference is that a list can store multiple data types (strings, floats), and arrays can store one data type. What do you think is the advantage of arrays over lists?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is the creation of a list\n",
    "my_list = [1.0, 'hello']\n",
    "\n",
    "# Here is the creation of a numpy array\n",
    "my_array = np.array([1.0, 'hello'])\n",
    "\n",
    "\n",
    "# Print both...what changes in the array vs. the list?\n",
    "print(my_list)\n",
    "print(my_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we have an array of numbers that represents the fluctuation of a stock throughout a week (this is the actual Adj Close prices of Apple from Feb 11th - Feb 15th). We can utilise numpy to calcualte different statistics based upon the stock prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stock prices for one week\n",
    "stock_prices = np.array([169.429993, 170.889999, 170.179993, 170.800003, 170.419998])\n",
    "\n",
    "# Calculate the mean stock price\n",
    "print(np.mean(stock_prices))\n",
    "\n",
    "# Calculate the standard deviation stock price\n",
    "print(np.std(stock_prices))\n",
    "\n",
    "# Calculate the median stock price\n",
    "print(np.median(stock_prices))\n",
    "\n",
    "# Calculate the Inner Quartile Range, or IQR of the data\n",
    "print(np.percentile(stock_prices, q=75) - np.percentile(stock_prices, q=25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you need a refresher on what **mean**, **median**, and **standard deviation** are, here is a good tutorial on...\n",
    "\n",
    "* Mean and median (also called **central tendency**): https://statistics.laerd.com/statistical-guides/measures-central-tendency-mean-mode-median.php\n",
    "* Standard deviation and IQR (also called **spread**): https://statistics.laerd.com/statistical-guides/measures-of-spread-range-quartiles.php\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrices\n",
    "\n",
    "Matrices are 2-dimensional data structures. Matrices can be thought of as a form of **tabular data**.\n",
    "\n",
    "For instance, we might want to represent stock prices of Apple throughout an entire month in a table, where the first column are all stock prices for Monday, second column is Tuesday, etc. Let's do this for four total weeks of data.\n",
    "\n",
    "These are the stocks for Apple from 11th of February to the 8th of March. Note the `np.nan` representing Monday, 18th of February, which was a public holiday in the U.S."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intialise the matrix (or, as you can see a 2-D np.array)\n",
    "apple_stock = np.array([ # Note the opening bracket\n",
    "    [171.050003, 170.100006, 171.389999, 169.710007, 171.25],\n",
    "    [np.nan, 169.710007, 171.190002, 171.800003, 171.580002], \n",
    "    [174.160004, 173.710007, 173.210007, 174.320007, 174.279999], \n",
    "    [175.690002, 175.940002, 174.669998, 173.869995, 170.320007]\n",
    "]) # and hte final closing bracket\n",
    "\n",
    "print(apple_stock)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrix indexing\n",
    "\n",
    "We can index the array using the following...\n",
    "\n",
    "* If we want to pull out a certain row of data, we can utilise the syntax `my_matrix[row_number, :]`, for example `my_matrix[1, :]` would say take the first index (second row) of data, and all columns\n",
    "* If we want to pull out a certain column of data, we can utilise the syntax `my_matrix[:, column_number]`, for example `my_matrix[:, 2]` would say take the second index (third column) of data, and all rows\n",
    "\n",
    "**REMINDER**: Python uses **zero-based** indexing!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the second row\n",
    "print(apple_stock[1, :])\n",
    "\n",
    "# Get the third column\n",
    "print(apple_stock[:, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also summarise data based upon rows and columns. For instance, we could to grab the _mean_ stock price per day of the week by taking the mean _across_ rows (i.e. for each column) by:\n",
    "\n",
    "* Using the np.mean() function: https://docs.scipy.org/doc/numpy/reference/generated/numpy.mean.html\n",
    "* Using the `axis` parameter. `axis=0` takes the mean _across rows_. `axis=1` takes the mean _across columns_.\n",
    "\n",
    "Here is an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the mean across rows (mean per each day of the week)\n",
    "print(np.mean(apple_stock, axis=0))\n",
    "\n",
    "# Take the mean across columns (mean per week)\n",
    "print(np.mean(apple_stock, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note...what happened to the row that had the `np.nan` value? This is a data cleaning problem. You should perform your own investigation on how to take the mean by eliminating the `np.nan`. Thought...\n",
    "\n",
    "If we are taking the mean across all Monday's (first column) in our matrix, and we have the `np.nan`, should we be taking the mean across 4 Monday's (including `np.nan`), or three Monday's (mean without the `np.nan`)?\n",
    "\n",
    "There are many more things you can do with numpy which we will not cover here. We recommend investigating in your own time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas\n",
    "\n",
    "Pandas stands for **Panel - Data**...with an \"s\" (Coders and acronyms are not great...but hey it's a fun name right??).\n",
    "\n",
    "\n",
    "![Panda](https://media.giphy.com/media/DD2NmqYhvLiP6/giphy.gif)\n",
    "\n",
    "\n",
    "Pandas is the main tabular data library in Python, and is very popular in data science. You can find the pandas documentation here: https://pandas.pydata.org/\n",
    "\n",
    "Like numpy, we typically import pandas and give it a nickname, called `pd`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can upload different types of tabular data (excel, csv files etc.) to pandas. For instance, to upload a csv file to a variable in Python, you can use the following command:\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "my_data = pd.read_csv(path_to_file)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "The `path_to_file` can be...\n",
    "\n",
    "* A path to your file on your computer (if this notebook was saved on your computer)\n",
    "* A location of a dataset on the internet\n",
    "\n",
    "For convenience, we'll use a dataset on the internet, specifically the [Iris dataset](https://archive.ics.uci.edu/ml/datasets/iris)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load data into variable\n",
    "filename = 'https://raw.githubusercontent.com/uiuc-cse/data-fa14/gh-pages/data/iris.csv'\n",
    "iris_data = pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we display the datatype of the `iris_data` variable, you can see that the variable is a **DataFrame**. A DataFrame is the type of object pandas stores tabular data in. I will refer to a dataframe in the following examples using the abbreviation **`df`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(iris_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with exploring any new dataset, it's always good to learn a little bit about the data we just downloaded to Python.\n",
    "\n",
    "The `df.head()` command allows us to see the first five rows of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```df.info()``` command gives information about the total number of entries in a DataFrame, column data types and the count of values in each column. This comes in handy during exploratory data analysis to indentify the columns with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `df.dtypes` command gives information about the columns, and the type of data within each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `df.desribe()` method gives summary information to the numerical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `df.shape` command gives the number of rows and columns of a dataframe in a tuple object of the format `(rows, columns)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** As you can tell, we are covering a lot of commands in this lesson. I highly recommend taking some time outside of the classroom to explore pandas examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas dataframes can be index'ed similarly to numpy. The main difference is, in addition to using _numerical_ indexing, pandas can be index'ed using the _names_ of its headers and rows.\n",
    "\n",
    "For example, the following will grab the first five rows of the `species` column in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_data['species'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to grab _multiple_ columns, we can include the names of the columns we want to use in a list.\n",
    "\n",
    "For example...\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "df[[col_1, col_2, col_3, ...]]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "See this yourself. Run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_data[['petal_length', 'petal_width']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also index rows. By default, unless specified, the row names are numbers, beginning at 0, and ending at the number of rows in the dataframe - 1.\n",
    "\n",
    "In the last cell, the first column displayed (with numbers 0 - 4) are the index.\n",
    "\n",
    "To index by both row _and_ column names, we need to use the `df.loc[rows, columns]` command. For instance, the following code grabs the element from the 3rd row (index 2), and petal_width column..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_data.loc[2, 'petal_width']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same rules apply as column names if we want to grab multiple rows and columns. We have to enclose the multiple rows in a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_data.loc[[0, 2], ['petal_width', 'petal_length']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Groupby\n",
    "\n",
    "Often we want to summarise data over specific categorical values. For instance, maybe we want to take the mean petal_width and petal_length for each species. The `groupby` method allows us to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.groupby([categorical_column])[[col 1 to summarise, col 2 to summarise]].summary_function()\n",
    "iris_data.groupby(['species'])[['petal_width', 'petal_length']].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the \"species\" became the index of the resulting dataframe. There is a `as_index` parameter that can be modified if we do not want to have the grouping column as the index. \n",
    "\n",
    "You can see the difference in the following output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.groupby([categorical_column], as_index=False)[[col 1 to summarise, col 2 to summarise]].summary_function()\n",
    "iris_data.groupby(['species'], as_index=False)[['petal_width', 'petal_length']].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible we want to add different summary statistics. We can do this using the `agg` function.\n",
    "\n",
    "The following code takes the mean and median over the petal_width and petal_length column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the mean and median\n",
    "iris_data.groupby(['species'], as_index=False)[['petal_width', 'petal_length']].agg(['mean', 'median'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another possibility is we might want to perform a summary for one column (e.g. take the mean of the petal_width), and a different summary for another column (e.g. take the median of petal_length). We can continue to use the `agg` function to do this, with a dictionary instead of a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the mean of petal_width and the median of petal_length\n",
    "iris_data.groupby(['species'], as_index=False).agg({'petal_width': 'mean', 'petal_length': 'median'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It might be helpful to play around with different combinations of groupby with multiple columns as well as agg."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often times we have data in two separate DataFrames, and we want to merge the data together based upon the columns that share similar data, and align like rows in the data.\n",
    "\n",
    "We can use the `pd.merge()` function to do this. Merges are similar to [SQL Joins](https://www.w3schools.com/sql/sql_join.asp) if you are familiar with that concept.\n",
    "\n",
    "Run the following cells to create two dataframes.\n",
    "\n",
    "* postal_codes holds information about people and what post code they live in\n",
    "* order_id holds information about people and an order they made on the internet\n",
    "\n",
    "We will not go over the specific syntax of creating these tables from dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make a postal code df\n",
    "postal_codes = pd.DataFrame({\n",
    "    'Name': ['Saad', 'Sally', 'Dan', 'Gretchen'],\n",
    "    'Postal Code': ['2000', '2001', '2002', '2000']\n",
    "})\n",
    "\n",
    "# Make an order id df\n",
    "order_id = pd.DataFrame({\n",
    "    'Name': ['Saad', 'Nick', 'Saad', 'James', 'Steph', 'Gretchen'],\n",
    "    'Order ID': [10001, 10002, 10003, 10004, 10005, 10006]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postal_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can tell, the `Name` column contains similar information in both dataframes.\n",
    "\n",
    "Let us say we want to merge together the dataframes, keeping only names that are in _both_ dataframes. This is how we would complete that task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge\n",
    "inner_merge = pd.merge(\n",
    "    left=postal_codes,\n",
    "    right=order_id,\n",
    "    how='inner',\n",
    "    on='Name'\n",
    ")\n",
    "\n",
    "inner_merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To break down the commmand...\n",
    "\n",
    "1. We use the pd.merge() function\n",
    "2. The left and right parameters dictate the two dataframes we want to merge\n",
    "3. `how='inner'` states to keep _only_ the names that appear in both DataFrames. This is the same as an \"inner join\" in SQL\n",
    "4. `on='Name'` described the column that contais the _same information_ in both DataFrames\n",
    "\n",
    "Sometimes, we want to keep _all_ the information that is in one of the two DataFrames. For instance, maybe we want to retain the information of everyone's postal code in the merged DataFrame, and show who did not have any orders.\n",
    "\n",
    "We can do this by change the `how` parameter to `how='left'`, which means \"keep _all_ the information on the left-hand DataFrame.\n",
    "\n",
    "Here is an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge\n",
    "left_merge = pd.merge(\n",
    "    left=postal_codes,\n",
    "    right=order_id,\n",
    "    how='left',\n",
    "    on='Name'\n",
    ")\n",
    "\n",
    "left_merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the `NaN` in the rows where there is no match.\n",
    "\n",
    "If we changed `how='left'` to `how='right'`, we would keep the information on the right-hand DataFrame, but not the left.\n",
    "\n",
    "The last type of merge keeps _all_ information in _both_ dataframes. This can be seen by changing `how='outer'`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge\n",
    "outer_merge = pd.merge(\n",
    "    left=postal_codes,\n",
    "    right=order_id,\n",
    "    how='outer',\n",
    "    on='Name'\n",
    ")\n",
    "\n",
    "outer_merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a visual to help with types of merges (also called joins)...\n",
    "\n",
    "\n",
    "![Joins](https://www.dofactory.com/Images/sql-joins.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other useful commands we did not cover\n",
    "\n",
    "There are a ton of other commands that are used frequently in Pandas that were not covered. It might be helpful to look at the following...\n",
    "\n",
    "* df.rename(): https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.rename.html\n",
    "* df.drop(): https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html\n",
    "* df.concat(): https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.concat.html\n",
    "* df.map(): https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.map.html\n",
    "* df.apply(): https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.apply.html\n",
    "* df.get_dummies(): https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html\n",
    "\n",
    "It might also be helpful to checkout the pd.Series object: https://pandas.pydata.org/pandas-docs/version/0.22/generated/pandas.Series.html\n",
    "\n",
    "Series are Pandas' one-dimensional equivalent to a dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 1\n",
    "\n",
    "In this challenge, we'll summarise different aspects of the **iris dataset** we have been using.\n",
    "\n",
    "Complete the following in the code cell below.\n",
    "\n",
    "* What is the min/median/max/average/standard deviation of \"petal_width\" column? _Hint: Use Google_.\n",
    "* How many types of iris plant species are within the dataset? Use [this](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.unique.html).\n",
    "* Calculate the ratio of sepal_length to sepal_width, and the ratio of petal_length to petal_width. Store each of these calculations respectively in columns called sepal_ratio and petal_ratio. You can use arthimetic to perform row-by-row operations, for example df['row1'] / df['row2'].\n",
    "\n",
    "You can store a new column using:\n",
    "\n",
    "```python\n",
    "df['my_new_col'] = # Insert manipulation here\n",
    "```\n",
    "\n",
    "* Store the columns, except for species, in a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Insert code here. You can create more cells below if you would like\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2\n",
    "\n",
    "Continue to use the Iris Dataset. Complete the following in the code cell below.\n",
    "\n",
    "We often [_normalise data, or standardise_](https://www.statisticshowto.datasciencecentral.com/normalized/) data to be able to compare the distribution of things that are on different scales. For instance, it is hard to compare the variance of data that represents the height of two people that are the same age but _different_ sexes because there are biological differences that contribute to different height distributions per sex. If we standardise this data, we can place the distributions on the same scale.\n",
    "\n",
    "In this exercise and the following we will use **groupby** and **merge** to normalise and standardise our data.\n",
    "\n",
    "* Use groupby to calculate _per species_ the petal_width...\n",
    "    - mean\n",
    "    - standard deviation\n",
    "    - min\n",
    "    - max\n",
    "    \n",
    "    **Make sure** to save the result in a dataframe called **iris_data_summaries**.\n",
    "\n",
    "\n",
    "* Rename the columns in the resulting dataframe. **iris_data_summaries.columns** is a list of the column names. You can assign this to a new list, for example `['petal_width_mean', 'petal_width_min', 'petal_width_max', 'petal_width_std']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Insert code here. You can create more cells below if you would like\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3\n",
    "\n",
    "Let's continue the following exercise using the **iris_data_summaries** DataFrame we just created.\n",
    "\n",
    "* _Left_ merge the `iris_data` summary table with the original `iris_data_summaries` _by `species`_ to create a table that looks like the following...\n",
    "\n",
    "| species |\tpetal_width |\tpetal_width_mean |\tpetal_width_min | petal_width_max |\tpetal_width_std |\n",
    "|---------|-------------|--------------------|------------------|-----------------|-----------------|\n",
    "| setosa  | 0.2     \t|              0.244 |             \t0.1 |          \t0.6   |0.10721          |\n",
    "| ...  | ...     \t|              ... |             \t... |          \t...   |..          |\n",
    "\n",
    "\n",
    "* Standardise the data by creating a new column called `petal_width_standard`, using the following formula...\n",
    "\n",
    "\n",
    "$$ \\frac{petal\\_width - petal\\_width\\_mean}{petal\\_width\\_std}$$\n",
    "\n",
    "\n",
    "\n",
    "* Normalise the data by creating a new column called `petal_width_normal`, using the following formula...\n",
    "\n",
    "$$ \\frac{petal\\_width - petal\\_width\\_min}{petal\\_width\\_max - petal\\_width\\_min}$$\n",
    "\n",
    "\n",
    "**NOTE** The previous formulas should NOT be copied directly into python. For instance the following code WILL RESULT IN AN ERROR!\n",
    "\n",
    "```python\n",
    "# THIS IS BAD CODE\n",
    "df['petal_width' - 'petal_width_std']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Insert code here. You can create more cells below if you would like\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 4\n",
    "\n",
    "Let's realise the value in what we just did.\n",
    "\n",
    "Calculate...\n",
    "\n",
    "* The _by species_ mean and std of `petal_width` and `petal_width_standard`\n",
    "* The _by species_ min and max of `petal_width` and `petal_width_normal`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Insert code here. You can create more cells below if you would like\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the notebook\n",
    "\n",
    "If you would like to retain your work, please follow the following directions:\n",
    "\n",
    "* On the top of this screen, in the header menu, click \"File\", then \"Download as\" and then \"Notebook\".\n",
    "\n",
    "* You will need to download [Python 3.7 with Anaconda](https://www.anaconda.com/distribution/) to use this in the future."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
