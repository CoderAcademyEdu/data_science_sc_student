{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coder Academy and Qantas Masterclass\n",
    "\n",
    "\n",
    "This notebook has the following sections...\n",
    "\n",
    "* [Part 1: Problem introduction](#Part-1:-Problem-introduction)\n",
    "* [Part 2: Using the notebook](#Part-2:-Using-the-notebook)\n",
    "* [Part 3: Background on image processing](#Part-3:-Background-on-image-processing)\n",
    "* [Part 4: Convolutional neural networks](#Part-4:-Convolutional-neural-networks)\n",
    "* [Part 5: Creating geographic boundaries](#Part-5:-Creating-geographic-boundaries)\n",
    "* [Part 6: Building the model](#Part-6:-Building-the-model)\n",
    "\n",
    "<br>\n",
    "\n",
    "![](https://cdn.newsapi.com.au/image/v1/102d01fda4089f60b42be982ee172c81)\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Problem introduction\n",
    "\n",
    "[Top](#Coder-Academy-and-Qantas-Masterclass) | [Previous section](#Coder-Academy-and-Qantas-Masterclass) | [Next section](#Part-2:-Using-the-notebook) | [Bottom](#Wrap-up)\n",
    "\n",
    "### What are we trying to do?\n",
    "\n",
    "As explained, [Qantas](qantas.com.au) serves a number of destinations around the world, many of which might _peak_ the interests of similar travelers.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"http://www.waitakinz.com/assets/Tourism-Operators/images/_resampled/ScaleHeightWyI1NTAiXQ/p-25EA0C05-D4C6-FFD9-45298B90A92328C9-2544003.jpg\" style=\"width:500px; height:300px\"></td>\n",
    "        <td><img src=\"https://static.independent.co.uk/s3fs-public/thumbnails/image/2018/06/27/15/gettyimages-699088407.jpg\" style=\"width:500px; height:300px\"></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><span style=\"font-size:16px\">Do you like traveling to the Southern Alps??</span></td>\n",
    "        <td><span style=\"font-size:16px\">Why not try the Swiss Alps...and fly with Qantas, or a partner?</span></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "It would be amazing if Qantas could identify by a picture _where_ someone was traveling, and potentially recommend them to travel to a _different_ but _similar_ location.\n",
    "\n",
    "In this workshop, we'll create an algorithm to **classify images based upon their location**. We'll then see how this algorithm could be used to recommend _new_ destinations based upon its similarity to the classified images.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "<img src=\"../img/Qantas_Classification_Biz.png\" width=\"750\">\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Data Pipeline\n",
    "\n",
    "In this session, we'll build a **data pipeline** to build an image relationship model. From [wikipedia](https://en.wikipedia.org/wiki/Pipeline_(computing))...\n",
    "\n",
    "> In computing, a **pipeline** (also known as a **data pipeline**) is a set of data processing elements connected in series, where the output of one element is the input of the next one. The elements of a pipeline are often executed in parallel or in time-sliced fashion.\n",
    "\n",
    "Data pipelines deal with the process of collecting, modifying and analysing a dataset towards some goal. Here's a picture of a data pipeline from [this medium blog](https://medium.com/the-data-experience/building-a-data-pipeline-from-scratch-32b712cfb1db)...\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/1600/1*8-NNHZhRVb5EPHK5iin92Q.png)\n",
    "\n",
    "For the rest of this lesson we'll build-up this pipeline. We will....\n",
    "\n",
    "1. Analyse an image, and learn about how computers **think about images**\n",
    "2. Develop a way **differentiate images by location**\n",
    "3. Create a **classification algorithm** to be able to predict _where_ an image comes from\n",
    "4. See our **prediction algorithm live!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Using the notebook\n",
    "\n",
    "[Top](#Coder-Academy-and-Qantas-Masterclass) | [Previous section](#Part-1:-Problem-introduction) | [Next section](#Part-3:-Background-on-image-processing) | [Bottom](#Wrap-up)\n",
    "\n",
    "### What is Python?\n",
    "\n",
    "Python is an _interpretive_ programming language invented in the 1980s. It's actually named after Monty Python and Holy Grail. In this class we'll be using Python to build our machine learning algorithms. \n",
    "\n",
    "#### Why learn Python?\n",
    "\n",
    "Python has gained popularity because it has an easier syntax (rules to follow while coding) than many other programming languages. Python is very diverse in its applications which has led to its adoption in areas such as data science and web development.\n",
    "\n",
    "All of the following companies actively use Python:\n",
    "\n",
    "![Image](https://www.probytes.net/wp-content/uploads/2018/08/appl.png)\n",
    "\n",
    "### How do I interact with this notebook?\n",
    "\n",
    "A Jupyter Notebook is an interactive way to work with code in a web browser. Jupyter is a pseudo-acronym for three programming languages: Julia, python and (e)r. Notebooks provide a format to add instructions + code in one file, which is why we're using it!\n",
    "\n",
    "We'll quickly do some practice to introduce you how to use this notebook. For a list of keyboard shortcuts you can take a look at [Max Melnick's](http://maxmelnick.com/2016/04/19/python-beginner-tips-and-tricks.html) beginner tips for Jupyter Notebook.\n",
    "\n",
    "Here's a quick run down of some of the most basic commands to use:\n",
    "\n",
    "- A cell with a **<span style=\"color:blue\">blue</span>** background is in **Command Mode**. This will allow you to toggle up/down cells using the arrow keys. You can press enter/return on a cell in command mode to enter edit mode\n",
    "\n",
    "- A cell with a **<span style=\"color:green\">green</span>** background is in **Edit Mode**. This will allow you to change the content of cells. You can press the escape key on a cell in command mode to enter edit mode\n",
    "\n",
    "- To run the contents of a cell, you can type:\n",
    "  - `cmd + enter`, which will run the cotents of a cell and keep the cursor in place\n",
    "  - `shift + enter`, which will run the contents of a cell, and move the cursor to the next cell (or create a new cell)\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Edit the below by changing \"Gretchen\" to your own name by entering edit mode, and then running the cell using the directions above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hello, Gretchen!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add/delete cells using the following commands in <span style=\"color:blue\">**Command Mode**</span>:\n",
    "\n",
    "- `a`, adds a cell above the current cell\n",
    "- `b`, adds a cell below the current cell\n",
    "- `d + d`, (pressing the \"d\" key twice in succession) deletes a cell\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Add/delete the cells such that each individual cell prints the numbers 1-5 in order. The numbers 2 and 4 are already completed for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we dig deep into the problem. Let's import some **modules** that will help us throughout the rest of the masterclass.\n",
    "\n",
    "> For those who do not know, a **module** or **library** is a set of python code-files that bring new capabilities into our programs. We need to `import` modules into our session, because not all python modules are available when we begin our notebook session. Thus, we can import exactly what we need for the specific code we create.\n",
    "\n",
    "In case you want to know more about the modules we will utilise today, here's a quick table.\n",
    "\n",
    "| Module name | Description |\n",
    "|-------------|-------------|\n",
    "| [numpy](https://www.numpy.org/) | A library for numerical and mathematical manipulation in Python |\n",
    "| [scipy](https://www.scipy.org/) | Contains many modules for scientific computing in Python |\n",
    "| [matplotlib](https://matplotlib.org/) | Creates plots and visualisations |\n",
    "| [PIL](https://pillow.readthedocs.io/en/stable/) | Allows for image processing and manipulation |\n",
    "| [sklearn](https://scikit-learn.org/) | Contains functions and methods for the creation and analysis of machine learning algorithms  |\n",
    "| [keras](https://keras.io/) | A library that allows for manipulations of neural networks, powered by other libraries, such as [Tensorflow](https://www.tensorflow.org/) |\n",
    "| [gzip](https://docs.python.org/3/library/gzip.html) | Allows for the compression and decompression of files |\n",
    "| [json](https://docs.python.org/3/library/json.html) | Encodes and decodes json files |\n",
    "| [requests](https://2.python-requests.org/en/master/) | Creates an interface to send HTTP requests in Python |\n",
    "| [urllib](https://docs.python.org/3/library/urllib.html) | Provides an interface for working with URLs |\n",
    "| [pandas](https://pandas.pydata.org/) | A Python library for manipulating tabular data |\n",
    "| [pyqtree](https://github.com/karimbahgat/Pyqtree) | Creates an efficient way for manipulating geographic data |\n",
    "| [time](https://docs.python.org/3/library/time.html) | Library for monitoring code runtime | \n",
    "| [os](https://docs.python.org/3/library/os.html) | Provides methods for accessing operating system information |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scipy and numpy\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "from scipy import ndimage\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from PIL import Image\n",
    "%matplotlib inline\n",
    "\n",
    "# Sklearn \n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Keras\n",
    "import keras\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from keras.layers import Dropout\n",
    "from keras.models import Model\n",
    "from keras.layers import Conv2D, GlobalAveragePooling2D\n",
    "\n",
    "# Requests,json and URL libraries\n",
    "from gzip import decompress\n",
    "from json import loads\n",
    "from requests import get\n",
    "import urllib\n",
    "\n",
    "# Pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Pyqtree\n",
    "from pyqtree import Index\n",
    "\n",
    "# Time\n",
    "import time\n",
    "\n",
    "# OS\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Background on image processing\n",
    "\n",
    "[Top](#Coder-Academy-and-Qantas-Masterclass) | [Previous section](#Part-2:-Using-the-notebook) | [Next section](#Part-4:-Convolutional-neural-networks) | [Bottom](#Wrap-up)\n",
    "\n",
    "### What is an image...in the mind of a computer?\n",
    "\n",
    "Let's start to get a feel about how a computer can process an image. To do that, we'll upload a single image into our Python notebook, and analyse some characteristics of the image. We'll use the [keras](https://keras.io/) library, to upload our image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the image\n",
    "image_file = '../data/Masterclass_Images/32704191297_81ba56ef37.jpg'\n",
    "img = image.load_img(image_file)\n",
    "\n",
    "# Make the image an array\n",
    "img_array = image.img_to_array(img)\n",
    "\n",
    "# Print the array shape\n",
    "print('The image shape: ')\n",
    "print(img_array.shape)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Print a section of the array and then a specific pixel\n",
    "print('A subsection of the image array: ')\n",
    "print(img_array[0:10, 0:10, 0])\n",
    "print(\"\\n\")\n",
    "\n",
    "print('A single pixel: ')\n",
    "print(img_array[-1, -1, :])\n",
    "print(\"\\n\")\n",
    "\n",
    "# Display the image\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code did something a little funky here...it converted the image to an array of numbers, something that looked like this...\n",
    "\n",
    "---\n",
    "\n",
    "A subsection of the image array: \n",
    "```python\n",
    "[[ 59.  44.  47.  52.  52.  55.  52.  52.  50.  56.]\n",
    " [ 51.  53.  53.  49.  50.  54.  54.  57.  55.  46.]\n",
    " [ 49.  56.  51.  51.  57.  55.  52.  52.  52.  51.]\n",
    " [ 52.  53.  49.  55.  57.  52.  54.  54.  53.  59.]\n",
    " [ 53.  51.  53.  56.  49.  50.  57.  55.  53.  38.]\n",
    " [ 54.  50.  56.  53.  51.  57.  52.  45.  57.  95.]\n",
    " [ 56.  50.  56.  51.  57.  59.  50.  75. 142.  89.]\n",
    " [ 56.  52.  59.  50.  54.  49.  58. 139.  37. 118.]\n",
    " [ 70.  74.  67.  54.  52.  37. 108.  80.  96. 160.]\n",
    " [ 60.  80.  71.  68.  61.  62. 124.  88. 140. 140.]]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "It also printed out a single **pixel** in the image. Something that looked like this...\n",
    "\n",
    "---\n",
    "\n",
    "```\n",
    "A single pixel: \n",
    "[102. 131. 147.]\n",
    "```\n",
    "---\n",
    "\n",
    "Let's dig into this a little bit more.\n",
    "\n",
    "#### Anatomy of an image\n",
    "\n",
    "The following picture visualises how a computer thinks about an image.\n",
    "\n",
    "---\n",
    "\n",
    "![](../img/Image_Anatomy.png)\n",
    "\n",
    "---\n",
    "\n",
    "Let's break this down.\n",
    "\n",
    "* The square in the bottom right-hand corner represents a single point in the image. We call each point in an image a **pixel**. Pixels are like tiny little squares, each with an individual colour.\n",
    "* All colours in an image can be represented by three numbers. The numbers represent the portion of a pixel that is **red, blue, or green**. We call these **RGB values**.\n",
    "  * The reason red, green and blue were chosen is because these are additive primary colours, meaning we can make any other colours from these three primary colours.\n",
    "  * Each number for R, G or B, will be in the range 0-255, meaning there are 256 distinct possibilities. The reason there are 256 actually is based upon how a computer stores this value in memory (it uses an 8-bit number).\n",
    "* The image has a **size**, that dictates how many of these RGB values there are. In this image there are **281 rows, and 500 columns** of pixels.\n",
    "\n",
    "### Convolutions\n",
    "\n",
    "#### How do computers differentiate between images?\n",
    "\n",
    "Let's take a look of two different images, one from the Sydney Opera house, and the other of the Southern Alps.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"../data/Masterclass_Images/32805115817_3ce92c9bd4.jpg\" style=\"width:400px; height:300px\"></td>\n",
    "        <td><img src=\"../data/Masterclass_Images/32704191297_81ba56ef37.jpg\" style=\"width:400px; height:300px\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "---\n",
    "\n",
    "### Thought exercise\n",
    "\n",
    "Using the ideas of **pixels** and **colours** as before. Re-draw these images on two pieces of paper, in the **simplest form possible**. Try to ask yourself...what are the **features** that distinguish these two images? Think about...\n",
    "\n",
    "* Distinct **shapes** in an image\n",
    "* Distinct **colours** in an image\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Let's see how a computer can translate the shapes into an image into a simpler form. Run the following code which will apply a **filter** to an image. What's the filter doing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add filter\n",
    "filter_1 = [\n",
    "    [-1, 0, 1],\n",
    "    [-2, 0, 2],\n",
    "    [-1, 0, 1]\n",
    "]\n",
    "\n",
    "# Convert to black and white\n",
    "bw = np.dot(img_array, [0.21, 0.72, 0.07]).astype(np.uint8)\n",
    "\n",
    "# Convolve\n",
    "output = ndimage.convolve(bw, filter_1, mode='constant')\n",
    "\n",
    "\n",
    "# Show\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(output, cmap='Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's another filter. What's going on in this one?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add filter\n",
    "filter_2 = [\n",
    "    [-1, -2, -1],\n",
    "    [0, 0, 0],\n",
    "    [1, 2, 1]\n",
    "]\n",
    "\n",
    "\n",
    "# Convolve\n",
    "output = ndimage.convolve(bw, filter_2, mode='constant')\n",
    "\n",
    "# Show\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(output, cmap='Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution\n",
    "\n",
    "These filters are using an operation called **convolution**. Convolution is a complex operation, but it allows us to change and filter an image to specific important parts. Here's how convolution works, mathematically.\n",
    "\n",
    "\n",
    "![](https://media.giphy.com/media/i4NjAwytgIRDW/giphy.gif)\n",
    "\n",
    "\n",
    "As you can see, there is a sliding orange **array or matrix** that moves across an image. At each point, the image pixels are being multiplied by the small numbers, and summed together. The result of this sliding matrix is posted on the right hand side.\n",
    "\n",
    "We call the moving array the **kernel** of the image. In our examples of above, we used two kernels, written below.\n",
    "\n",
    "\n",
    "```python\n",
    "filter_1 = [\n",
    "    [-1, 0, 1],\n",
    "    [-2, 0, 2],\n",
    "    [-1, 0, 1]\n",
    "]\n",
    "\n",
    "filter_2 = [\n",
    "    [-1, -2, -1],\n",
    "    [0, 0, 0],\n",
    "    [1, 2, 1]\n",
    "]\n",
    "```\n",
    "\n",
    "A lot of image editing software uses convolution. [Here's a great link](http://setosa.io/ev/image-kernels/) to show you different kinds of filters.\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Play with the code below to apply different kernels to the image. See what each kernel does. Feel free to change-up the filters and create your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create filters\n",
    "filters = [\n",
    "    [\n",
    "        [0.0625, 0.125, 0.0625],\n",
    "        [0.125, 0.25, 0.125],\n",
    "        [0.0625, 0.125, 0.0625]\n",
    "    ],\n",
    "    [\n",
    "        [0, -1, 0],\n",
    "        [-1, 5, -1],\n",
    "        [0, -1, 0]\n",
    "    ],\n",
    "    # Insert your filter here\n",
    "    [\n",
    "        [0, 0, 0],\n",
    "        [0, 1, 0],\n",
    "        [0, 0, 0]\n",
    "    ]\n",
    "]\n",
    "\n",
    "# CHANGE \"filter_type = 0\" to \"filter_type = 1\" or \"filter_type = 2\" to change the filter\n",
    "filter_type = 2\n",
    "output = ndimage.convolve(bw, filters[filter_type], mode='constant')\n",
    "\n",
    "# Show\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.imshow(bw, cmap='Greys')\n",
    "plt.title('Original image')\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.imshow(output, cmap='Greys')\n",
    "plt.title('Convolved Image')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, as you've likely noticed, different filters can extract particular features OF an image. Now let's go back to our original problem...if we have pictures of the southern alps, and we have pictures of the opera house, how do we distinguish what photographic features distinguish the opera house from the alps?\n",
    "\n",
    "## Part 4: Convolutional neural networks\n",
    "\n",
    "[Top](#Coder-Academy-and-Qantas-Masterclass) | [Previous section](#Part-3:-Background-on-image-processing) | [Next section](#Part-5:-Creating-geographic-boundaries) | [Bottom](#Wrap-up)\n",
    "\n",
    "The beauty of **machine learning** is that it allows computers to find out what are the distinguishing features of an image by crunching millions upon millions of pixels for us, finding nuances that are not readily noticable by the human eye, but are noticable in **patterns amongst pixel values**.\n",
    "\n",
    "So you might be asking, how do we do this? Let's take a little detour into **classification**.\n",
    "\n",
    "### What is classification?\n",
    "\n",
    "> **Classification** is an area of machine learning that tries to build **algorithms** that input a set of data, and output a class label. In our case, we want to build algorithms that **input an image** and output whether an algorithm belongs to one of two classes, the **opera house**, or the **Southern Alps**.\n",
    "\n",
    "If this is too complicated...this [example](https://www.youtube.com/watch?v=vIci3C4JkL0) might help.\n",
    "\n",
    "<img src=\"https://d3ansictanv2wj.cloudfront.net/Figure_1-71076f8ac360d6a065cf19c6923310d2.jpg\" width=\"500\">\n",
    "\n",
    "In math speak, we are essentially trying to define a **function** where the input is an image, and the output is either \"opera house\" or \"Southern Alps\".\n",
    "\n",
    "```\n",
    "f(image) = 0 if image is \"Opera House\"\n",
    "f(image = 1 if image is \"Southern Alps\"\n",
    "```\n",
    "\n",
    "To do this, we can train what is called a **convolutional neural network**.\n",
    "\n",
    "### What is a neural network?\n",
    "\n",
    "\n",
    "A **neural network** is a type of machine learning algorithm that can be used to classify things. It was actually created to resemble how neurons in the brain connect with each other, and the models look like the gif below.\n",
    "\n",
    "![](https://thumbs.gfycat.com/DeadlyDeafeningAtlanticblackgoby-max-1mb.gif)\n",
    "\n",
    "In the image, we're feeding the **image with the number 7** to the **input layer** of the network. The image is 28x28 pixels, which is why there are a total of 784 input neurons (28 x 28 = 784). The network has been trained to recognise the seven, and you can see as there are certain nodes/edges activated in the **middle/hidden layers**, and **output layers** that are specifically activated when a seven is inputted in the network.\n",
    "\n",
    "Since this is a network with input, hidden and output layers, we call this type of neural network a **deep neural network**, and this type of machine learning **deep learning**.\n",
    "\n",
    "Let's play around with training a neural network to recognise digits. The following code will load a set of [pictures that represent digits from the sklearn library](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html#sklearn.datasets.load_digits)/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the digits\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# Show the first digit\n",
    "print('The first digit is a: ' + str(digits.target[0]))\n",
    "print('The size of the images are: ' + str(digits.images[0].shape))\n",
    "plt.imshow(digits.images[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's build up a small neural network with three layers...\n",
    "\n",
    "* An input layer layer, with 64 total inputs\n",
    "* One hidden layer\n",
    "* An output layer, with labels 0-9 at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a model\n",
    "model = Sequential()\n",
    "\n",
    "# Input layer\n",
    "model.add(Flatten())\n",
    "# Hidden layer\n",
    "model.add(Dense(32, activation='relu'))\n",
    "# Output layer\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='rmsprop',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Fit\n",
    "train_x, test_x, train_y, test_y = train_test_split(digits.images, digits.target)\n",
    "\n",
    "# Reformat y\n",
    "train_y_new = []\n",
    "for y in train_y:\n",
    "    temp = np.zeros(10)\n",
    "    temp[y] = 1\n",
    "    train_y_new.append(temp)\n",
    "train_y_new = np.array(train_y_new)\n",
    "\n",
    "# Train\n",
    "model.fit(train_x, train_y_new, batch_size=32, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional side note...\n",
    "\n",
    "For those who are analysing the code above, you might see we use something called `train_test_split`, which splits our data into two different datasets...\n",
    "\n",
    "1. A dataset called `train` that we use to fit the model\n",
    "2. Another dataset called `test`, which we later use to analyse how _well_ our model performs\n",
    "\n",
    "Why do we not train and test model performance on the same dataset? Think about studying for a math test...if we practiced using just the problems we already have completed, we'd get really good at understanding those problems, but not necessarily be able to understand _new_ information. \n",
    "\n",
    "In machine learning, we do not want computers to just understand the data we have at hand, we want to see how it will predict _new_ data it has not been exposed to you. Thus, test sets are _held out_ of model training, and are used to simulate what it's like to expose our algorithms to _new_ information.\n",
    "\n",
    "### Exercise\n",
    "\n",
    "The following code will use our model to predict an output, and then show the true image.\n",
    "\n",
    "Play around by changing the `ind` variable (it can be any number between 0-449). Are there any numbers that tend to be wrong more than others, or did we do a good job?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE INDEX HERE\n",
    "ind = 132\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(test_x[[ind]])\n",
    "# Get index of max\n",
    "y_pred = np.argmax(y_pred)\n",
    "\n",
    "print('Predict number: ' + str(y_pred))\n",
    "plt.imshow(test_x[ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional: Adding complexity\n",
    "\n",
    "So, we just made what's called a **fully connected network** with a lot of **dense layers**. Based upon the amount of layers we made, and neurons, the model tuned the following number of parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print model parameters\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see there are a total of (65 * 32) + (33 * 10) = 2,410 parameters we train.\n",
    "\n",
    "Now this model fitting isn't going to work very well on more complex detections. Let's upload images of the Opera House and the Southern Alps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Targets\n",
    "target_values = []\n",
    "\n",
    "# Upload opera house\n",
    "prefix_opera_house = '../data/Masterclass_Images/Opera_House/'\n",
    "opera_house_array = []\n",
    "for filename in os.listdir(prefix_opera_house):\n",
    "    if 'jpg' in filename:\n",
    "        img = image.load_img(\n",
    "            prefix_opera_house + filename, \n",
    "            target_size=(299, 299)\n",
    "        )\n",
    "        opera_house_array.append(image.img_to_array(img).astype(np.uint8))\n",
    "        target_values.append(0)\n",
    "        \n",
    "# Upload southern alps\n",
    "prefix_southern_alps = '../data/Masterclass_Images/Southern_Alps/'\n",
    "southern_alps_array = []\n",
    "for filename in os.listdir(prefix_southern_alps):\n",
    "    if 'jpg' in filename:\n",
    "        img = image.load_img(\n",
    "            prefix_southern_alps + filename, \n",
    "            target_size=(299, 299)\n",
    "        )\n",
    "        southern_alps_array.append(image.img_to_array(img).astype(np.uint8))\n",
    "        target_values.append(1)\n",
    "        \n",
    "\n",
    "# Combine\n",
    "img_data = np.array(opera_house_array + southern_alps_array)\n",
    "target_values = np.array(target_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the same network configuration we used earlier, and see how our model performs. We'll have to adjust our model slightly, since the input images are now 200 x 200 pixels, with RGB values. Also, we are only trying to predict two values, where \n",
    "\n",
    "```\n",
    "0 = Sydney Opera House\n",
    "1 = the Southern Alps\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model\n",
    "model = Sequential()\n",
    "\n",
    "# Input layer\n",
    "model.add(Flatten())\n",
    "# Hidden layer\n",
    "model.add(Dense(200, activation='relu'))\n",
    "# Output layer\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Split dataset\n",
    "train_img_x, cross_val_img_x, train_img_y, cross_val_img_y = train_test_split(\n",
    "    img_data, target_values\n",
    ")\n",
    "\n",
    "# Fit\n",
    "model.fit(x=train_img_x, y=train_img_y, batch_size=32, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might notice that the training accuracy is not going up!! There's a few reasons why this might happen...\n",
    "\n",
    "* We're using a pretty small dataset\n",
    "* The dataset we have does not distinguish well between the Opera House and the Southern Alps\n",
    "* Our model isn't complicated enough\n",
    "\n",
    "So we have a few choices to make...\n",
    "\n",
    "1. We could build a more complicated model\n",
    "2. We could try to scrape more data\n",
    "3. We could augment our dataset using the data we currently have, by adding random noise\n",
    "\n",
    "Let's pretend for the purposes of this workshop (2) is off the table. So...let's start with (1).\n",
    "\n",
    "To complicate our model, we could add more layers, and thus have more parameters that are trained in the model to capture nuances. Let's try this out in the code below, and time the training time. We'll also print some summary data at the end of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start\n",
    "start = time.time()\n",
    "\n",
    "# Add more layers\n",
    "model = Sequential()\n",
    "\n",
    "# Input layer\n",
    "model.add(Flatten())\n",
    "# Hidden layers\n",
    "model.add(Dense(200, activation='relu'))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "# Output layer\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Fit\n",
    "model.fit(x=img_data, y=target_values, batch_size=32, epochs=1)\n",
    "\n",
    "# End time\n",
    "end = time.time()\n",
    "print(\"\\n\")\n",
    "print('Training took a total of: %0.2f seconds' % (end - start))\n",
    "\n",
    "# Model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a lot of parameters, and not much payoff!\n",
    "\n",
    "Each fully connected layer creates (the layer size + 1) * (the next layer size) parameters to train, and unless we add a _ton_ more, we're not going to get much better accuracy. So instead, we'll use what's called **convolutional layers** to train the network, which will allow us to train layers with **far less parameters**.\n",
    "\n",
    "### Convolutional layers\n",
    "\n",
    "Convolutional layers work by finding **kernels**, or the **arrays we described earlier**, to pick-out interesting parts of images.\n",
    "\n",
    "---\n",
    "\n",
    "![](../img/Qantas_CNN_Image.png)\n",
    "\n",
    "---\n",
    "\n",
    "We then use a small amount of fully connected layers at the end of the network to finish our model.\n",
    "\n",
    "---\n",
    "\n",
    "![](../img/CNN.png)\n",
    "\n",
    "---\n",
    "\n",
    "Let's create our convolutional neural network. You'll see the network has some convolutional layers in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make model\n",
    "convo_model = Sequential()\n",
    "\n",
    "# Convolutional layers\n",
    "convo_model.add(Conv2D(32, (3, 3), input_shape=(299, 299, 3), activation='relu'))\n",
    "convo_model.add(GlobalAveragePooling2D())\n",
    "\n",
    "# Fully connected layers\n",
    "convo_model.add(Dense(200, activation='relu'))\n",
    "convo_model.add(Dense(1, activation='sigmoid'))\n",
    "# Compile\n",
    "convo_model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Start\n",
    "start = time.time()\n",
    "\n",
    "# Fit\n",
    "convo_model.fit(x=img_data, y=target_values, batch_size=32, epochs=1)\n",
    "\n",
    "# End time\n",
    "end = time.time()\n",
    "\n",
    "print(\"\\n\")\n",
    "print('Training took a total of: %0.2f seconds' % (end - start))\n",
    "\n",
    "# Model summary\n",
    "convo_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still...not great, but we are reaching slightly better acuracy with more parameters! The reality is that convolutional neural networks take _a lot of data_ to train properly, and we only have 300 images in our training dataset. They're more efficient still then fully-connected networks. \n",
    "\n",
    "What people typically do is utilise **pre-trained networks**, and fit these pre-trained networks to their specific dataset. Let's do that, by using [Google's Inception-v3 model](https://medium.com/@sh.tsang/review-inception-v3-1st-runner-up-image-classification-in-ilsvrc-2015-17915421f77c) as a base model to our network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build onto Google's model. Run the cell below to do the build."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create baseline_model\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False)\n",
    "output = base_model.output\n",
    "output = GlobalAveragePooling2D()(output)\n",
    "\n",
    "# Add dense layers\n",
    "output = Dense(200, activation='relu')(output)\n",
    "output = Dropout(0.5)(output) # Dropout helps prevent overfitting\n",
    "output = Dense(200, activation='relu')(output)\n",
    "output = Dropout(0.5)(output)\n",
    "output = Dense(1, activation='sigmoid')(output)\n",
    "\n",
    "# Create model\n",
    "inceptionv3_model = Model(inputs=base_model.input, outputs=output)\n",
    "\n",
    "for layer in inceptionv3_model.layers[:249]:\n",
    "    layer.trainable = False\n",
    "for layer in inceptionv3_model.layers[249:]:\n",
    "    layer.trainable = True\n",
    "    \n",
    "inceptionv3_model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer='rmsprop',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Start\n",
    "start = time.time()\n",
    "\n",
    "# Train model\n",
    "inceptionv3_model.fit(\n",
    "    x=preprocess_input(img_data), \n",
    "    y=target_values, \n",
    "    batch_size=32, \n",
    "    epochs=2\n",
    ")\n",
    "\n",
    "# End time\n",
    "end = time.time()\n",
    "\n",
    "print(\"\\n\")\n",
    "print('Training took a total of: %0.2f seconds' % (end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "The following code allows you to input any image using a URL within the `my_image` variable. It then uses the `inceptionv3_model` we just created to predict...is the image more like the **Opera House**, or the **Southern Alps**?\n",
    "\n",
    "Play around with changing `my_image`. Think about when the answer is wrong or right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert image here\n",
    "my_image = 'https://raw.githubusercontent.com/dadler6/Australia_Images/master/27597053638_6d0e4ec38c.jpg'\n",
    "\n",
    "# Upload image and resize\n",
    "image_open = Image.open(urllib.request.urlopen(my_image))\n",
    "image_open = image_open.resize((299, 299))\n",
    "image_array = image.img_to_array(image_open)\n",
    "\n",
    "# Now predict\n",
    "pred = inceptionv3_model.predict(preprocess_input(np.array([image_array])))\n",
    "\n",
    "# Print result\n",
    "if np.round(pred[0][0]) == 0:\n",
    "    print('Model predicted: Sydney Opera House, with probability: %.2f' % (1 - pred))\n",
    "else:\n",
    "    print('Model predicted: Southern Alps, with probability: %.2f' % pred)\n",
    "    \n",
    "# Show\n",
    "plt.imshow(image_array.astype(np.uint8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Creating geographic boundaries\n",
    "\n",
    "[Top](#Coder-Academy-and-Qantas-Masterclass) | [Previous section](#Part-4:-Convolutional-neural-networks) | [Next section](#Part-6:-Building-the-model) | [Bottom](#Wrap-up)\n",
    "\n",
    "Let's review what we've done. Thus far we have...\n",
    "\n",
    "* Examined how images are visualised by a computer\n",
    "* Examined how we can use the **convolution** operation to extract meaningful features from an image\n",
    "* Setup a **classification algorithm** using a **convolutional neural network** to classify images from two locations\n",
    "\n",
    "What are we missing?? Well...people do not just travel to the Sydney Opera House, or the Southern Alps. There are millions (and likely more) of pictures online from different locations that Qantas and its partners travel to. The big question becomes...\n",
    "\n",
    "> Given the images that exist online...how do we develop the **right classification labels** to be able to match people towards new travel locations??\n",
    "\n",
    "---\n",
    "\n",
    "<img src=\"../img/Qantas_Classification_Drawing.png\" width=\"600\">\n",
    "\n",
    "---\n",
    "\n",
    "### Bounding boxes\n",
    "\n",
    "We're going to use a technique based upon [this paper](https://arxiv.org/abs/1602.05314) by Google to develop a method to take images from a _ton_ of different locations, and figure out what labels our algorithm should utilise. The idea is simply...\n",
    "\n",
    "* Use geolocated images, with a latitude and longitude per every image, to identify areas of dense photo activity\n",
    "* Restrict these areas to areas with _a lot_ of Qantas customers\n",
    "* Develop a classification algorithm to identify which area an image fits in\n",
    "\n",
    "We call each area a **bounding box**, meaning it bounds a geographic area with a box, and each box can become a classification label.\n",
    "\n",
    "For example, the following bounding boxes were built off of a training set with images from Central Australia, Cairns, Sydney, Melbourne and the Southern Alps. Each section of the graph has been given a class label.\n",
    "\n",
    "---\n",
    "\n",
    "<img src=\"../img/bounding_boxes.png\" width=\"500\">\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Running the code\n",
    "\n",
    "The following code can be used to create bounding boxes. We won't go over the code, but run it to enable it's use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadLocations(df):\n",
    "    \"\"\"\n",
    "    Create a location that can be used to make boxes\n",
    "    \n",
    "    :param df: the DataFrame with latitude and longitude location\n",
    "    \n",
    "    :return spindex: The object that can make the bounding boxes\n",
    "    \"\"\"\n",
    "    # Makes a quad tree\n",
    "    lonList = []\n",
    "    latList = []\n",
    "    # Creates a bounding box surrounding Australia and NZ (x_min, y_min, x_max, y_max)\n",
    "    spindex = Index(max_items=200, bbox=(113.38, -50.00, 170, -10.83))\n",
    "    count = 0\n",
    "    # Inserts photos into box based upon location\n",
    "    for index, row in df.iterrows():\n",
    "        decimalLon, decimalLat = float(row['longitude']), float(row['latitude'])\n",
    "        fStr = str(int(row['id'])) + '_' + str(row['secret']) + '.jpg'\n",
    "        spindex.insert(fStr,(decimalLon,decimalLat,decimalLon,decimalLat))\n",
    "        count+=1    \n",
    "    return spindex\n",
    "\n",
    "def walk(parent, boxes, num_data):\n",
    "    \"\"\"\n",
    "    Take a walk to see if we should make a box\n",
    "    \n",
    "    :param parent: The parent of the current box\n",
    "    :param boxes: The current boxes list\n",
    "    :param num_data: The number of data points to create a box\n",
    "    \"\"\"\n",
    "    # Get center of the bounding box\n",
    "    boxStr = (\n",
    "        str(parent.center[0]) + ',' + str(parent.center[1]) + ',' + str(parent.width) + ',' + str(parent.height)\n",
    "    )\n",
    "    images = []\n",
    "    # Go through each image (if images exist)\n",
    "    for node in parent.nodes:\n",
    "        images.append(node.item)\n",
    "    \n",
    "    # If there exist num_data points in this area, create box\n",
    "    if len(images) > num_data:\n",
    "        boxes[boxStr] = images\n",
    "\n",
    "    # Go through each child in the tree\n",
    "    for child in parent.children:   \n",
    "        walk(child, boxes, num_data)\n",
    "\n",
    "def buildBoxes(spindex, num_data):\n",
    "    \"\"\"\n",
    "    Build bounding boxes\n",
    "    \n",
    "    :input spindex: An index of location data points\n",
    "    :input num_data: The number of data points\n",
    "    \n",
    "    :return boxes: The boxes\n",
    "    \"\"\"\n",
    "    boxes = {}\n",
    "    walk(spindex, boxes, num_data)\n",
    "    return boxes\n",
    "\n",
    "def drawDistribution(boxes, y=None):\n",
    "    \"\"\"\n",
    "    Draw area with boxes\n",
    "    \n",
    "    :inputs boxes: The boxes\n",
    "    \"\"\"    \n",
    "    f = open('../data/Polygons.js','w')\n",
    "    keys = sorted(boxes.keys())\n",
    "    if y is None:\n",
    "        y = [0.1] * len(boxes)\n",
    "    else:\n",
    "        y /= y.max()\n",
    "    for i in range(0,len(keys)):\n",
    "        key = keys[i]\n",
    "        lon, lat, width, height = key.split(',')\n",
    "        lon=float(lon)\n",
    "        lat=float(lat)\n",
    "        width=float(width)\n",
    "        height=float(height)\n",
    "        ptList = [[lat-height/2,lon-width/2],[lat-height/2,lon+width/2],[lat+height/2,lon+width/2],[lat+height/2,lon-width/2]]\n",
    "        (r,g,b,a) = cm.Reds(y[i])\n",
    "        r*=255\n",
    "        g*=255\n",
    "        b*=255\n",
    "        color = '#%02x%02x%02x' % (int(r), int(g), int(b))\n",
    "        f.write(\"L.polygon(\"+str(ptList)+\", {fillColor: '\"+color+\"',fillOpacity: 0.5}).addTo(mymap).bindPopup(\"+str(i)+\");\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we create bounding boxes, there's an active choice about how many data points need to be in a certain geographic area to consider it a box. There's a trade-off to this...\n",
    "\n",
    "* The more data points in a box, the more images, and the easier it is to train a CNN\n",
    "* The less data points, the more geography specific boxes we have\n",
    "\n",
    "Let's look at an example.\n",
    "\n",
    "The [file downloaded here](https://www.australiantownslist.com/) has a list of longitude and latitude coordinates of Australian towns. What we can do is create bounding boxes based upon geographic density of towns being proximal to each other. Let's upload the file into a variable called `aus_towns`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Australia towns\n",
    "aus_towns = pd.read_csv('../data/au-towns-sample.csv')\n",
    "aus_towns['secret'] = 'fill'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "The variable `num_datapoints` controls how many data points (towns) need to be proximal to make a bounding box. Raise and lower num_datapoints to create different bounding boxes. You can see the boxes you've drawn using the link below.\n",
    "\n",
    "[Click here to see your boxes!](../data/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET num_datapoints here\n",
    "num_datapoints = 10\n",
    "\n",
    "# Add the locations\n",
    "sp = loadLocations(aus_towns)\n",
    "\n",
    "# Create boxes\n",
    "b = buildBoxes(sp, num_datapoints)\n",
    "\n",
    "# Draw\n",
    "drawDistribution(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build boxes for an actual image dataset that is hosted on GitHub. You can see the images [here](https://github.com/dadler6/Australia_Images/blob/master/27597053638_6d0e4ec38c.jpg). It comprises images from five places in the area...\n",
    "\n",
    "* The Sydney Opera House\n",
    "* The Sourthern Alps\n",
    "* Uluru\n",
    "* Melbourne CBD\n",
    "* Cairns\n",
    "\n",
    "Run the following code which will create bounding boxes off of this dataset. Checkout the created boxes [at this link](../data/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add data\n",
    "aus_image_data = pd.read_csv('../data/all_images.csv')\n",
    "\n",
    "# SET num_datapoints here\n",
    "num_datapoints = 50\n",
    "\n",
    "# Add the locations\n",
    "sp = loadLocations(aus_image_data.loc[aus_image_data['Downloaded'] == 1, :])\n",
    "\n",
    "# Create boxes\n",
    "b = buildBoxes(sp, num_datapoints)\n",
    "\n",
    "# Draw\n",
    "drawDistribution(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Building the model\n",
    "\n",
    "[Top](#Coder-Academy-and-Qantas-Masterclass) | [Previous section](#Part-5:-Creating-geographic-boundaries) | [Next section](#Wrap-up) | [Bottom](#Wrap-up)\n",
    "\n",
    "Let's now build our final model, and then for the rest of the masterclass, we'll let you have a go at changing up...\n",
    "\n",
    "* The model\n",
    "* The image to test out\n",
    "\n",
    "Run the following code to upload our images from the GitHub repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gzipped_json(url):\n",
    "    return loads(decompress(get(url).content))\n",
    "\n",
    "github_url = 'https://github.com/dadler6/Australia_Images/raw/master/'\n",
    "\n",
    "# Upload\n",
    "aus_img_dict = dict()\n",
    "aus_img_dict.update(get_gzipped_json(github_url + 'images_0.json.gz'))\n",
    "print('25% Uploaded')\n",
    "aus_img_dict.update(get_gzipped_json(github_url + 'images_1.json.gz'))\n",
    "print('50% Uploaded')\n",
    "aus_img_dict.update(get_gzipped_json(github_url + 'images_2.json.gz'))\n",
    "print('75% Uploaded')\n",
    "aus_img_dict.update(get_gzipped_json(github_url + 'images_3.json.gz'))\n",
    "print('100% Uploaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's add labels to each of our images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image labels\n",
    "categories = dict(\n",
    "    zip(aus_image_data.Photo_Category.unique(), range(len(aus_image_data.Photo_Category.unique())))\n",
    ")\n",
    "\n",
    "# Now get each category\n",
    "image_inputs = []\n",
    "image_categories = []\n",
    "\n",
    "# Iterate through each image\n",
    "for k in aus_img_dict:\n",
    "    # Get ID\n",
    "    temp_id = int(k.split('_')[0])\n",
    "    # Get the photo category\n",
    "    cat = aus_image_data.loc[aus_image_data['id'] == temp_id, 'Photo_Category'].iloc[0]\n",
    "    cat_list = np.zeros(len(aus_image_data.Photo_Category.unique()))\n",
    "    cat_list[categories[cat]] = 1\n",
    "    # Append to lists so ordering is consistent\n",
    "    image_inputs.append(aus_img_dict[k])\n",
    "    image_categories.append(cat_list)\n",
    "\n",
    "# Make arrays\n",
    "image_inputs = np.array(image_inputs)\n",
    "image_categories = np.array(image_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a couple images for the data we just uploaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image_inputs[0].astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image_inputs[300].astype(np.uint8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "\n",
    "Let's train a final model to try and predict whether an inputted image is alike to the five locations within our dataset. We've started the code for you, but try and do the following create the dense layers. \n",
    "\n",
    "**NOTE**, the last dense layer should not have one output, but **5** because we have five different locations within our model.\n",
    "\n",
    "The following code creates a `hidden layer:`\n",
    "\n",
    "```python\n",
    "output = Dense(OUTPUT_SIZE, activation='relu')(output)\n",
    "```\n",
    "\n",
    "The following code creates the `final layer:`\n",
    "```python\n",
    "output = Dense(OUTPUT_SIZE, activation='softmax')(output)\n",
    "```\n",
    "\n",
    "In addition, the `loss` when you compile the model needs to changed from `binary_crossentropy` to `categorical_crossentropy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create baseline_model\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False)\n",
    "output = base_model.output\n",
    "output = GlobalAveragePooling2D()(output)\n",
    "\n",
    "# Add dense layers\n",
    "output = Dense(200, activation='relu')(output)\n",
    "output = Dropout(0.5)(output) # Dropout helps prevent overfitting\n",
    "output = Dense(200, activation='relu')(output)\n",
    "output = Dropout(0.5)(output)\n",
    "output = Dense(5, activation='sigmoid')(output)\n",
    "\n",
    "# Create model\n",
    "final_model = Model(inputs=base_model.input, outputs=output)\n",
    "\n",
    "for layer in final_model.layers[:249]:\n",
    "    layer.trainable = False\n",
    "for layer in final_model.layers[249:]:\n",
    "    layer.trainable = True\n",
    "    \n",
    "final_model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='rmsprop',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Start time\n",
    "start = time.time()\n",
    "\n",
    "# Train model\n",
    "final_model.fit(\n",
    "    x=preprocess_input(image_inputs), \n",
    "    y=image_categories, \n",
    "    batch_size=32, \n",
    "    epochs=1\n",
    ")\n",
    "\n",
    "# End time\n",
    "end = time.time()\n",
    "\n",
    "print(\"\\n\")\n",
    "print('Training took a total of: %0.2f seconds' % (end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code can be used to **predict where images are from** using the model. Change the `my_image` variable with an image URL of your choosing to change the image.\n",
    "\n",
    "Once you've predicted the value, you can go to [this link](../data/index.html) to visualise the most likely areas for your image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert image here\n",
    "my_image = 'http://static.asiawebdirect.com/m/phuket/portals/phuket-com/homepage/yourguide/romantic/beaches/pagePropertiesImage/phuket-romantic-beaches.jpg'\n",
    "\n",
    "# Upload image and resize\n",
    "image_open = Image.open(urllib.request.urlopen(my_image))\n",
    "image_open = image_open.resize((299, 299))\n",
    "image_array = image.img_to_array(image_open)\n",
    "\n",
    "# Now predict\n",
    "pred = final_model.predict(preprocess_input(np.array([image_array])))\n",
    "    \n",
    "# Show\n",
    "plt.imshow(image_array.astype(np.uint8))\n",
    "\n",
    "# Draw distribution\n",
    "drawDistribution(b, pred[0])\n",
    "\n",
    "# Print results\n",
    "print('Model likelihoods: ')\n",
    "df = pd.DataFrame(categories.items(), columns=['Category', 'Index'])\n",
    "df['Likelihood'] = pred[0]\n",
    "df[['Category', 'Likelihood']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap-up\n",
    "\n",
    "[Top](#Coder-Academy-and-Qantas-Masterclass) | [Previous section](#Part-6:-Building-the-model) | [Next section](#Wrap-up) | [Bottom](#Wrap-up)\n",
    "\n",
    "Thank you for attending our masterclass! We hope we _demystified_ a little bit of what actually occurs when you build a machine learning process. Big thank you to Natalie Ganderton and Daniel Walsh from Qantas for their time.\n",
    "\n",
    "If you would like to download your work for today, please click **File->Download As->.html**. You will not be able to run the cells, but you will be able to view the material you learned within a web browser.\n",
    "\n",
    "### Survey\n",
    "\n",
    "We would appreciate it if you could complete a quick feedback survey for tonight's Masterclass. You can find the survey here: http://bit.ly/ml_qantas_survey\n",
    "\n",
    "THANK YOU!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
