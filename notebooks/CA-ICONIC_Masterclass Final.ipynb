{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coder Academy and THE ICONIC Masterclass\n",
    "\n",
    "This notebook has the following sections...\n",
    "\n",
    "* [Part 0: Using this notebook](#Part-0:-Using-this-notebook)\n",
    "* [Part 1: Introduction](#Part-1:-Introduction)\n",
    "* [Part 2: First look at our dataset](#Part-2:-First-look-at-our-dataset)\n",
    "* [Part 3: Data pre-processing and feature engineering](#Part-3:-Data-pre-processing-and-feature-engineering)\n",
    "* [Part 4: Clustering the dataset](#Part-4:-Clustering-the-dataset)\n",
    "* [Part 5: Classifying our inferred gender](#Part-5:-Classifying-our-inferred-gender)\n",
    "* [Part 6: Putting it altogether and next steps](#Part-6:-Putting-it-altogether-and-next-steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../img/iconic_coder.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 0: Using this notebook\n",
    "\n",
    "[Top](#Coder-Academy-and-THE-ICONIC-Masterclass) | [Previous Section](#Coder-Academy-and-THE-ICONIC-Masterclass) | [Next Section](#Part-1:-Introduction) | [Bottom](#Wrap-up)\n",
    "\n",
    "## What is Python?\n",
    "\n",
    "Python is an _interpretive_ programming language invented in the 1980s. It's actually named after Monty Python and Holy Grail. In this class we'll be using Python to build our machine learning algorithms. \n",
    "\n",
    "### Why learn Python?\n",
    "\n",
    "Python has gained popularity because it has an easier syntax (rules to follow while coding) than many other programming languages. Python is very diverse in its applications which has led to its adoption in areas such as data science and web development.\n",
    "\n",
    "All of the following companies actively use Python:\n",
    "\n",
    "![Image](https://www.probytes.net/wp-content/uploads/2018/08/appl.png)\n",
    "\n",
    "## How do I interact with this notebook?\n",
    "\n",
    "A Jupyter Notebook is an interactive way to work with code in a web browser. Jupyter is a pseudo-acronym for three programming languages: Julia, python and (e)r. Notebooks provide a format to add instructions + code in one file, which is why we're using it!\n",
    "\n",
    "We'll quickly do some practice to introduce you how to use this notebook. For a list of keyboard shortcuts you can take a look at [Max Melnick's](http://maxmelnick.com/2016/04/19/python-beginner-tips-and-tricks.html) beginner tips for Jupyter Notebook.\n",
    "\n",
    "Here's a quick run down of some of the most basic commands to use:\n",
    "\n",
    "- A cell with a **<span style=\"color:blue\">blue</span>** background is in **Command Mode**. This will allow you to toggle up/down cells using the arrow keys. You can press enter/return on a cell in command mode to enter edit mode\n",
    "\n",
    "- A cell with a **<span style=\"color:green\">green</span>** background is in **Edit Mode**. This will allow you to change the content of cells. You can press the escape key on a cell in command mode to enter edit mode\n",
    "\n",
    "- To run the contents of a cell, you can type:\n",
    "  - `cmd + enter`, which will run the cotents of a cell and keep the cursor in place\n",
    "  - `shift + enter`, which will run the contents of a cell, and move the cursor to the next cell (or create a new cell)\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Edit the below by changing \"Gretchen\" to your own name by entering edit mode, and then running the cell using the directions above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hello, Gretchen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add/delete cells using the following commands in <span style=\"color:blue\">**Command Mode**</span>:\n",
    "\n",
    "- `a`, adds a cell above the current cell\n",
    "- `b`, adds a cell below the current cell\n",
    "- `d + d`, (pressing the \"d\" key twice in succession) deletes a cell\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Add/delete the cells such that each individual cell prints the numbers 1-5 in order. The numbers 2 and 4 are already completed for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Introduction\n",
    "\n",
    "[Top](#Coder-Academy-and-THE-ICONIC-Masterclass) | [Previous Section](#Part-0:-Using-this-notebook) | [Next Section](#Part-2:-First-look-at-our-dataset) | [Bottom](#Wrap-up)\n",
    "\n",
    "## The problem\n",
    "\n",
    "As explained, [THE ICONIC](https://www.theiconic.com.au/) does not receive all information about a person when they create an online profile, but the more information they receive from a person, the better they can tailor their marketing towards specific individuals.\n",
    "\n",
    "The goal of this masterclass is to develop a way to infer information about an individual by aligning buying behaviours with demographic traits.\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Take 10 minutes to do some internet research, and try to find out how online buying behaviours differ between genders. Write down your findings - they might inform how you build your algorithms for the remainder of the masterclass!\n",
    "\n",
    "\n",
    "## Data Pipeline\n",
    "\n",
    "In this session, we'll build a **data pipeline** to infer gender based upon behavioural data. From [wikipedia](https://en.wikipedia.org/wiki/Pipeline_(computing))...\n",
    "\n",
    "> In computing, a **pipeline** (also known as a **data pipeline**) is a set of data processing elements connected in series, where the output of one element is the input of the next one. The elements of a pipeline are often executed in parallel or in time-sliced fashion.\n",
    "\n",
    "Data pipelines deal with the process of collecting, modifying and analysing a dataset towards some goal. Here's a picture of a data pipeline from [this medium blog](https://medium.com/the-data-experience/building-a-data-pipeline-from-scratch-32b712cfb1db)...\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/1600/1*8-NNHZhRVb5EPHK5iin92Q.png)\n",
    "\n",
    "For the rest of this lesson we'll build-up this pipeline. We will....\n",
    "\n",
    "1. Analyse our dataset, by taking a look at the columns available\n",
    "2. Process the dataset, by creating **usuable features** for our algorithms and **normalisng** these features\n",
    "3. **Infer a gender** on our dataset using clustering\n",
    "4. Analyse our inferred gender, and build a **classification algorithm** that can be used to predict our inferred gender from new data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: First look at our dataset\n",
    "\n",
    "[Top](#Coder-Academy-and-THE-ICONIC-Masterclass) | [Previous Section](#Part-1:-Introduction) | [Next Section](#Part-3:-Data-pre-processing-and-feature-engineering) | [Bottom](#Wrap-up)\n",
    "\n",
    "Let's start to look at the data available to us. This data has been provided by the **THE ICONIC**. It has been **de-identified**, meaning it has been modified in such a way where the data could not lead back to the actual population of individuals it represents.\n",
    "\n",
    "## Pandas introduction\n",
    "\n",
    "To analyse our dataset with Python we should first load the dataset. To do this, we will use the [Pandas](https://pandas.pydata.org/) module.\n",
    "\n",
    "> A **module** is a set of code-files that can be loaded to add additional capabilities to our program\n",
    "\n",
    "Pandas allows us to manipulate tabular data in Python. Let's import pandas so that we can use it for the rest of our Python session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas and give it the nickname \"pd\"\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can upload our data set into Python, and take a quick look at the data available to us.\n",
    "\n",
    "---\n",
    "\n",
    "Run the following code cell which will do the following...\n",
    "\n",
    "1. Upload our data into Python, specifically within a variable called `raw_data`\n",
    "2. Print a brief description of each column in our dataset, including...\n",
    " * The type of data, which will be called `<class 'pandas.core.frame.DataFrame'>`\n",
    " * The type of `index`, or row names from the data: `RangeIndex: 46030 entries, 0 to 46029`\n",
    " * Number of data columns: `Data columns (total 42 columns)`\n",
    " * The column names and information\n",
    "\n",
    "\n",
    "For example, the following describes a column called `afterpay_payments`, of which there are `46030` filled-in values for this column (non-blank), and the column is a number, or `int64`.\n",
    "\n",
    "```\n",
    "afterpay_payments                46030 non-null int64\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the dataset\n",
    "raw_data = pd.read_csv('../data/data_iconic_workshop.csv.gz')\n",
    "\n",
    "# Print basic information about the dataset\n",
    "raw_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So...what do these columns mean? Some of them might be pretty obvious, like `orders` probably represents the number of orders for a customer. But what might `sacc_items` mean? Here's a little more information about the dataset to make life easier for you. \n",
    "\n",
    "| Column                   | Value   | Description                                                              | \n",
    "|--------------------------|---------|--------------------------------------------------------------------------| \n",
    "| customer_id              | string  | ID of the customer - super duper hashed                                  | \n",
    "| days_since_first_order   | integer | Days since the first order was made                                      | \n",
    "| days_since_last_order    | integer | Days since the last order was made                                       | \n",
    "| int_is_newsletter_subscriber | string  | Flag for a newsletter subscriber (1 = Yes, 0 = No)                                        | \n",
    "| orders                   | integer | Number of orders                                                         | \n",
    "| items                    | integer | Number of items                                                          | \n",
    "| cancels                  | integer | Number of cancellations - when the order is cancelled after being placed | \n",
    "| returns                  | integer | Number of returned orders                                                | \n",
    "| different_addresses      | integer | Number of times a different billing and shipping address was used        | \n",
    "| shipping_addresses       | integer | Number of different shipping addresses used                              | \n",
    "| devices                  | integer | Number of unique devices used                                            | \n",
    "| vouchers                 | integer | Number of times a voucher was applied                                    | \n",
    "| cc_payments              | integer | Binary indicating if credit card was used for payment                       | \n",
    "| paypal_payments          | integer | Binary indicating if PayPal was used for payment                              | \n",
    "| afterpay_payments        | integer | Binary indicating if AfterPay was used for payment                            | \n",
    "| apple_payments           | integer | Binary indicating if Apple Pay was used for payment                           | \n",
    "| female_items             | integer | Number of items purchased for women                                         | \n",
    "| male_items               | integer | Number of items purchased for men                                           | \n",
    "| unisex_items             | integer | Number of unisex items purchased                                         | \n",
    "| wapp_items               | integer | Number of Women Apparel items purchased                                  | \n",
    "| wftw_items               | integer | Number of Women Footwear items purchased                                 | \n",
    "| mapp_items               | integer | Number of Men Apparel items purchased                                    | \n",
    "| wacc_items               | integer | Number of Women Accessories items purchased                              | \n",
    "| macc_items               | integer | Number of Men Accessories items purchased                                | \n",
    "| mftw_items               | integer | Number of Men Footwear items purchased                                   | \n",
    "| wspt_items               | integer | Number of Women Sport items purchased                                    | \n",
    "| mspt_items               | integer | Number of Men Sport items purchased                                      | \n",
    "| curvy_items              | integer | Number of Curvy items purchased                                          | \n",
    "| sacc_items               | integer | Number of Sport Accessories items purchased                              | \n",
    "| msite_orders             | integer | Number of Mobile Site orders                                             | \n",
    "| desktop_orders           | integer | Number of Desktop orders                                                 | \n",
    "| android_orders           | integer | Number of Android app orders                                             | \n",
    "| ios_orders               | integer | Number of iOS app orders                                                 | \n",
    "| other_device_orders      | integer | Number of Other device orders                                            | \n",
    "| work_orders              | integer | Number of orders shipped to work                                         | \n",
    "| home_orders              | integer | Number of orders shipped to home                                         | \n",
    "| parcelpoint_orders       | integer | Number of orders shipped to a parcelpoint                                | \n",
    "| other_collection_orders  | integer | Number of orders shipped to other collection points                      | \n",
    "| average_discount_onoffer | float   | Average discount rate of items typically purchased                       | \n",
    "| average_discount_used    | float   | Average discount finally used on top of existing discount                | \n",
    "| revenue                  | float   | $ Dollar spent overall per person                                        |\n",
    "\n",
    "\n",
    "We have already performed some data cleaning for you to save time. To give some perspective on the data cleaning process, since **a lot of a data scientist's job is to clean data** we have...\n",
    "\n",
    "* Removed null values, or blank values, within the data\n",
    "* Confirmed units within the columns are appropriate\n",
    "* Changed relevant string variables to numerical values (computers really do not like text...)\n",
    "\n",
    "You can see that our data has a _ton_ of different information about a customer. Let's just take a look at a single row of data, which represents a specific customer's buying patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the first row of data\n",
    "raw_data[0:1].transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an idea about the type of data available to us, it might be helpful to start prying the dataset for useful trends.\n",
    "\n",
    "Remember we are trying to categorise our customers as males and females. We know a little bit about what purchasing behaviours for males and females looks like, so it might be helpful to see whether these qualitative trends are evident in our dataset. If so, we might be able to utilise these features to **cluster** or **separate** the individuals in our dataset into their respective male/female groups.\n",
    "\n",
    "Looking at raw data is tough to do...especially when we have ~45,000 rows to deal with in our dataset.\n",
    "\n",
    "What might be more helpful is to visualise our dataset (humans like visuals over text!). We need to have questions in-hand, as there are 40 columns to look at, and there's no easy way to look at every column at once.\n",
    "\n",
    "![](https://www.quantinsti.com/wp-content/uploads/2017/07/seaburn-1.png)\n",
    "\n",
    "The following code will import the [`matplotlib`](https://matplotlib.org/) and [`seaborn`](https://matplotlib.org/) libraries, the two main libraries we will use for data visualisation within this masterclass.\n",
    "\n",
    "It will also run the \n",
    "\n",
    "```python\n",
    "%matplotlib inline\n",
    "```\n",
    "\n",
    "command, which will allow us to render the images we create _within_ our notebook, instead of explicitly running a command each time to show the graphs/plots we generate with seaborn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import seaborn\n",
    "import seaborn as sns\n",
    "\n",
    "# State to render images inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributions\n",
    "\n",
    "Something that we often like to look at are the **distributions** of our dataset. One way to visualise the distribution of our data is by using a boxplot.\n",
    "\n",
    "<img src=\"../img/boxplot_revenue.png\" width=\"800\">\n",
    "\n",
    "A boxplot shows the **spread** of our data, including:\n",
    "\n",
    "1. The minimum value\n",
    "2. The maximum value\n",
    "3. The median\n",
    "\n",
    "When you plot the data with a boxplot, you may observe that some of the points seem far away from the majority of values. These are called **outliers**. For example, if you received a dataset that described the height of staff at your workplace, and there were a few people that were 2.1 meters tall, they would be outliers, because people aren't normally 2.1 meters tall.\n",
    "\n",
    "From [nist](https://www.itl.nist.gov/div898/handbook/prc/section1/prc16.htm):\n",
    "\n",
    "> An **outlier** is an observation that lies an abnormal distance from other values in a random sample from a population. In a sense, this definition leaves it up to the analyst (or a consensus process) to decide what will be considered abnormal. Before abnormal observations can be singled out, it is necessary to characterize normal observations.\n",
    "\n",
    "### Exercise\n",
    "\n",
    "The following code uses the [sns.boxplot](https://seaborn.pydata.org/generated/seaborn.boxplot.html) command to visualise variables within our dataset. It iterates through an input list of variables to show us these distributions.\n",
    "\n",
    "Take a look at the variables available to us, and choose some variables you are interested in visualising, and add them to the list below. Data points which have outliers are typically far-away from the center box within the box plot.\n",
    "\n",
    "Take note of which features have a lot of outliers as you visualise the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of features\n",
    "boxplot_vars = ['male_items', 'female_items']\n",
    "\n",
    "# Create boxplot\n",
    "for v in boxplot_vars:\n",
    "    plt.figure(figsize=(10, 3))\n",
    "    sns.boxplot(raw_data[v])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier filtering\n",
    "\n",
    "It is good to get rid of really extreme points if they are non-normal to a dataset. Also outliers can make it _really hard_ to visualise data!\n",
    "\n",
    "Run the following cell to define a function we can use for outlier filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_cutoff(my_data, cols, cutoff=1.5):\n",
    "    \"\"\"\n",
    "    Filter out outliers from a dataset within a set of columns and using a specified\n",
    "    cuttoff values.\n",
    "    \n",
    "    inputs: my_data <pd.DataFrame>: A dataset\n",
    "            cols <list>: A list of columns to filter\n",
    "            cutoff <float>: A cutoff value\n",
    "            \n",
    "    output: The filtered DataFrame\n",
    "    \"\"\"\n",
    "    # Get original number of columns\n",
    "    orig_num_rows = my_data.shape[0]\n",
    "        \n",
    "    # Go through columns\n",
    "    for c in cols:\n",
    "        # Get IQR\n",
    "        percentile_25 = my_data[c].quantile(q=0.25)\n",
    "        percentile_75 = my_data[c].quantile(q=0.75)\n",
    "\n",
    "        # Calculate cutoff * IQR\n",
    "        iqr = percentile_75 - percentile_25\n",
    "        high_cut = percentile_75 + (iqr * cutoff)\n",
    "        low_cut = percentile_25 - (iqr * cutoff)\n",
    "\n",
    "        # Filter\n",
    "        my_data = my_data.loc[(my_data[c] >= low_cut) & (my_data[c] <= high_cut), :]\n",
    "    \n",
    "    # Print the amount of data lost\n",
    "    print('Number of columns eliminated: '  + str(orig_num_rows - my_data.shape[0]))\n",
    "    \n",
    "    return my_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Add variables to the 'cols_to_filter' list below to filter outliers within those columns. **NOTE** the more columns we filter, and the _lower_ the cutoff, the more data we will lose, which is not good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to filter\n",
    "cols_to_filter = ['items', 'revenue']\n",
    "cutoff = 10\n",
    "\n",
    "# Filter data\n",
    "filtered_data = outlier_cutoff(raw_data.copy(), cols_to_filter, cutoff=cutoff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlations\n",
    "\n",
    "Two variables **correlate** when:\n",
    "\n",
    "> Either of the variables are so related that one directly implies or is complementary to the other (from [Merriam-Webster](https://www.merriam-webster.com/dictionary/correlate))\n",
    "\n",
    "Put into mathematics, this simply means that knowing information about one variable _implies_ information about another variables.\n",
    "\n",
    "Let's take a look at an example of a correlation. Here is a positive correlation based upon the revenue from a customer vs. the number of items they purchased:\n",
    "\n",
    "<img src=\"../img/corr_items_revenue.png\" width=\"800\">\n",
    "\n",
    "\n",
    "If someone told you that the number of items a customer bought was increasing, and asked you if the revenue increased too, you'd probably say yes. This is because you know that more items bought by a customer generates more revenue.\n",
    "\n",
    "There are two main types of correlation:\n",
    "\n",
    "> A **positive** correlation implies raising the value of one variable will also raise the value of another variable. Subsequently, lowering the value of one variable will lower the value of another variable.\n",
    "\n",
    "> A **negative** correlation implies the opposite. Raising the value of one variable _lowers_ the value of another variable.\n",
    "\n",
    "There is a numerical measure of correlation, called $r^2$. There are three main points about the $r^2$ value of variables...\n",
    "\n",
    "* A $r^2$ that approaches 1 implies **positive correlation**\n",
    "* A $r^2$ that approaches -1 implies **negative correlation** \n",
    "* A $r^2$ that approaches 0 implies **no correlation**\n",
    "\n",
    "Here are some pictures to show you examples of correlated, non-correlated, and negative correlated variables with corresponding $r^2$ values.\n",
    "\n",
    "<br>\n",
    "\n",
    "![](https://www.statisticshowto.datasciencecentral.com/wp-content/uploads/2012/10/pearson-2-small.png)\n",
    "\n",
    "\n",
    "We can use a [seaborn heatmap](https://seaborn.pydata.org/generated/seaborn.heatmap.html) to show us the correlation of different variables. Let's first create a function that makes a heatmap for us.\n",
    "\n",
    "The function can always be called as so...\n",
    "\n",
    "```python\n",
    "make_heatmap(my_data, columns)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_heatmap(my_data, columns):\n",
    "    \"\"\"\n",
    "    Make a heatmap of data given certain columns.\n",
    "    \n",
    "    inputs: my_data <pd.DataFrame>: The data we want to draw the heatmap of\n",
    "            columns <list>: The columns to draw the heatmap of\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a figure\n",
    "    plt.figure(figsize=(20, 10))\n",
    "\n",
    "    # Draw a heatmap\n",
    "    sns.heatmap(my_data[numerical_cols].corr(), annot=True, fmt=\".1f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First get numerical columns\n",
    "numerical_cols = [i for i in filtered_data.columns if str(filtered_data[i].dtype) != 'object']\n",
    "\n",
    "# Make heatmatp\n",
    "make_heatmap(filtered_data, numerical_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Think about the following...\n",
    "\n",
    "* What variables correlate with each other? Did you expect these to correlate?\n",
    "* Are there any correlations that _do not exist_ that you imagined would be there?\n",
    "* Any new insights on how this might help decipher male vs. female buyers?\n",
    "\n",
    "Why do looking at correlations matter? \n",
    "\n",
    "> Typically, we want to eliminate correlated variables because keeping both variables do not give _any extra information_ about the underlying data.\n",
    "\n",
    "There are a few different ways to eliminate correlation. A simple way to do it is using something called [Principle Component Analysis (PCA)](http://setosa.io/ev/principal-component-analysis/), which creates new variables that reduce the correlation within our data. The issue is that PCA is not very **explainable**, and it's not very transparent how the outputs of PCA reflect your original features.\n",
    "\n",
    "We won't cover PCA in this masterclass, but we'll come back to correlation reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Data pre-processing and feature engineering\n",
    "\n",
    "[Top](#Coder-Academy-and-THE-ICONIC-Masterclass) | [Previous Section](#Part-2:-First-look-at-our-dataset) | [Next Section](#Part-4:-Clustering-the-dataset) | [Bottom](#Wrap-up)\n",
    "\n",
    "We are now going to start using the word **feature** interchangeably **variable**.\n",
    "\n",
    "> A **variable** often refers to the raw-columns within our dataset. A **feature** is an input to our machine learning algorithm. A variable might _become_ a feature, or we might create a feature off of one or more variables.\n",
    "\n",
    "For example, maybe we have data that is supposed to help us predict the price of a house. This dataset might include variables such as the number of bedrooms and bathrooms. What we might realise is that the number of bedrooms and bathrooms individually do not affect the price of the house, but the total number of rooms do. \n",
    "\n",
    "So, what we do is we make a new variable called **total_rooms = bedrooms + bathrooms**, and use this variable in our machine learning algorithm, but _not_ the individual bedrooms and bathrooms. Thus, the **total_rooms** is a _feature_.\n",
    "\n",
    "### Feature creation\n",
    "\n",
    "We can use the variables (columns) in our dataset as a starting point for features in our algorithm. We might also want to create other features in our dataset that help us get a better understanding of our customers. These features might also reduce noise from correlated variables.\n",
    "\n",
    "For example, pretend we have the variable `female_items` that describes the number of items that were categorised as \"female\" bought on the ICONIC website. If customer 1 buys 50 `female_items`, and customer 2 buys 600 `female_items`, does this tell us customer 2 tends to buy female items, or does it tell us simply that customer 2 just tends to buy more? We can see the correlation between `female_items` and `items` is pretty evident by plotting a scatterplot of these two variable below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make scatterplot\n",
    "plt.figure(figsize=(15, 5))\n",
    "sns.scatterplot(x=filtered_data['female_items'], y=filtered_data['items'])\n",
    "plt.title('Items vs. Female Items')\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create one feature to help reduce this correlation. What we'll do is divide the `female_items` by the total `items`, and make a new feature called `pct_female_items`. We'll then plot the `pct_female_items` vs. `items` to show that the correlation is reduced.\n",
    "\n",
    "We'll also drop `female_items`, and also `male_items`, since the `pct_female_items` and `items` together both imply this information.\n",
    "\n",
    "**Thoughts...** why is it ok to drop`male_items`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pct_female_items variable\n",
    "filtered_data['pct_female_items'] = filtered_data['female_items'] / filtered_data['items']\n",
    "\n",
    "# Drop male_items and female_items\n",
    "filtered_data.drop(['male_items', 'female_items'], inplace=True, axis=1)\n",
    "\n",
    "# Plot pct_female_items vs. items\n",
    "plt.figure(figsize=(15, 5))\n",
    "sns.scatterplot(x=filtered_data['pct_female_items'], y=filtered_data['items'])\n",
    "plt.title('Items vs. Pct Female Items')\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty funky shape...but definitely not correlated. We'll leave some time at the end of this lesson to create more features.\n",
    "\n",
    "### Normalising Data\n",
    "\n",
    "Machine learning algorithms are often susceptible to bias based upon the different scales within our dataset. For instance, run the following code, which will show you an example of two columns, `revenue` and `items`, which are on different scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make figure\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# Draw figure\n",
    "sns.boxplot(data=filtered_data[['revenue', 'items']], orient='h')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the revenue distribution is much more varied than the items distribution (which makes sense, as one item might cost $100)! Often times, the machine learning algorithms we will use prioritise splitting datasets towards variables that have a lot of variance. Thus a typical step performed in data pre-processing is called **normalisation**, which will set our data on the same scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardise_data(my_data, cols_to_norm):\n",
    "    \"\"\"\n",
    "    Normalise the dataset.\n",
    "    \n",
    "    inputs: my_data <pd.DataFrame>, the data to normalise\n",
    "            cols_to_norm <list>, the columns to normalise\n",
    "                             \n",
    "    output: The normalised dataset\n",
    "    \"\"\"\n",
    "    # Create a normaliser\n",
    "    sclr = StandardScaler()\n",
    "    \n",
    "    # Fit the dataset\n",
    "    sclr.fit(my_data[cols_to_norm])\n",
    "    norm_my_data = pd.DataFrame(sclr.transform(my_data[cols_to_norm]), columns=cols_to_norm)\n",
    "    \n",
    "    return norm_my_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use all numerical columns as features\n",
    "numerical_cols = [i for i in filtered_data.columns if str(filtered_data[i].dtype) != 'object']\n",
    "\n",
    "# Standardise data\n",
    "norm_data = standardise_data(filtered_data, numerical_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Clustering the dataset\n",
    "\n",
    "[Top](#Coder-Academy-and-THE-ICONIC-Masterclass) | [Previous Section](#Part-3:-Data-pre-processing-and-feature-engineering) | [Next Section](#Part-5:-Classifying-our-inferred-gender) | [Bottom](#Wrap-up)\n",
    "\n",
    "## Introduction to Clustering\n",
    "\n",
    "Clustering allows us to separate the data into groups, or clusters using the features we have created within our data. Here is a gif that shows an example of clustering data into two clusters, based upon two variables: the male items purchased, and the number of mobile device orders.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td style=\"padding:25px\"><img src=\"../img/cluster_w_titles.png\" width=\"350\"></td>\n",
    "        <td style=\"padding:25px\"><img src=\"https://cdn-images-1.medium.com/max/1600/1*WkU1q0Cuha2QKU5JnkcZBw.gif\" width=\"350\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "As you see there are two variables in the dataset, and each colour on the video represents a different cluster, of which there are three total. In this video, we are performing two steps:\n",
    "\n",
    "1. An **Update Cluster Assignment** step: which re-colours each point in space as a new cluster\n",
    "2. An **Update Cluster Centers** step: which moves around the small diamonds to new spaces\n",
    "\n",
    "## Clustering Algorithm\n",
    "\n",
    "We call this method of clustering **K-Means** clustering. This method places **K** diamonds, or centers, randomly within our dataset (in the above example K=3). It then iteratively moves these centers by:\n",
    "\n",
    "1. Labeling the points closest to them as belonging to \"their\" cluster (the **assignment step**)\n",
    "2. Then shifts the centers by putting them at the current center point of the labeled points (the **update step**)\n",
    "\n",
    "Let's run K-Means on our dataset. As you might have guessed, we'll create **two clusters**, one for our inferred males, and one for females. The following function can be used to run K-Means.\n",
    "\n",
    "First we need to import K-Means from the [sklearn library](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html). We will also import the [silhouette score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html), which gives a measure on how well our clustering performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_kmeans(my_data, cont_features):\n",
    "    \"\"\"\n",
    "    Run K-Means on a dataset with a given feature set.\n",
    "    \n",
    "    inputs: my_data <pd.DataFrame>, the dataset to create clusters from\n",
    "            cont_features <list>, the list of features to run K-Means on\n",
    "            \n",
    "    output: the clusters\n",
    "    \"\"\"\n",
    "\n",
    "    # Run kmeans\n",
    "    kmeans = KMeans(n_clusters=2, random_state=42).fit(my_data[cont_features])\n",
    "    pred = kmeans.predict(my_data[cont_features])\n",
    "    \n",
    "    # Score\n",
    "    silhouette = silhouette_score(my_data[cont_features], pred)\n",
    "    print(\"The silhouette score is: \" + str(silhouette))\n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run K-Means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = run_kmeans(norm_data, norm_data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One note on our silhouette score:\n",
    "\n",
    "> The **silhouette score** is a measure of cluster performance. It tells us information about how **compact** and **far apart** our clusters are. \n",
    "\n",
    "If the sihouette score is...\n",
    "\n",
    "* Close to 1.0, it means our clusters are compact and far apart. \n",
    "* Close to -1.0, it means that any data point is closer to an opposing cluster, and should be placed into that opposing cluster rather than the cluster it is currently within."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Classifying our inferred gender\n",
    "\n",
    "[Top](#Coder-Academy-and-THE-ICONIC-Masterclass) | [Previous Section](#Part-4:-Clustering-the-dataset) | [Next Section](#Part-6:-Putting-it-altogether-and-next-steps) | [Bottom](#Wrap-up)\n",
    "\n",
    "Each cluster we have made represents the \"inferred\" gender. Though we do not have the actual gender, if the differentiation of these clusters corresponds to the qualitative knowledge we know about male vs. female consumers, we can have some level of confidence that these are two differentiated groups of people. \n",
    "\n",
    "Even if these groups do not correspond to the actual gender of a person, our feature space can inform us about how we could tailor marketing to each of these groups. What we then need to do is create an algorithm that captures _new_ information and classifies this new information into each of our inferred gender classes.\n",
    "\n",
    "\n",
    "Here's a visual of this process:\n",
    "\n",
    "---\n",
    "\n",
    "![](../img/ICONIC_Classification_Drawing.png)\n",
    "\n",
    "---\n",
    "\n",
    "We'll look at a simple algorithm to do this, called a randaom forest which is very _transparent_ about how our features inform the cluster labels.\n",
    "\n",
    "**Note:** Random forests are very simple algorithms to train and test, because they are fast, and it is easy to peel back the layers of the algorithm and see how changes in the inputs affect the results of the prediction. After training a random forest getting an understanding about the underlying features that infer gender, one might use something more sophisticated, such as a [neural network](https://www.digitaltrends.com/cool-tech/what-is-an-artificial-neural-network/) using the [Tensorflow](https://www.tensorflow.org/tutorials/) library, to create more accurate predictions.\n",
    "\n",
    "That being said, a good mantra to follow in data science is...**if the simplest solution works, stick with it!**\n",
    "\n",
    "\n",
    "## Classification Algorithm - Random Forest\n",
    "\n",
    "A random forest is a classification algorithm that can be used to classify new data into our two classes. From [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html):\n",
    "\n",
    "> A **random forest** is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.\n",
    "\n",
    "To break this down a little further, random forest's create trees of data by splitting the data along features. The goal is to use these split points to accurately sort our data into the classes at hand.\n",
    "\n",
    "The following image shows an example of using a tree of data to classify whether someone survived the Titanic disaster (or not) using the gender, age, and cabin class of a passenger.\n",
    "\n",
    "---\n",
    "\n",
    "<img src=\"../img/THE_ICONIC_RF.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "---\n",
    "\n",
    "Random forests create multiple trees from different samples of a dataset. When one uses the forest to then predict a class, that sample's prediction is calculated from each tree, and the majority prediction is used as the final class label.\n",
    "\n",
    "The following code will [import the RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) from the sklearn library. It will also import the [StratifiedKFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html) module from sklearn, which is a tool that will help us gain better knowledge of how our algorithm will perform on new data, and the [cross_val_score](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html) module, which will score the effectiveness of these models.\n",
    "\n",
    "We also import the [time](https://docs.python.org/3/library/time.html) module, which lets us evaluate model training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function will allow us to train and validate the performance of a random forest classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_random_forest(my_data, features, target_var, k=10, n_estimators=100, max_depth=2):\n",
    "    \"\"\"\n",
    "    Train and test a random forest using K-Fold cross validation.\n",
    "    \n",
    "    inputs: my_data <pd.DataFrame>, the input data\n",
    "            features <list>, the list of features\n",
    "            target_var <str>, the name of the column to predict\n",
    "            k <int>, the number of folds to use for the algorithm\n",
    "    outputs: model <RandomForestClassifier>, the final trained random forest classifier\n",
    "             results <list>, f1 score of the final results\n",
    "             feat_importance <pd.DataFrame>, the importance of each feature in a dataframe\n",
    "             \n",
    "    \"\"\"\n",
    "    # Start time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Create a RandomForestClassifier\n",
    "    clf = RandomForestClassifier(\n",
    "        n_estimators=n_estimators, \n",
    "        max_depth=max_depth,\n",
    "        random_state=42,\n",
    "        class_weight=\"balanced_subsample\",\n",
    "    )\n",
    "    \n",
    "    # Train with StratifiedKFold\n",
    "    kfold = StratifiedKFold(n_splits=k, random_state=42)\n",
    "    results = cross_val_score(clf, my_data[features], my_data[target_var], cv=kfold, scoring='f1')\n",
    "    \n",
    "    # Print result\n",
    "    print(\"F1: Mean %.3f +/- (%.3f)\" % (results.mean(), results.std()))\n",
    "\n",
    "    # Fit with all data\n",
    "    clf.fit(my_data[features], my_data[target_var])\n",
    "    \n",
    "    # Feature importance\n",
    "    feat_importance = pd.DataFrame(\n",
    "        clf.feature_importances_, index=features, columns=['Importance']\n",
    "    ).sort_values(['Importance'], ascending=False)\n",
    "    feat_importance['Index'] = range(feat_importance.shape[0])\n",
    "    \n",
    "    # Graph\n",
    "    feat_importance_cut = feat_importance.loc[feat_importance['Importance'] > 0.01, :]\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    sns.pointplot(x='Index', y='Importance', data=feat_importance_cut, linestyles='')\n",
    "    plt.xlabel(xlabel='')\n",
    "    for i, ind in enumerate(feat_importance_cut.index.values):\n",
    "        x = feat_importance.loc[ind, 'Index']\n",
    "        y = feat_importance.loc[ind, 'Importance']\n",
    "        plt.text(x+0.08, y, ind, fontsize=9)\n",
    "        \n",
    "    # End\n",
    "    end_time = time.time()\n",
    "    print('Elasped time: %.2f seconds' % (end_time - start_time))\n",
    "    \n",
    "    # Return the model and the feature importance\n",
    "    return clf, results, feat_importance[['Importance']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to train the algorithm and take a look at the outputs. We will go back to using the filtered_data we originally created, to give us an idea about how the _original_ columns factored into our cluster creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use all numerical columns as features\n",
    "numerical_cols = [i for i in filtered_data.columns if str(filtered_data[i].dtype) != 'object']\n",
    "\n",
    "rf_data = filtered_data.copy()\n",
    "rf_data['Clusters'] = clusters\n",
    "model, results, feat_importance = create_random_forest(rf_data, numerical_cols, 'Clusters', k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forests work really well in the following cases:\n",
    "\n",
    "* They train **very fast**\n",
    "* Random forests do not require a lot of **preprocessing** to use\n",
    "* They decrease the likelihood to **overfitting**, meaning, they generalise well to new data\n",
    "* They are **transparent**, meaning, we know the impact of how features in our model impact the class distinction\n",
    "\n",
    "The graph printed out shows all features that had an importance over 0.01 for our prediction. The _higher_ the importance, the more relevant the feature is towards destinguishing classes. \n",
    "\n",
    "In addition, we used an \"F1\" score to determine how well our algorithm performed. An \"F1\" score closer to 1.0 means that our algorithm is achieving perfect predictions. An [F1 score](https://en.wikipedia.org/wiki/F1_score) is commonly used when the classes we are predicting are _imbalanced_, meaning we do not necessarily have 50% of our data in one class, and 50% of our data within another class.\n",
    "\n",
    "### Optional: Parameter Tuning and Cross-validation\n",
    "\n",
    "There are a few other parameters we have not talked about yet that can affect how good our model is, namely, the **n_estimators**, and the **max_depth**\n",
    "\n",
    "> The **number of estimators**, or **n_estimators** describes the number of trees used for the forest. Generally, _more trees_ reduce overfitting, but lower the model performance on our training data.\n",
    "\n",
    "> The **max_depth**, deepens our tree. The more depth, the better the fit to our current dataset. Usually the larger depth, the more susceptible we are to overfitting. \n",
    "\n",
    "We call the process of tuning parameters, such as n_estimators and max_depth, cross-validation.\n",
    "\n",
    "### Exercise\n",
    "\n",
    "The following code has two variables that can be used to change the `n_estimators` and `max_depth` used within our model. Two example values are given. Play with the parameters and check how they impact the accuracy of the model. Also look how the total run time of the model is influenced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use all numerical columns as features\n",
    "numerical_cols = [i for i in filtered_data.columns if str(filtered_data[i].dtype) != 'object']\n",
    "\n",
    "# Set variables\n",
    "n_estimators = 100\n",
    "max_depth = 50\n",
    "\n",
    "# Run the model\n",
    "model, results, feat_importance = create_random_forest(\n",
    "    rf_data, numerical_cols, 'Clusters', k=10, n_estimators=n_estimators, max_depth=max_depth\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 6: Putting it altogether and next steps\n",
    "\n",
    "[Top](#Coder-Academy-and-THE-ICONIC-Masterclass) | [Previous Section](#Part-5:-Classifying-our-inferred-gender) | [Next Section](#Wrap-up) | [Bottom](#Wrap-up)\n",
    "\n",
    "Let's put it all together. The following function will combine _all_ the steps we have done above into a single pipeline. Here's a visual to show all of the steps in numbered order.\n",
    "\n",
    "---\n",
    "\n",
    "<img src=\"../img/ICONIC_Full_Pipeline_Drawing.png\" width=\"700\">\n",
    "\n",
    "---\n",
    "\n",
    "Note there are some new pieces of the puzzle we have not dealt with.\n",
    "\n",
    "* When we ran our random forest, we saw what features had an impact towards our cluster labels. Usually data scientists have a good **qualitative understanding** of what should influence their outcome. Did the features that were important within the random forest match your understanding? If not, there are a few things we could do...\n",
    "  * We could remove these features, and limit which columns we use for clustering\n",
    "  * We could create more new features, which might help eliminate some of the correlations. Remember, we did this with the `pct_female_items` column we created\n",
    "\n",
    "* To actually process our new data, we would have to somehow host our trained algorithm. Tools like [Amazon Sagemaker](https://aws.amazon.com/sagemaker/) allow for easy hosting of machine learning algorithms. We won't go into this tonight, but it might be good to check out one of these tools.\n",
    "\n",
    "As you see, **most** of these decisions involve data cleaning, and most of a data scientist's job involves data cleaning! We need to make sure our algorithms are _understandable_, since there is uncertainty in the accuracy of our predictions.\n",
    "\n",
    "## Setting up our pipeline\n",
    "\n",
    "Run the following code which creates the entire pipline. It...\n",
    "\n",
    "1. Transforms the data using normalisation\n",
    "2. Clusters the data based upon these transformed features\n",
    "3. Runs a random forest to gauge how features contributes to our classification\n",
    "4. Outputs the random forest model to predict new data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gender_inference_pipeline(\n",
    "    my_data,\n",
    "    features,\n",
    "    k_folds=5,\n",
    "    rf_estimators=100,\n",
    "    rf_depth=10,\n",
    "):\n",
    "    \"\"\"\n",
    "    Run entire pipeline based upon a given number of features\n",
    "    \n",
    "    inputs: my_data <pd.DataFrame>, the dataset\n",
    "            features <list>, the list of variables to use for the data\n",
    "            k_folds <int>, the number of folds to train upon\n",
    "            rf_estimators <int>, number of trees in the rf\n",
    "            rf_depth <int>, the depth of each tree\n",
    "            \n",
    "    outputs: The final model, f1_scores from training, feature importance, and inferred gender\n",
    "    \"\"\"\n",
    "    # Print features\n",
    "    print('Using the following features to infer gender: ')\n",
    "    print('*'* 54)\n",
    "    for i in features:\n",
    "        print('* ' + i + ' ' * (50 - len(i)) + ' *')\n",
    "    print('*'* 54)\n",
    "    print('')\n",
    "\n",
    "    \n",
    "    # Reset index\n",
    "    my_data = my_data.reset_index(drop=True)\n",
    "    \n",
    "    # Normalise data\n",
    "    norm_data = standardise_data(my_data, features)\n",
    "    \n",
    "    # Cluster data\n",
    "    clusters = run_kmeans(norm_data, features)\n",
    "    \n",
    "    # Train random forest\n",
    "    rf_data = my_data.copy()\n",
    "    rf_data['Clusters'] = clusters\n",
    "    model, results, feat_importance = create_random_forest(\n",
    "        rf_data, features, 'Clusters', k=k_folds, n_estimators=rf_estimators, max_depth=rf_depth\n",
    "    )\n",
    "\n",
    "    return model, results, feat_importance, clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following to train the entire pipeline with filtered data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We will use all numerical columns as features\n",
    "numerical_cols = [i for i in filtered_data.columns if str(filtered_data[i].dtype) != 'object']\n",
    "\n",
    "# Run the model\n",
    "end_model, f1_scores, feat_importance, clusters = gender_inference_pipeline(\n",
    "    my_data=filtered_data,\n",
    "    features=numerical_cols,\n",
    "    k_folds=5,\n",
    "    rf_estimators=100,\n",
    "    rf_depth=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "There are a **ton** of parameters we could change within our model. I've also recopied the list of columns within our data below.\n",
    "\n",
    "| Column                   | Value   | Description                                                              | \n",
    "|--------------------------|---------|--------------------------------------------------------------------------| \n",
    "| customer_id              | string  | ID of the customer - super duper hashed                                  | \n",
    "| days_since_first_order   | integer | Days since the first order was made                                      | \n",
    "| days_since_last_order    | integer | Days since the last order was made                                       | \n",
    "| int_is_newsletter_subscriber | string  | Flag for a newsletter subscriber (1 = Yes, 0 = No)                                        | \n",
    "| orders                   | integer | Number of orders                                                         | \n",
    "| items                    | integer | Number of items                                                          | \n",
    "| cancels                  | integer | Number of cancellations - when the order is cancelled after being placed | \n",
    "| returns                  | integer | Number of returned orders                                                | \n",
    "| different_addresses      | integer | Number of times a different billing and shipping address was used        | \n",
    "| shipping_addresses       | integer | Number of different shipping addresses used                              | \n",
    "| devices                  | integer | Number of unique devices used                                            | \n",
    "| vouchers                 | integer | Number of times a voucher was applied                                    | \n",
    "| cc_payments              | integer | Binary indicating if credit card was used for payment                       | \n",
    "| paypal_payments          | integer | Binary indicating if PayPal was used for payment                              | \n",
    "| afterpay_payments        | integer | Binary indicating if AfterPay was used for payment                            | \n",
    "| apple_payments           | integer | Binary indicating if Apple Pay was used for payment                           | \n",
    "| pct_female_items             | integer | Percentage of items purchased for women                                         | \n",
    "| unisex_items             | integer | Number of unisex items purchased                                         | \n",
    "| wapp_items               | integer | Number of Women Apparel items purchased                                  | \n",
    "| wftw_items               | integer | Number of Women Footwear items purchased                                 | \n",
    "| mapp_items               | integer | Number of Men Apparel items purchased                                    | \n",
    "| wacc_items               | integer | Number of Women Accessories items purchased                              | \n",
    "| macc_items               | integer | Number of Men Accessories items purchased                                | \n",
    "| mftw_items               | integer | Number of Men Footwear items purchased                                   | \n",
    "| wspt_items               | integer | Number of Women Sport items purchased                                    | \n",
    "| mspt_items               | integer | Number of Men Sport items purchased                                      | \n",
    "| curvy_items              | integer | Number of Curvy items purchased                                          | \n",
    "| sacc_items               | integer | Number of Sport Accessories items purchased                              | \n",
    "| msite_orders             | integer | Number of Mobile Site orders                                             | \n",
    "| desktop_orders           | integer | Number of Desktop orders                                                 | \n",
    "| android_orders           | integer | Number of Android app orders                                             | \n",
    "| ios_orders               | integer | Number of iOS app orders                                                 | \n",
    "| other_device_orders      | integer | Number of Other device orders                                            | \n",
    "| work_orders              | integer | Number of orders shipped to work                                         | \n",
    "| home_orders              | integer | Number of orders shipped to home                                         | \n",
    "| parcelpoint_orders       | integer | Number of orders shipped to a parcelpoint                                | \n",
    "| other_collection_orders  | integer | Number of orders shipped to other collection points                      | \n",
    "| average_discount_onoffer | float   | Average discount rate of items typically purchased                       | \n",
    "| average_discount_used    | float   | Average discount finally used on top of existing discount                | \n",
    "| revenue                  | float   | $ Dollar spent overall per person                                        |\n",
    "\n",
    "#### Your goal is to change the...\n",
    "\n",
    "* Features used in the model by editing the `features` list\n",
    "* If you want to be an adventurous coder, you could create more features\n",
    "* We could also filter out more outliers in our dataset, based upon your results\n",
    "* Other parameters (for those who know how to do parameter tuning)...\n",
    "  * The depth of the random forest\n",
    "  * The number of random forest estimators\n",
    "\n",
    "First see...\n",
    "\n",
    "1. How do variables separate the inferred gender?\n",
    "2. How do modifying the other parameters change your model accuracy?\n",
    "\n",
    "We will also create an [sns.pairplot](https://seaborn.pydata.org/generated/seaborn.pairplot.html) visualisation of the top three features in the model, based upon these clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "feature_list = ['desktop_orders', 'days_since_last_order', 'pct_female_items', 'android_orders']\n",
    "\n",
    "# Affects random forest\n",
    "rf_estimators = 100\n",
    "rf_depth = 50\n",
    "k_folds = 2\n",
    "\n",
    "# Run the model\n",
    "end_model, f1_scores, feat_importance, clusters = gender_inference_pipeline(\n",
    "    my_data=filtered_data,\n",
    "    features=feature_list,\n",
    "    k_folds=5,\n",
    "    rf_estimators=100,\n",
    "    rf_depth=50,\n",
    ")\n",
    "\n",
    "# Create pair plot\n",
    "pairplot_data = filtered_data[feat_importance.index[range(min(3, feat_importance.shape[0]))]].copy()\n",
    "pairplot_data['Inferred Gender'] = ['Cluster 1' if i == 1 else 'Cluster 0' for i in clusters]\n",
    "sns.pairplot(\n",
    "    pairplot_data, \n",
    "    hue='Inferred Gender', \n",
    "    plot_kws={'alpha': 0.6, 's': 50, 'edgecolor': 'k'}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the model to predict an inferred gender\n",
    "\n",
    "The function above outputted an `end_model` variable with our trained random froest. We could use this to predict an inferred gender on any data. Here's an example, using the first value from our training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predit gender\n",
    "gender = end_model.predict(filtered_data[feature_list].loc[[0], :])\n",
    "\n",
    "print('Inferred gender(s): ' + str(gender))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrap-up\n",
    "\n",
    "Thank you for attending our masterclass! We hope we _demystified_ a little bit of what actually occurs when you build a machine learning process. Big thank you to Kshira Saagar from THE ICONIC for his time and lending us data for the workshop.\n",
    "\n",
    "We will be sending out a **survey over email** to get your feedback on the session!\n",
    "\n",
    "If you would like to download your work for today, please click **File->Download As->.html**. You will not be able to run the cells, but you will be able to view the material you learned within a web browser."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
