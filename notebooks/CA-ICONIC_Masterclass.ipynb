{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coder Academy and THE ICONIC Masterclass\n",
    "\n",
    "This notebook has the following sections...\n",
    "\n",
    "* [Part 0: Using this notebook](#Part-0:-Using-this-notebook)\n",
    "* [Part 1: Introduction](#Part-1:-Introduction)\n",
    "* [Part 2: First look at our dataset](#Part-2:-First-look-at-our-dataset)\n",
    "* [Part 3: Data pre-processing and feature engineering](#Part-3:-Data-pre-processing-and-feature-engineering)\n",
    "* [Part 4: Clustering the dataset](#Part-4:-Clustering-the-dataset)\n",
    "* [Part 5: Classifying our inferred gender](#Part-5:-Classifying-our-inferred-gender)\n",
    "* [Part 6: Putting it altogether and next steps](#Part-6:-Putting-it-altogether-and-next-steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../img/iconic_coder.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 0: Using this notebook\n",
    "\n",
    "[Top](#Coder-Academy-and-THE-ICONIC-Masterclass) | [Previous Section](#Coder-Academy-and-THE-ICONIC-Masterclass) | [Next Section](#Part-1:-Introduction) | [Bottom](#Wrap-up)\n",
    "\n",
    "## What is Python?\n",
    "\n",
    "Python is an _interpretive_ programming language invented in the 1980s. It's actually named after Monty Python and Holy Grail. In this class we'll be using Python to build our machine learning algorithms. \n",
    "\n",
    "### Why learn Python?\n",
    "\n",
    "Python has gained popularity because it has an easier syntax (rules to follow while coding) than many other programming languages. Python is very diverse in its applications which has led to its adoption in areas such as data science and web development.\n",
    "\n",
    "All of the following companies actively use Python:\n",
    "\n",
    "![Image](https://www.probytes.net/wp-content/uploads/2018/08/appl.png)\n",
    "\n",
    "## How do I interact with this notebook?\n",
    "\n",
    "A Jupyter Notebook is an interactive way to work with code in a web browser. Jupyter is a pseudo-acronym for three programming languages: Julia, python and (e)r. Notebooks provide a format to add instructions + code in one file, which is why we're using it!\n",
    "\n",
    "We'll quickly do some practice to introduce you how to use this notebook. For a list of keyboard shortcuts you can take a look at [Max Melnick's](http://maxmelnick.com/2016/04/19/python-beginner-tips-and-tricks.html) beginner tips for Jupyter Notebook.\n",
    "\n",
    "Here's a quick run down of some of the most basic commands to use:\n",
    "\n",
    "- A cell with a **<span style=\"color:blue\">blue</span>** background is in **Command Mode**. This will allow you to toggle up/down cells using the arrow keys. You can press enter/return on a cell in command mode to enter edit mode\n",
    "\n",
    "- A cell with a **<span style=\"color:green\">green</span>** background is in **Edit Mode**. This will allow you to change the content of cells. You can press the escape key on a cell in command mode to enter edit mode\n",
    "\n",
    "- To run the contents of a cell, you can type:\n",
    "  - `cmd + enter`, which will run the cotents of a cell and keep the cursor in place\n",
    "  - `shift + enter`, which will run the contents of a cell, and move the cursor to the next cell (or create a new cell)\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Edit the below by changing \"Gretchen\" to your own name by entering edit mode, and then running the cell using the directions above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hello, Gretchen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add/delete cells using the following commands in <span style=\"color:blue\">**Command Mode**</span>:\n",
    "\n",
    "- `a`, adds a cell above the current cell\n",
    "- `b`, adds a cell below the current cell\n",
    "- `d + d`, (pressing the \"d\" key twice in succession) deletes a cell\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Add/delete the cells such that each individual cell prints the numbers 1-5 in order. The numbers 2 and 4 are already completed for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Introduction\n",
    "\n",
    "[Top](#Coder-Academy-and-THE-ICONIC-Masterclass) | [Previous Section](#Part-0:-Using-this-notebook) | [Next Section](#Part-2:-First-look-at-our-dataset) | [Bottom](#Wrap-up)\n",
    "\n",
    "## The problem\n",
    "\n",
    "As explained, [THE ICONIC](https://www.theiconic.com.au/) does not receive all information about a person when they create an online profile, but the more information they receive from a person, the better they can tailor their marketing towards specific individuals.\n",
    "\n",
    "The goal of this masterclass is to develop a way to infer information about an individual by aligning buying behaviours with demographic traits.\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Take 10 minutes to do some internet research, and try to find out how online buying behaviours differ between genders. Write down your findings - they might inform how you build your algorithms for the remainder of the masterclass!\n",
    "\n",
    "\n",
    "## Data Pipeline\n",
    "\n",
    "In this session, we'll build a **data pipeline** to infer gender based upon behavioural data. From [wikipedia](https://en.wikipedia.org/wiki/Pipeline_(computing))...\n",
    "\n",
    "> In computing, a **pipeline** (also known as a **data pipeline**) is a set of data processing elements connected in series, where the output of one element is the input of the next one. The elements of a pipeline are often executed in parallel or in time-sliced fashion.\n",
    "\n",
    "Data pipelines deal with the process of collecting, modifying and analysing a dataset towards some goal. Here's a picture of a data pipeline from [this medium blog](https://medium.com/the-data-experience/building-a-data-pipeline-from-scratch-32b712cfb1db)...\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/1600/1*8-NNHZhRVb5EPHK5iin92Q.png)\n",
    "\n",
    "For the rest of this lesson we'll build-up this pipeline. We will....\n",
    "\n",
    "1. Analyse our dataset, by taking a look at the columns available and create **usuable features** for our algorithms\n",
    "2. Process the dataset, by using **normalisation** and **PCA** to get rid of noise\n",
    "3. **Infer a gender** on our dataset using clustering\n",
    "4. Analyse our inferred gender, and build a **classification algorithm** that can be used to predict our inferred gender from new data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: First look at our dataset\n",
    "\n",
    "[Top](#Coder-Academy-and-THE-ICONIC-Masterclass) | [Previous Section](#Part-1:-Introduction) | [Next Section](#Part-3:-Data-pre-processing-and-feature-engineering) | [Bottom](#Wrap-up)\n",
    "\n",
    "Let's start to look at the data available to us. This data has been provided by the **THE ICONIC**. It has been **de-identified**, meaning it has been modified in such a way where the data could not lead back to the actual population of individuals it represents.\n",
    "\n",
    "## Pandas introduction\n",
    "\n",
    "To analyse our dataset with Python we should first load the dataset. To do this, we will use the [Pandas](https://pandas.pydata.org/) module.\n",
    "\n",
    "> A **module** is a set of code-files that can be loaded to add additional capabilities to our program\n",
    "\n",
    "Pandas allows us to manipulate tabular data in Python. Let's import pandas so that we can use it for the rest of our Python session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas and give it the nickname \"pd\"\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can upload our data set into Python, and take a quick look at the data available to us.\n",
    "\n",
    "---\n",
    "\n",
    "Run the following code cell which will do the following...\n",
    "\n",
    "1. Upload our data into Python, specifically within a variable called `raw_data`\n",
    "2. Print a brief description of each column in our dataset, including...\n",
    " * The type of data, which will be called `<class 'pandas.core.frame.DataFrame'>`\n",
    " * The type of `index`, or row names from the data: `RangeIndex: 46030 entries, 0 to 46029`\n",
    " * Number of data columns: `Data columns (total 42 columns)`\n",
    " * The column names and information\n",
    "\n",
    "\n",
    "For example, the following describes a column called `afterpay_payments`, of which there are `46030` filled-in values for this column (non-blank), and the column is a number, or `int64`.\n",
    "\n",
    "```\n",
    "afterpay_payments                46030 non-null int64\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the dataset\n",
    "raw_data = pd.read_csv('../data/data_iconic_workshop.csv.gz')\n",
    "\n",
    "# Print basic information about the dataset\n",
    "raw_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So...what do these columns mean? Some of them might be pretty obvious, like `orders` probably represents the number of orders for a customer. But what might `sacc_items` mean? Here's a little more information about the dataset to make life easier for you. \n",
    "\n",
    "| Column                   | Value   | Description                                                              | \n",
    "|--------------------------|---------|--------------------------------------------------------------------------| \n",
    "| customer_id              | string  | ID of the customer - super duper hashed                                  | \n",
    "| days_since_first_order   | integer | Days since the first order was made                                      | \n",
    "| days_since_last_order    | integer | Days since the last order was made                                       | \n",
    "| int_is_newsletter_subscriber | string  | Flag for a newsletter subscriber (1 = Yes, 0 = No)                                        | \n",
    "| orders                   | integer | Number of orders                                                         | \n",
    "| items                    | integer | Number of items                                                          | \n",
    "| cancels                  | integer | Number of cancellations - when the order is cancelled after being placed | \n",
    "| returns                  | integer | Number of returned orders                                                | \n",
    "| different_addresses      | integer | Number of times a different billing and shipping address was used        | \n",
    "| shipping_addresses       | integer | Number of different shipping addresses used                              | \n",
    "| devices                  | integer | Number of unique devices used                                            | \n",
    "| vouchers                 | integer | Number of times a voucher was applied                                    | \n",
    "| cc_payments              | integer | Binary indicating if credit card was used for payment                       | \n",
    "| paypal_payments          | integer | Binary indicating if PayPal was used for payment                              | \n",
    "| afterpay_payments        | integer | Binary indicating if AfterPay was used for payment                            | \n",
    "| apple_payments           | integer | Binary indicating if Apple Pay was used for payment                           | \n",
    "| female_items             | integer | Number of items purchased for women                                         | \n",
    "| male_items               | integer | Number of items purchased for men                                           | \n",
    "| unisex_items             | integer | Number of unisex items purchased                                         | \n",
    "| wapp_items               | integer | Number of Women Apparel items purchased                                  | \n",
    "| wftw_items               | integer | Number of Women Footwear items purchased                                 | \n",
    "| mapp_items               | integer | Number of Men Apparel items purchased                                    | \n",
    "| wacc_items               | integer | Number of Women Accessories items purchased                              | \n",
    "| macc_items               | integer | Number of Men Accessories items purchased                                | \n",
    "| mftw_items               | integer | Number of Men Footwear items purchased                                   | \n",
    "| wspt_items               | integer | Number of Women Sport items purchased                                    | \n",
    "| mspt_items               | integer | Number of Men Sport items purchased                                      | \n",
    "| curvy_items              | integer | Number of Curvy items purchased                                          | \n",
    "| sacc_items               | integer | Number of Sport Accessories items purchased                              | \n",
    "| msite_orders             | integer | Number of Mobile Site orders                                             | \n",
    "| desktop_orders           | integer | Number of Desktop orders                                                 | \n",
    "| android_orders           | integer | Number of Android app orders                                             | \n",
    "| ios_orders               | integer | Number of iOS app orders                                                 | \n",
    "| other_device_orders      | integer | Number of Other device orders                                            | \n",
    "| work_orders              | integer | Number of orders shipped to work                                         | \n",
    "| home_orders              | integer | Number of orders shipped to home                                         | \n",
    "| parcelpoint_orders       | integer | Number of orders shipped to a parcelpoint                                | \n",
    "| other_collection_orders  | integer | Number of orders shipped to other collection points                      | \n",
    "| average_discount_onoffer | float   | Average discount rate of items typically purchased                       | \n",
    "| average_discount_used    | float   | Average discount finally used on top of existing discount                | \n",
    "| revenue                  | float   | $ Dollar spent overall per person                                        |\n",
    "\n",
    "\n",
    "We have already performed some data cleaning for you to save time. To give some perspective on the data cleaning process, since **a lot of a data scientist's job is to clean data** we have...\n",
    "\n",
    "* Removed null values, or blank values, within the data\n",
    "* Confirmed units within the columns are appropriate\n",
    "* Changed relevant string variables to numerical values (computers really do not like text...)\n",
    "\n",
    "You can see that our data has a _ton_ of different information about a customer. Let's just take a look at a single row of data, which represents a specific customer's buying patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the first row of data\n",
    "raw_data[0:1].transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In data analysis, a **five-number summary** is another way to help us understand a dataset. It is a summary that consists of five values: \n",
    "\n",
    "1. the most extreme values in the data set, which include the **maximum**\n",
    "2. and the **minimum**\n",
    "3. the **lower quartile** (bottom 25% of the total values fall within this)\n",
    "4. and **upper quartile** (top 25% of the values fall within this)\n",
    "5. and the **median** (Half the values are above this and half below, it is the 50% mark). \n",
    "\n",
    "Pandas offers a way to find the five-number summary for every individual column in a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an idea about the type of data available to us, it might be helpful to start prying the dataset for useful trends.\n",
    "\n",
    "Remember we are trying to categorise our customers as males and females. We know a little bit about what purchasing behaviours for males and females looks like, so it might be helpful to see whether these qualitative trends are evident in our dataset. If so, we might be able to utilise these features to **cluster** or **separate** the individuals in our dataset into their respective male/female groups.\n",
    "\n",
    "Looking at raw data is tough to do...especially when we have ~45,000 rows to deal with in our dataset.\n",
    "\n",
    "What might be more helpful is to visualise our dataset (humans like visuals over text!). We need to have questions in-hand, as there are 40 columns to look at, and there's no easy way to look at every column at once.\n",
    "\n",
    "![](https://www.quantinsti.com/wp-content/uploads/2017/07/seaburn-1.png)\n",
    "\n",
    "The following code will import the [`matplotlib`](https://matplotlib.org/) and [`seaborn`](https://matplotlib.org/) libraries, the two main libraries we will use for data visualisation within this masterclass.\n",
    "\n",
    "It will also run the \n",
    "\n",
    "```python\n",
    "%matplotlib inline\n",
    "```\n",
    "\n",
    "command, which will allow us to render the images we create _within_ our notebook, instead of explicitly running a command each time to show the graphs/plots we generate with seaborn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import seaborn\n",
    "import seaborn as sns\n",
    "\n",
    "# State to render images inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributions\n",
    "\n",
    "Something that we often like to look at are the **distributions** of our dataset. One way to visualise the distribution of our data is by using a boxplot.\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*2c21SkzJMf3frPXPAR_gZA.png\" width=\"500\">\n",
    "\n",
    "A boxplot visualises the **five number summary** we spoke about earlier. It shows...\n",
    "\n",
    "1. The minimum value\n",
    "2. The maximum value\n",
    "3. Different dividers of our dataset, including the 25th, 50th (median), and 75th quantile\n",
    "\n",
    "When you plot the data with a boxplot, you may observe that some of the points seem far away from the majority of values. These are called **outliers**. For example, if you received a dataset that described the height of staff at your workplace, and there were a few people that were 2.1 meters tall, they would be outliers, because people aren't normally 2.1 meters tall.\n",
    "\n",
    "From [nist](https://www.itl.nist.gov/div898/handbook/prc/section1/prc16.htm):\n",
    "\n",
    "> An **outlier** is an observation that lies an abnormal distance from other values in a random sample from a population. In a sense, this definition leaves it up to the analyst (or a consensus process) to decide what will be considered abnormal. Before abnormal observations can be singled out, it is necessary to characterize normal observations.\n",
    "\n",
    "### Exercise\n",
    "\n",
    "The following code uses the [sns.boxplot](https://seaborn.pydata.org/generated/seaborn.boxplot.html) command to visualise variables within our dataset. It iterates through an input list of variables to show us these distributions.\n",
    "\n",
    "Take a look at the variables available to us, and choose some variables you are interested in visualising, and add them to the list below. Data points which have outliers are typically far-away from the center box within the box plot.\n",
    "\n",
    "Take note of which features have a lot of outliers as you visualise the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of features\n",
    "boxplot_vars = ['male_items', 'female_items']\n",
    "\n",
    "# Create boxplot\n",
    "for v in boxplot_vars:\n",
    "    plt.figure(figsize=(10, 3))\n",
    "    sns.boxplot(raw_data[v])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier filtering\n",
    "\n",
    "It is good to get rid of really extreme points if they are non-normal to a dataset. Also outliers can make it _really hard_ to visualise data!\n",
    "\n",
    "Run the following cell to define a function we can use for outlier filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_cutoff(my_data, cols, cutoff=1.5):\n",
    "    \"\"\"\n",
    "    Filter out outliers from a dataset within a set of columns and using a specified\n",
    "    cuttoff values.\n",
    "    \n",
    "    inputs: my_data <pd.DataFrame>: A dataset\n",
    "            cols <list>: A list of columns to filter\n",
    "            cutoff <float>: A cutoff value\n",
    "            \n",
    "    output: The filtered DataFrame\n",
    "    \"\"\"\n",
    "    # Get original number of columns\n",
    "    orig_num_rows = my_data.shape[0]\n",
    "        \n",
    "    # Go through columns\n",
    "    for c in cols:\n",
    "        # Get IQR\n",
    "        percentile_25 = my_data[c].quantile(q=0.25)\n",
    "        percentile_75 = my_data[c].quantile(q=0.75)\n",
    "\n",
    "        # Calculate cutoff * IQR\n",
    "        iqr = percentile_75 - percentile_25\n",
    "        high_cut = percentile_75 + (iqr * cutoff)\n",
    "        low_cut = percentile_25 - (iqr * cutoff)\n",
    "\n",
    "        # Filter\n",
    "        my_data = my_data.loc[(my_data[c] >= low_cut) & (my_data[c] <= high_cut), :]\n",
    "    \n",
    "    # Print the amount of data lost\n",
    "    print('Number of columns eliminated: '  + str(orig_num_rows - my_data.shape[0]))\n",
    "    \n",
    "    return my_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Add variables to the 'cols_to_filter' list below to filter outliers within those columns. **NOTE** the more columns we filter, and the _lower_ the cutoff, the more data we will lose, which is not good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to filter\n",
    "cols_to_filter = ['items', 'revenue']\n",
    "cutoff = 10.0\n",
    "\n",
    "# Filter data\n",
    "filtered_data = outlier_cutoff(raw_data.copy(), cols_to_filter, cutoff=cutoff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlations\n",
    "\n",
    "Two variable **correlate** when:\n",
    "\n",
    "> Either of the variables are so related that one directly implies or is complementary to the other (from [Merriam-Webster](https://www.merriam-webster.com/dictionary/correlate))\n",
    "\n",
    "Put into mathematics, this simply means that knowing information about one variable _implies_ information about another variables.\n",
    "\n",
    "Let's take a look at an example of a correlation. Here is a positive correlation based upon the price of a barrel of oil vs. a gallon of gas:\n",
    "\n",
    "<img src=\"http://www.randalolson.com/wp-content/uploads/oil-vs-gas-prices.png\" width=\"500\">\n",
    "\n",
    "\n",
    "If someone told you that the price of oil was increasing, and asked you if the price of gas would increase too, you'd probably say yes. This is because you know that the price of oil is related to the price of gas.\n",
    "\n",
    "There are two main types of correlation:\n",
    "\n",
    "> A **positive** correlation implies raising the value of one variable will also raise the value of another variable. Subsequently, lowering the value of one variable will lower the value of another variable.\n",
    "\n",
    "> A **negative** correlation implies the opposite. Raising the value of one variable _lowers_ the value of another variable.\n",
    "\n",
    "There is a numerical measure of correlation, called $r^2$. There are three main key points about the $r^2$ value of variables...\n",
    "\n",
    "* A $r^2$ that approaches 1 implies **positive correlation**\n",
    "* A $r^2$ that approaches -1 implies **negative correlation** \n",
    "* A $r^2$ that approaches 0 implies **no correlation**\n",
    "\n",
    "Here are some pictures to show you examples of correlated, non-correlated, and negative correlated variables with corresponding $r^2$ values.\n",
    "\n",
    "<br>\n",
    "\n",
    "![](https://www.statisticshowto.datasciencecentral.com/wp-content/uploads/2012/10/pearson-2-small.png)\n",
    "\n",
    "\n",
    "We can use a [seaborn heatmap](https://seaborn.pydata.org/generated/seaborn.heatmap.html) to show us the correlation of different variables. Let's first create a function that makes a heatmap for us.\n",
    "\n",
    "The function can always be called as so...\n",
    "\n",
    "```python\n",
    "make_heatmap(my_data, columns)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_heatmap(my_data, columns):\n",
    "    \"\"\"\n",
    "    Make a heatmap of data given certain columns.\n",
    "    \n",
    "    inputs: my_data <pd.DataFrame>: The data we want to draw the heatmap of\n",
    "            columns <list>: The columns to draw the heatmap of\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a figure\n",
    "    plt.figure(figsize=(20, 10))\n",
    "\n",
    "    # Draw a heatmap\n",
    "    sns.heatmap(my_data[numerical_cols].corr(), annot=True, fmt=\".1f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First get numerical columns\n",
    "numerical_cols = [i for i in filtered_data.columns if str(filtered_data[i].dtype) != 'object']\n",
    "\n",
    "# Make heatmatp\n",
    "make_heatmap(filtered_data, numerical_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Think about the following...\n",
    "\n",
    "* What variables correlate with each other? Did you expect these to correlate?\n",
    "* Are there any correlations that _do not exist_ that you imagined would be there?\n",
    "* Any new insights on how this might help decipher male vs. female buyers?\n",
    "\n",
    "Why do looking at correlations matter? \n",
    "\n",
    "> Typically, we want to eliminate correlated variables because keeping both variables do not give _any extra information_ when they are both within a dataset. Usually, it just causes random noise to keep both variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Data pre-processing and feature engineering\n",
    "\n",
    "[Top](#Coder-Academy-and-THE-ICONIC-Masterclass) | [Previous Section](#Part-2:-First-look-at-our-dataset) | [Next Section](#Part-4:-Clustering-the-dataset) | [Bottom](#Wrap-up)\n",
    "\n",
    "We are now going to start using the word **feature** interchangeably **variable**.\n",
    "\n",
    "> A **variable** we are using to refer to the raw-columns within our dataset. A **feature** is an input to our machine learning algorithm. A variable might _become_ a feature, or we might create a feature off of one or more variables.\n",
    "\n",
    "For example, maybe we have data that is supposed to help us predict the price of a house. This dataset might include variables such as the number of bedrooms and bathrooms. What we might realise is that the number of bedrooms and bathrooms individuallly do not affect the price of the house, but the total number of rooms do. \n",
    "\n",
    "So, what we do is we make a new variable called **total_rooms = bedrooms + bathrooms**, and use this variable in our machine learning algorithm, but _not_ the individual bedrooms and bathrooms. Thus, the **total_rooms** is a _feature_.\n",
    "\n",
    "### Normalising Data\n",
    "\n",
    "Machine learning algorithms are often susceptible to bias based upon the different scales within our dataset. For instance, run the following code, which will show you an example of two columns, `revenue` and `items` which are on different scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make figure\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# Draw figure\n",
    "sns.boxplot(data=filtered_data[['revenue', 'items']], orient='h')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the revenue distribution is much more varied than the items distribution (which makes sense, as one item might cost $100)! Often times, the machine learning algorithms we will use prioritise splitting datasets towards variables that have a lot of variance. Thus a typical step performed in data pre-processing is called **normalisation**, which will set our data on the same scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardise_data(my_data, cols_to_norm):\n",
    "    \"\"\"\n",
    "    Normalise the dataset.\n",
    "    \n",
    "    inputs: my_data <pd.DataFrame>, the data to normalise\n",
    "            cols_to_norm <list>, the columns to normalise\n",
    "                             \n",
    "    output: The normalised dataset\n",
    "    \"\"\"\n",
    "    # Create a normaliser\n",
    "    sclr = StandardScaler()\n",
    "    \n",
    "    # Fit the dataset\n",
    "    sclr.fit(my_data[cols_to_norm])\n",
    "    norm_my_data = pd.DataFrame(sclr.transform(my_data[cols_to_norm]), columns=cols_to_norm)\n",
    "    \n",
    "    return norm_my_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_data = standardise_data(filtered_data, numerical_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA\n",
    "\n",
    "To get rid of correlation within our dataset, we often use a concept called [**Principal Component Analysis**](http://setosa.io/ev/principal-component-analysis/), or PCA.\n",
    "\n",
    "Now PCA has a lot of big words within it, but let's make it simple...\n",
    "\n",
    "> **Principle Component Analysis (PCA)** eliminates variables within our dataset by **reducing correlation**. It does this by creating new variables that combine the old variables in such a way where the variation between point-to-point of the old data stays the same, but we reduce the number of dimensions.\n",
    "\n",
    "Ok...that may not be so simple. But let's take a picture from the website linked above.\n",
    "\n",
    "---\n",
    "> ![](../img/pca_img.png)\n",
    "---\n",
    "\n",
    "You might be able to see that the two graphed variables are _highly_ correlated. The new dimension, `pc1`, points in the direction where the variables have the maximum variation _together_.\n",
    "\n",
    "Let's perform PCA. We'll need to [import PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) from the sklearn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pca(my_data, n_components=2):\n",
    "    \"\"\"\n",
    "    Run PCA to reduce the dimensionality and eliminate overly correlated variables.\n",
    "    \n",
    "    input: my_data <pd.DataFrame>, the data we are working with\n",
    "           n_components int, the number of components we want\n",
    "    \n",
    "    output: The DataFrame after PCA\n",
    "    \"\"\"\n",
    "    \n",
    "    # Run PCA\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca.fit(my_data)\n",
    "\n",
    "    # Fit PCA\n",
    "    my_data_result = pd.DataFrame(data=pca.transform(my_data))\n",
    "    \n",
    "    # Explained variance\n",
    "    explained_variance = pca.explained_variance_ratio_\n",
    "    \n",
    "    # Print explained variance\n",
    "    print('Explained variance: ')\n",
    "    print(explained_variance.sum())\n",
    "    \n",
    "    # Create df to hold var information\n",
    "    temp = pd.DataFrame(\n",
    "        data=explained_variance, \n",
    "        columns=['Explained Variance']\n",
    "    )\n",
    "    temp['Explained Variance Cumulative'] = explained_variance.cumsum()\n",
    "    temp['Components'] = range(temp.shape[0])\n",
    "    \n",
    "    # Plot explained variance\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.pointplot(x='Components', y='Explained Variance', data=temp)\n",
    "    plt.title('Explained Variance')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.pointplot(x='Components', y='Explained Variance Cumulative', data=temp)\n",
    "    plt.title('Cumulative Explained Variance')\n",
    "\n",
    "    return my_data_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run PCA. The code will tell the total explained variance in our dataset (1.0 = all the variance explained), and plot the explained variance, and the cumulative explained variance, on two plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_data = create_pca(norm_data[numerical_cols], 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise \n",
    "\n",
    "Play around with the `create_pca` function by adding components until the last component created explains <5% of the variance within our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code here\n",
    "\n",
    "pca_data = create_pca(norm_data[numerical_cols], 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Clustering the dataset\n",
    "\n",
    "[Top](#Coder-Academy-and-THE-ICONIC-Masterclass) | [Previous Section](#Part-3:-Data-pre-processing-and-feature-engineering) | [Next Section](#Part-5:-Classifying-our-inferred-gender) | [Bottom](#Wrap-up)\n",
    "\n",
    "## Introduction to Clustering\n",
    "\n",
    "Clustering allows us to separate the data into groups, or clusters using the features we have created within our data. Here is a gif that shows an example of clustering data into three clusters.\n",
    "\n",
    "![](https://i.stack.imgur.com/kVx8d.gif)\n",
    "\n",
    "As you see there are two variables in the dataset, and each colour on the video represents a different cluster, of which there are three total. In this video, you can see that we are performing two steps (watch the title of the graph to find out the two steps).\n",
    "\n",
    "1. An **Update Cluster Assignment** step: which re-colours each point in space as a new cluster\n",
    "2. An **Update Cluster Centers** step: which moves around the small diamonds to new spaces\n",
    "\n",
    "## Clustering Algorithm\n",
    "\n",
    "We call this method of clustering **K-Means** clustering. This method places **K** diamonds, or centers, randomly within our dataset (in the above example K=3). It then iteratively moves these centers by:\n",
    "\n",
    "1. Labeling the points closest to them as belonging to \"their\" cluster (the **assignment step**)\n",
    "2. Then shifts the centers by putting them at the current center point of the labeled points (the **update step**)\n",
    "\n",
    "Let's run K-Means on our dataset. As you might have guessed, we'll create **two clusters**, one for our inferred males, and one for females. The following function can be used to run K-Means.\n",
    "\n",
    "First we need to import K-Means from the [sklearn library](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html). We will also import the [silhouette score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html), which gives a measure on how well our clustering performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_kmeans(my_data, cont_features):\n",
    "    \"\"\"\n",
    "    Run K-Means on a dataset with a given feature set.\n",
    "    \n",
    "    inputs: my_data <pd.DataFrame>, the dataset to create clusters from\n",
    "            cont_features <list>, the list of features to run K-Means on\n",
    "            \n",
    "    output: the clusters\n",
    "    \"\"\"\n",
    "\n",
    "    # Run kmeans\n",
    "    kmeans = KMeans(n_clusters=2, random_state=42).fit(my_data[cont_features])\n",
    "    pred = kmeans.predict(my_data[cont_features])\n",
    "    \n",
    "    # Score\n",
    "    silhouette = silhouette_score(my_data[cont_features], pred)\n",
    "    print(\"The silhouette score is: \" + str(silhouette))\n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run K-Means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = run_kmeans(pca_data, pca_data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One note on our silhouette score:\n",
    "\n",
    "> The **silhouette score** is a measure of cluster performance. It tells us information about how **compact** and **far apart** our clusters are. \n",
    "\n",
    "If the sihouette score is...\n",
    "\n",
    "* Close to 1.0, it means our clusters are compact and far apart. \n",
    "* Close to -1.0, it means that any data point is closer to an opposing cluster, and should be placed into that opposing cluster rather than the cluster it is currently within."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Classifying our inferred gender\n",
    "\n",
    "[Top](#Coder-Academy-and-THE-ICONIC-Masterclass) | [Previous Section](#Part-4:-Clustering-the-dataset) | [Next Section](#Part-6:-Putting-it-altogether-and-next-steps) | [Bottom](#Wrap-up)\n",
    "\n",
    "Each cluster we have made represents the \"inferred\" gender. Though we do not have the actual gender, if the differentiation of these clusters corresponds to the qualitative knowledge we know about male vs. female consumers, we can have some level of confidence that these are two differentiated groups of people. \n",
    "\n",
    "Even if these groups do not correspond to the actual gender of a person, our feature space can inform us about how we could tailor marketing to each of these groups. What we then need to do is create an algorithm that captures _new_ information and classifies this new information into each of our inferred gender classes.\n",
    "\n",
    "\n",
    "Here's a visual of this process:\n",
    "\n",
    "---\n",
    "\n",
    "![](../img/ICONIC_Classification_Drawing.png)\n",
    "\n",
    "---\n",
    "\n",
    "We'll look at two different algorithms to do this.\n",
    "\n",
    "1. A random forest, which is very _transparent_ about how our features inform the cluster labels\n",
    "2. A neural network, which will likely perform better than the random forest, but not as easily give us insight into what makes these clusters different\n",
    "\n",
    "\n",
    "## Classification Algorithm - Random Forest\n",
    "\n",
    "A random forest is a classification algorithm that can be used to classify new data into our two classes. From [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html):\n",
    "\n",
    "> A **random forest** is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.\n",
    "\n",
    "To break this down a little further, random forest's create trees of data by splitting the data along features. The goal is to use these split points to accurately sort our data into the classes at hand.\n",
    "\n",
    "The following image shows an example of using a tree of data to classify whether someone survived the Titanic disaster (or not) using the gender, age, and cabin class of a passenger.\n",
    "\n",
    "---\n",
    "\n",
    "<img src=\"https://i.pinimg.com/originals/96/2f/a1/962fa1e5b1e3072cbc911cb158915606.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "---\n",
    "\n",
    "Random forests create multiple trees from different samples of a dataset. When one uses the forest to then predict a class, that sample's prediction is calculated from each tree, and the majority prediction is used as the final class label.\n",
    "\n",
    "The following code will [import the RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) from the sklearn library. It will also import the [StratifiedKFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html) module from sklearn, which is a tool that will help us gain better knowledge of how our algorithm will perform on new data, and the [cross_val_score](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html) module, which will score the effectiveness of these models.\n",
    "\n",
    "We also import the [time](https://docs.python.org/3/library/time.html) module, which lets us evaluate model training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function will allow us to train and validate the performance of a random forest classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_random_forest(my_data, features, target_var, k=10, n_estimators=100, max_depth=2):\n",
    "    \"\"\"\n",
    "    Train and test a random forest using K-Fold cross validation.\n",
    "    \n",
    "    inputs: my_data <pd.DataFrame>, the input data\n",
    "            features <list>, the list of features\n",
    "            target_var <str>, the name of the column to predict\n",
    "            k <int>, the number of folds to use for the algorithm\n",
    "    outputs: model <RandomForestClassifier>, the final trained random forest classifier\n",
    "             results <list>, f1 score of the final results\n",
    "             feat_importance <pd.DataFrame>, the importance of each feature in a dataframe\n",
    "             \n",
    "    \"\"\"\n",
    "    # Start time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Create a RandomForestClassifier\n",
    "    clf = RandomForestClassifier(\n",
    "        n_estimators=n_estimators, \n",
    "        max_depth=max_depth,\n",
    "        random_state=42,\n",
    "        class_weight=\"balanced_subsample\",\n",
    "    )\n",
    "    \n",
    "    # Train with StratifiedKFold\n",
    "    kfold = StratifiedKFold(n_splits=k, random_state=42)\n",
    "    results = cross_val_score(clf, my_data[features], my_data[target_var], cv=kfold, scoring='f1')\n",
    "    \n",
    "    # Print result\n",
    "    print(\"F1: Mean %.3f +/- (%.3f)\" % (results.mean(), results.std()))\n",
    "\n",
    "    # Fit with all data\n",
    "    clf.fit(my_data[features], my_data[target_var])\n",
    "    \n",
    "    # Feature importance\n",
    "    feat_importance = pd.DataFrame(\n",
    "        clf.feature_importances_, index=features, columns=['Importance']\n",
    "    ).sort_values(['Importance'], ascending=False)\n",
    "    feat_importance['Index'] = range(feat_importance.shape[0])\n",
    "    \n",
    "    # Graph\n",
    "    feat_importance_cut = feat_importance.loc[feat_importance['Importance'] > 0.01, :]\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    sns.pointplot(x='Index', y='Importance', data=feat_importance_cut, linestyles='')\n",
    "    plt.xlabel(xlabel='')\n",
    "    for i, ind in enumerate(feat_importance_cut.index.values):\n",
    "        x = feat_importance.loc[ind, 'Index']\n",
    "        y = feat_importance.loc[ind, 'Importance']\n",
    "        plt.text(x+0.08, y, ind, fontsize=9)\n",
    "        \n",
    "    # End\n",
    "    end_time = time.time()\n",
    "    print('Elasped time: %.2f seconds' % (end_time - start_time))\n",
    "    \n",
    "    # Return the model and the feature importance\n",
    "    return clf, results, feat_importance[['Importance']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to train the algorithm and take a look at the outputs. We will go back to using the filtered_data we originally created, to give us an idea about how the _original_ columns factored into our cluster creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_data = filtered_data.copy()\n",
    "rf_data['Clusters'] = clusters\n",
    "model, results, feat_importance = create_random_forest(rf_data, numerical_cols, 'Clusters', k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forests work really well in the following cases:\n",
    "\n",
    "* They train **very fast**\n",
    "* Random forests do not require a lot of **preprocessing** to use\n",
    "* They decrease the likelihood to **overfitting**, meaning, they generalise well to new data\n",
    "* They are **transparent**, meaning, we know the impact of how features in our model impact the class distinction\n",
    "\n",
    "The graph printed out shows all features that had an importance over 0.01 for our prediction. The _higher_ the importance, the more relevant the feature is towards destinguishing classes. \n",
    "\n",
    "In addition, we used an \"F1\" score to determine how well our algorithm performed. An \"F1\" score closer to 1.0 means that our algorithm is achieving perfect predictions. An [F1 score](https://en.wikipedia.org/wiki/F1_score) is commonly used when the classes we are predicting are _imbalanced_, meaning we do not necessarily have 50% of our data in one class, and 50% of our data within another class.\n",
    "\n",
    "### Cross-validation\n",
    "\n",
    "There are a few other parameters we have not talked about yet that can affect how good our model is, namely, the **n_estimators**, and the **max_depth**\n",
    "\n",
    "> The **number of estimators**, or **n_estimators** describe the number of trees used for the forest. Generally, _more trees_ reduce overfitting, but lower the model performance on our training data.\n",
    "\n",
    "> The **max_depth**, deepens our tree. The more depth, the better the fit to our current dataset. Usually the larger depth, the more susceptible we are to overfitting. \n",
    "\n",
    "We call the process of tuning parameters, such as n_estimators and max_depth, cross-validation.\n",
    "\n",
    "### Exercise\n",
    "\n",
    "The following code has two variables that can be used to change the `n_estimators` and `max_depth` used within our model. Two example values are given. Play with the parameters and check how they impact the accuracy of the model. Also look how the total run time of the model is influenced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set variables\n",
    "n_estimators = 100\n",
    "max_depth = 50\n",
    "\n",
    "# Run the model\n",
    "model, results, feat_importance = create_random_forest(\n",
    "    rf_data, numerical_cols, 'Clusters', k=10, n_estimators=n_estimators, max_depth=max_depth\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an idea about how classification algorithms work, and we can gauge which features are contributing the most to our prediction algorithm. We will use a more sophisticated algortihm to train our final model.\n",
    "\n",
    "\n",
    "## Classification Algorithm - Deep Neural Networks\n",
    "\n",
    "A **deep neural network** is a part of a family of **deep learning** algorithms. [This article](https://www.technologyreview.com/s/513696/deep-learning/) does a great job in describing what deep learning is:\n",
    "\n",
    "> **Deep-learning software** attempts to mimic the activity in layers of neurons in the neocortex, the wrinkly 80 percent of the brain where thinking occurs. The software learns, in a very real sense, to recognize patterns in digital representations of sounds, images, and other data.\n",
    "\n",
    "If you've heard the buzz around **A.I.** recently, it's likely because of deep learning. Software systems like Google Translate, and self-driving cars use neural networks for decision making.\n",
    "\n",
    "Here is an example of using an artificial neural network (ANN) to recognise the \"7\" within an image.\n",
    "\n",
    "---\n",
    "\n",
    "![](https://thumbs.gfycat.com/BaggyFearlessCrocodile-max-1mb.gif)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Building an algorithm\n",
    "\n",
    "This network is composed of three main parts.\n",
    "\n",
    "* The first layer is called the **input** layer. The **input** layer is the size of the number of features we have.\n",
    "* The middle layers are called **hidden** layers. The more hidden layers we have the _deeper_ are network. Deeper networks take longer to train, but often can find more interesting patterns within our data.\n",
    "* The last layer is called an **output** layer. The activation in the output layer tells us the classification.\n",
    "\n",
    "We are going to use the [tf.keras](https://www.tensorflow.org/guide/keras) library to create our neural network. Tensorflow is an open-source library developed by Google Brain that allows for the training of deep neural networks.\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/1600/0*a6XSwHsfvz_oWSSJ.jpg)\n",
    "\n",
    "Let's import the necessary libraries from Tensorflow to be able to create our model. We'll also import the actual [f1_score function](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html) from sklearn, and the [numpy library](https://www.numpy.org/), which is another common python mathematical library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following code to create a function that sets up the neural network. It will use k-fold cross validation like our random forest did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ann(my_data, features, target_var, architecture=None, num_iterations=10, k=5, thresh=0.5, verbose=1):\n",
    "    \"\"\"\n",
    "    Create a Neural Network and train it.\n",
    "    \n",
    "    inputs: my_data <pd.DataFrame>, the dataset\n",
    "            features <list>, the list of variables to use for the data\n",
    "            target_var <str>, column with the target variable to classify\n",
    "            architecture <list>, hidden layer architecutre\n",
    "            num_iterations <int>, the number of iterations for training\n",
    "            k <int>, the number of folds to train upon\n",
    "            threshold <float>, anything above the threshold will be classified as class \"1\" in the network\n",
    "            verbose <int>, 1 if we want to see output, 0 otherwise\n",
    "            \n",
    "    outputs: The final model, f1_scores from training, and inferred gender\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Time\n",
    "    start_time = time.time()\n",
    "        \n",
    "    # Create network architecture\n",
    "    temp = [keras.layers.Dense(len(features), activation=tf.nn.sigmoid)]\n",
    "    if architecture is not None:\n",
    "        # Add hidden layers\n",
    "        for i in architecture:\n",
    "            temp.append(keras.layers.Dense(i, activation=tf.nn.relu))\n",
    "    # Add last layer\n",
    "    temp.append(keras.layers.Dense(1, activation=tf.nn.sigmoid))\n",
    "\n",
    "    \n",
    "    # Run K-Fold cross validation and fit\n",
    "    kfold = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    f1s = []\n",
    "    \n",
    "    for train, test in kfold.split(my_data[features], my_data[target_var]):\n",
    "        # Build model\n",
    "        ann = keras.Sequential(temp)\n",
    "        ann.compile(\n",
    "            optimizer='adam', \n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "                \n",
    "        # Fit data\n",
    "        ann.fit(\n",
    "            my_data.loc[train, features].values, my_data.loc[train, target_var].values, \n",
    "            epochs=num_iterations, \n",
    "            batch_size=256,\n",
    "            class_weight={\n",
    "                0: (my_data[target_var] == 1).sum() / (my_data[target_var] == 0).sum(),\n",
    "                1: 1\n",
    "            },\n",
    "            verbose=verbose\n",
    "        )\n",
    "        \n",
    "        # Predict data\n",
    "        y_pred = pd.DataFrame(ann.predict(my_data.loc[test, features]).flatten(), columns=['Prediction'])\n",
    "        y_pred['Class'] = 0\n",
    "        y_pred.loc[y_pred['Prediction'] > thresh, 'Class'] = 1\n",
    "    \n",
    "        # F1 score\n",
    "        f1s.append(f1_score(my_data.loc[test, target_var].values, y_pred['Class']))\n",
    "        print(\"\\n\")\n",
    "        \n",
    "    # Print scores\n",
    "    print(\"F1: Mean %.3f +/- (%.3f)\" % (np.mean(f1s), np.std(f1s)))\n",
    "    \n",
    "    # Train one final model and print outputs\n",
    "    ann = keras.Sequential(temp)\n",
    "    ann.compile(\n",
    "        optimizer='adam', \n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    # Fit data\n",
    "    ann.fit(\n",
    "        my_data[features].values, my_data[target_var].values, \n",
    "        epochs=num_iterations, \n",
    "        batch_size=256,\n",
    "        class_weight={\n",
    "            0: (my_data[target_var] == 1).sum() / (my_data[target_var] == 0).sum(),\n",
    "            1: 1\n",
    "        },\n",
    "        verbose=0\n",
    "    )\n",
    "        \n",
    "    # Print end result\n",
    "    # End time\n",
    "    end_time = time.time()\n",
    "    print('Elasped time: %.2f seconds' % (end_time - start_time))\n",
    "    \n",
    "    \n",
    "    return ann, f1s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following code to build an algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "ann_data = filtered_data.copy().reset_index(drop=True)\n",
    "ann_data['Clusters'] = clusters\n",
    "\n",
    "# Train\n",
    "model, f1_scores = create_ann(\n",
    "    ann_data, numerical_cols, 'Clusters', architecture=None, num_iterations=10, k=2, thresh=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning the algorithm\n",
    "\n",
    "A few notes on what we did above.\n",
    "\n",
    "* We performed 2 fold cross-validation, meaning that the algorithm fit twice with two different training/validation sets\n",
    "* We also only trained the network for 10 iterations. The more iterations, the higher accuracy we will get, but the longer it will take to train our network\n",
    "* We also only used an **input** and **output** layer, so this was not a very deep network. We can add more layers by adjusting the architecture input. Note that the _first_ and _last_ layers will always be the number of fetures and the number of classes - 1\n",
    "* We can also adjust the _threshold_ at which we classify something in the network as a cluster. The higher the threshold, the less likely something will be classified as a positive value\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Adjust the values of the following parameters\n",
    "\n",
    "* `architecture` (each additional entry will create a new hidden layer)\n",
    "* `num_iterations`\n",
    "* `threshold` (between 0 and 1)\n",
    "* `k` (the number of folds to check, which should at least be 2)\n",
    "\n",
    "And see how it affects the training time and accuracy of the model. Try to pick a set of parameters that leads to good model accuracy. A set of example parameters are listed for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Parameters to tune\n",
    "num_iterations = 25\n",
    "architecture = [20, 10]\n",
    "threshold = 0.6\n",
    "k = 5\n",
    "\n",
    "# Train\n",
    "model, f1_scores = create_ann(\n",
    "    ann_data, numerical_cols, 'Clusters', \n",
    "    architecture=architecture, \n",
    "    num_iterations=num_iterations, \n",
    "    k=k, \n",
    "    thresh=threshold,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 6: Putting it altogether and next steps\n",
    "\n",
    "[Top](#Coder-Academy-and-THE-ICONIC-Masterclass) | [Previous Section](#Part-5:-Classifying-our-inferred-gender) | [Next Section](#Wrap-up) | [Bottom](#Wrap-up)\n",
    "\n",
    "Let's put it all together. The following function will combine _all_ the steps we have done above into a single pipeline. Here's a visual to show all of the steps in numbered order.\n",
    "\n",
    "---\n",
    "\n",
    "![](../img/ICONIC_Full_Pipeline_Drawing.png)\n",
    "\n",
    "---\n",
    "\n",
    "Note there are some new pieces of the puzzle we have not dealt with.\n",
    "\n",
    "* When we ran our random forest, we saw what features had an impact towards are cluster labels. Usually data scientists have a good **qualitative understanding** of what should influence their outcome. Did the features that were important within the random forest match your understanding? If not, there are a few things we could do...\n",
    "  * We could remove these features, and limit which columns we use for clustering\n",
    "  * We could create new features, which might help eliminate some of the correlations. For instance, the number of items we buy is likely correlated with the number of male/female items we buy (buying more items = buying more of any item on average). We could wrap all these features together by having a single feature such as the % of female items (which will imply the % male items)\n",
    "  \n",
    "* Neural Networks might be impacted by variables that have a naturally large variance. Variance is often impacted by scale. For instance, the variance of our `revenue` variable should be naturally larger than the variance of our `items` variable. Thus it might make sense to also _normalise_ the data prior to training on the neural network\n",
    "\n",
    "* Neural Networks might also be subject to random noise due to correlation. Thus it might make sense to also run _PCA_ on our data prior to training it within the network.\n",
    "\n",
    "* To actually process our new data, we would have to somehow host our trained algorithm. Tools like [Amazon Sagemaker](https://aws.amazon.com/sagemaker/) allow for easy hosting of machine learning algorithms. We won't go into this tonight, but it might be good to check out one of these tools.\n",
    "\n",
    "As you see, **75%** of these decisions involve data cleaning, and most of a data scientist's job involves data cleaning! We need to make sure our algorithms are _understandable_, essentially since there is uncertainty in the accuracy of our predictions.\n",
    "\n",
    "## Setting up our pipeline\n",
    "\n",
    "Run the following code which creates the entire pipline. It...\n",
    "\n",
    "1. Transforms the data using normalisation and PCA\n",
    "2. Clusters the data based upon these transformed features\n",
    "3. Runs a random forest to gauge how features contributes to our classification\n",
    "4. Trains a ANN\n",
    "\n",
    "The code also allows us to choose whether we want to run PCA and normalisation prior to training a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gender_inference_pipeline(\n",
    "    my_data,\n",
    "    features,\n",
    "    num_pca_components=3,\n",
    "    k_folds=5,\n",
    "    rf_estimators=100,\n",
    "    rf_depth=10,\n",
    "    norm_ann=False,\n",
    "    pca_ann=False,\n",
    "    pca_ann_components=None,\n",
    "    architecture=None,\n",
    "    num_iterations=10,\n",
    "    threshold=0.5\n",
    "):\n",
    "    \"\"\"\n",
    "    Run entire pipeline based upon a given number of features\n",
    "    \n",
    "    inputs: my_data <pd.DataFrame>, the dataset\n",
    "            features <list>, the list of variables to use for the data\n",
    "            num_pca_components <int>, the number of components to use for a KNN\n",
    "            k_folds <int>, the number of folds to train upon\n",
    "            rf_estimators <int>, number of trees in the rf\n",
    "            rf_depth <int>, the depth of each tree\n",
    "            norm_ann <bool>, True to normalise variables before ANN training\n",
    "            pca_ann <bool>, True to run PCA prior to ANN training\n",
    "            pca_ann_components <int>, Number of components to use for PCA in the ANN\n",
    "            architecture <list>, hidden layer architecutre\n",
    "            num_iterations <int>, the number of iterations for training\n",
    "            threshold <float>, anything above the threshold will be classified as class \"1\" in the network\n",
    "            \n",
    "    outputs: The final model, f1_scores from training, and inferred gender\n",
    "    \"\"\"\n",
    "    \n",
    "    # Reset index\n",
    "    my_data = my_data.reset_index(drop=True)\n",
    "    \n",
    "    # Normalise data\n",
    "    norm_data = standardise_data(my_data, features)\n",
    "    # PCA data\n",
    "    pca_data = create_pca(norm_data[features], num_pca_components)\n",
    "    \n",
    "    # Cluster data\n",
    "    clusters = run_kmeans(pca_data, pca_data.columns)\n",
    "    \n",
    "    # Train random forest\n",
    "    rf_data = my_data.copy()\n",
    "    rf_data['Clusters'] = clusters\n",
    "    create_random_forest(rf_data, features, 'Clusters', k=k_folds, n_estimators=rf_estimators, max_depth=rf_depth)\n",
    "    \n",
    "    # Norm ann\n",
    "    if norm_ann:\n",
    "        ann_data = norm_data.copy()\n",
    "    else:\n",
    "        ann_data = my_data.copy()\n",
    "        \n",
    "    \n",
    "    # If PCA ann\n",
    "    if pca_ann:\n",
    "        # Get num components\n",
    "        if pca_ann_components is None:\n",
    "            pca_ann_components = num_pca_components\n",
    "        # Run PCA\n",
    "        ann_data = create_pca(ann_data[features], pca_ann_components)\n",
    "        features = ann_data.columns\n",
    "        \n",
    "    # Now run ANN\n",
    "    ann_data['Clusters'] = clusters\n",
    "    \n",
    "    # Train\n",
    "    model, f1_scores = create_ann(\n",
    "        ann_data, features, 'Clusters', \n",
    "        architecture=architecture, \n",
    "        num_iterations=num_iterations, \n",
    "        k=k_folds, \n",
    "        thresh=threshold,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    return model, f1_scores, clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following to train the entire pipeline with raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run the model\n",
    "end_model, f1_scores, inferred_gender = gender_inference_pipeline(\n",
    "    my_data=filtered_data,\n",
    "    features=numerical_cols,\n",
    "    num_pca_components=3,\n",
    "    k_folds=5,\n",
    "    rf_estimators=100,\n",
    "    rf_depth=10,\n",
    "    norm_ann=False,\n",
    "    pca_ann=False,\n",
    "    pca_ann_components=None,\n",
    "    architecture=None,\n",
    "    num_iterations=10,\n",
    "    threshold=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "There are a **ton** of parameters we could change within our model. I've also recopied the list of columns within our data below.\n",
    "\n",
    "| Column                   | Value   | Description                                                              | \n",
    "|--------------------------|---------|--------------------------------------------------------------------------| \n",
    "| customer_id              | string  | ID of the customer - super duper hashed                                  | \n",
    "| days_since_first_order   | integer | Days since the first order was made                                      | \n",
    "| days_since_last_order    | integer | Days since the last order was made                                       | \n",
    "| int_is_newsletter_subscriber | string  | Flag for a newsletter subscriber (1 = Yes, 0 = No)                                        | \n",
    "| orders                   | integer | Number of orders                                                         | \n",
    "| items                    | integer | Number of items                                                          | \n",
    "| cancels                  | integer | Number of cancellations - when the order is cancelled after being placed | \n",
    "| returns                  | integer | Number of returned orders                                                | \n",
    "| different_addresses      | integer | Number of times a different billing and shipping address was used        | \n",
    "| shipping_addresses       | integer | Number of different shipping addresses used                              | \n",
    "| devices                  | integer | Number of unique devices used                                            | \n",
    "| vouchers                 | integer | Number of times a voucher was applied                                    | \n",
    "| cc_payments              | integer | Binary indicating if credit card was used for payment                       | \n",
    "| paypal_payments          | integer | Binary indicating if PayPal was used for payment                              | \n",
    "| afterpay_payments        | integer | Binary indicating if AfterPay was used for payment                            | \n",
    "| apple_payments           | integer | Binary indicating if Apple Pay was used for payment                           | \n",
    "| female_items             | integer | Number of items purchased for women                                         | \n",
    "| male_items               | integer | Number of items purchased for men                                           | \n",
    "| unisex_items             | integer | Number of unisex items purchased                                         | \n",
    "| wapp_items               | integer | Number of Women Apparel items purchased                                  | \n",
    "| wftw_items               | integer | Number of Women Footwear items purchased                                 | \n",
    "| mapp_items               | integer | Number of Men Apparel items purchased                                    | \n",
    "| wacc_items               | integer | Number of Women Accessories items purchased                              | \n",
    "| macc_items               | integer | Number of Men Accessories items purchased                                | \n",
    "| mftw_items               | integer | Number of Men Footwear items purchased                                   | \n",
    "| wspt_items               | integer | Number of Women Sport items purchased                                    | \n",
    "| mspt_items               | integer | Number of Men Sport items purchased                                      | \n",
    "| curvy_items              | integer | Number of Curvy items purchased                                          | \n",
    "| sacc_items               | integer | Number of Sport Accessories items purchased                              | \n",
    "| msite_orders             | integer | Number of Mobile Site orders                                             | \n",
    "| desktop_orders           | integer | Number of Desktop orders                                                 | \n",
    "| android_orders           | integer | Number of Android app orders                                             | \n",
    "| ios_orders               | integer | Number of iOS app orders                                                 | \n",
    "| other_device_orders      | integer | Number of Other device orders                                            | \n",
    "| work_orders              | integer | Number of orders shipped to work                                         | \n",
    "| home_orders              | integer | Number of orders shipped to home                                         | \n",
    "| parcelpoint_orders       | integer | Number of orders shipped to a parcelpoint                                | \n",
    "| other_collection_orders  | integer | Number of orders shipped to other collection points                      | \n",
    "| average_discount_onoffer | float   | Average discount rate of items typically purchased                       | \n",
    "| average_discount_used    | float   | Average discount finally used on top of existing discount                | \n",
    "| revenue                  | float   | $ Dollar spent overall per person                                        |\n",
    "\n",
    "#### Your goal is to change the...\n",
    "\n",
    "* Features used in the model by editing the `features` list\n",
    "* Other parameters incuding...\n",
    "  * Num components\n",
    "  * Whether to normalise/PCA ANN\n",
    "  * The threshold of the ANN\n",
    "  * The architecture\n",
    "  \n",
    "First see...\n",
    "\n",
    "1. How do variables separate the inferred gender?\n",
    "2. How do modifying the other parameters change your model accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "feature_list = ['items', 'male_items', 'devices']\n",
    "\n",
    "# Affects clustering\n",
    "num_pca_components = 2 # MUST BE LESS THAN THE LENGTH OF YOUR FEATURES\n",
    "k_folds = 2\n",
    "\n",
    "# Affects random forest\n",
    "rf_estimators = 50\n",
    "rf_depth = 10\n",
    "\n",
    "# Affects ANN\n",
    "norm_ann = True\n",
    "pca_ann = False\n",
    "pca_ann_components = 3\n",
    "architecture = [20, 10]\n",
    "num_iterations = 50\n",
    "threshold = 0.6\n",
    "\n",
    "\n",
    "# Run the model\n",
    "end_model, f1_scores, inferred_gender = gender_inference_pipeline(\n",
    "    my_data=filtered_data,\n",
    "    features=feature_list,\n",
    "    num_pca_components=num_pca_components,\n",
    "    k_folds=k_folds,\n",
    "    rf_estimators=rf_estimators,\n",
    "    rf_depth=rf_depth,\n",
    "    norm_ann=norm_ann,\n",
    "    pca_ann=pca_ann,\n",
    "    pca_ann_components=pca_ann_components,\n",
    "    architecture=architecture,\n",
    "    num_iterations=num_iterations,\n",
    "    threshold=threshold\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use the [sns.pairplot](https://seaborn.pydata.org/generated/seaborn.pairplot.html) visualisation to see how the gender inference impacts your end model. **DO NOT** put too many features within the pairplot. Your notebook WILL CRASH!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write list of features to pair plot\n",
    "pairplot_cols = ['male_items', 'female_items']\n",
    "\n",
    "# Create data to plot\n",
    "pairplot_data = filtered_data[pairplot_cols].copy()\n",
    "pairplot_data['Inferred Gender'] = ['Gender_' + str(i) for i in inferred_gender]\n",
    "\n",
    "# Plot\n",
    "sns.pairplot(pairplot_data, hue='Inferred Gender')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrap-up\n",
    "\n",
    "Thank you for attending our masterclass! We hope we _demystified_ a little bit of what actually occurs when you create a machine learning algorithm. Big thank you to Kshira Saagar from THE ICONIC for his time and lending us data for the workshop.\n",
    "\n",
    "We will be sending out a **survey over email** to get your feedback on the session!\n",
    "\n",
    "If you would like to download your work for today, please click **File->Download As->.html**. You will not be able to run the cells, but you will be able to view the material you learned within a web browser."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
