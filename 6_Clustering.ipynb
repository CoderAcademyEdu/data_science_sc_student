{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Week 6 - Unsupervised Learning and More Vision\n",
    "\n",
    "[Top](#ML-Week-6---Unsupervised-Learning-and-More-Vision) | [Previous week](#ML-Week-5---Artificial-Neural-Networks) | [Start of Lesson](#Part-1:-Intro-To-Unsupervised-Learning) | [Bottom](#Cheers)\n",
    "\n",
    "Today's lesson will be on both unsupervised learning techniques and a continuation on from exploring the power of deep computer vision. This lesson will be broken down into:\n",
    "\n",
    "* [Part 1: Intro To Unsupervised Learning](#Intro-To-Unsupervised-Learning)\n",
    "* [Part 2: K-Means Clustering](#K-Means-Clustering)\n",
    "* [Part 3: Non Linear K-Means](#Part-3:-Non-Linear-K-Means)\n",
    "* [Part 4: Autoencoders](#Part-4:-Autoencoders)\n",
    "* [Part 5: Convolutional Neural Networks (CNNs) (Optional)](#Part-5:-Convolutional-Neural-Networks-(CNNs)-(Optional))\n",
    "* [Part 6: Using a Pretrained CNN](#Part-6:-Using-a-Pretrained-CNN)\n",
    "* [Part 7: Applications of Deep Computer Vision](#Part-7:-Applications-of-Deep-Computer-Vision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import scale\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import scale\n",
    "import pylab as pl\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import Binarizer\n",
    "# %matplotlib notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Intro To Unsupervised Learning\n",
    "\n",
    "Unsupervised learning is a branch of machine learning that only requires raw data input to make inferences. That is to say we don't need any data labels or external information. Within this field there are two main groups of tasks:\n",
    "\n",
    "* Clustering: Where we try and group data points based on their inherent features\n",
    "* Dimensionality Reduction: Where we try and reduce the number of features in our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: K-Means Clustering\n",
    "\n",
    "The K-means algorithm is a fundamental clustering algorithm that can search for a predetermined number of clusters in an unlabeled multidimensional dataset. The definition of a valid k-means cluster is simple, it consists of both a \n",
    "\n",
    "* cluster center which is simply the average coordinate of all points in the cluster\n",
    "* and clusters of points which are all closest to their respective cluster center than any other cluster center\n",
    "\n",
    "\n",
    "We start with a very simple example of 4 blobs of data. \n",
    "\n",
    "### Exercise \n",
    "\n",
    "Just using your own visual intuition, try and classify the dataset into 4, 3 and 2 separate clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use SKLearn data generator\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "X, y_true = make_blobs(n_samples=300, centers=4,\n",
    "                       cluster_std=0.60, random_state=0)\n",
    "plt.scatter(X[:, 0], X[:, 1], s=50);\n",
    "plt.title(\"Groups of Customers\")\n",
    "plt.ylabel(\"Age of customer\")\n",
    "plt.xlabel(\"Likelyhood of buying a new smartphone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we just did manually can be done algorithmically using sklearns k-means implementation. We simply create a kmeans object with our desired clusters and fit the data much like any other machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=4)\n",
    "kmeans.fit(X)\n",
    "y_kmeans = kmeans.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now visualise the the cluster centers which are in black and also the data which has now been ascribed classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='RdYlGn')\n",
    "\n",
    "centers = kmeans.cluster_centers_\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.8);\n",
    "plt.title(\"Groups of Customers\")\n",
    "plt.ylabel(\"Age of customer\")\n",
    "plt.xlabel(\"Likelyhood of buying a new smartphone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Run K-means with cluster sizes of 2 to 4 and see if it matches your visual predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Non Linear K-Means\n",
    "\n",
    "Unfortunately like many machine learning algorithms the basic approaches fail on anything that's a little more complex. We can see below that the algorithm doesn't cluster the data as we would expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "X, y = make_moons(200, noise=.05, random_state=0)\n",
    "\n",
    "labels = KMeans(2, random_state=0).fit_predict(X)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels,\n",
    "            s=50, cmap='PiYG');\n",
    "plt.title(\"Lemons vs Cherries\")\n",
    "plt.ylabel(\"Abundance of fruit\")\n",
    "plt.xlabel(\"Amount of UVA sunlight\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily we can combat these problems by transforming them into linear problems in a higher dimensional space. This is exactly what spectral clustering does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralClustering\n",
    "model = SpectralClustering(n_clusters=2, affinity='nearest_neighbors',\n",
    "                           assign_labels='kmeans')\n",
    "labels = model.fit_predict(X)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels,\n",
    "            s=50, cmap='PiYG');\n",
    "plt.title(\"Lemons vs Cherries\")\n",
    "plt.ylabel(\"Abundance of fruit\")\n",
    "plt.xlabel(\"Amount of UVA sunlight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits import mplot3d\n",
    "from ipywidgets import interact, fixed\n",
    "\n",
    "#Define a z scale based on the label value manually\n",
    "r=np.where(labels==0,0,10)\n",
    "def plot_3D(elev=30, azim=30, X=X, y=labels):\n",
    "    ax = plt.subplot(projection='3d')\n",
    "    ax.scatter3D(X[:, 0], X[:, 1], r, c=y, s=50, cmap='PiYG')\n",
    "    ax.view_init(elev=elev, azim=azim)\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_zlabel('r')\n",
    "\n",
    "interact(plot_3D, elev=[-90,0,10,40], azip=(-180, 180),\n",
    "         X=fixed(X), y=fixed(y));\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Autoencoders\n",
    "\n",
    "Autoencoders are a special type of artificial neural network that can learn data encodings in an unsupervised way. We call this data encoding a representation and the process of finding it \"representation learning\". We intentionally design an artifical neural network so that there is a bottleneck that forceably learns the most compressed representation of an input image. If there is some intrinsic structure in the inputs we should be able to learn a representation of it with much less data than the total input data. \n",
    "\n",
    "A basic overview of how this works can be seen in the image below. We put the image into the network, learn its compressed representation and then try and decode that representation back to get a reconstructed image.\n",
    "\n",
    "<img src=\"img/autoencoder.png\">\n",
    "\n",
    "\n",
    "### Denoising Autoencoders\n",
    "\n",
    "One particularly useful type of autoencoder is the denoising autoencoder. This variant allows us to try and learn a robust representation of our inputs by introducing random noise into the images. The encoder then hopefully learns how to ignore this noise and reconstruct clean images. This process is callled denoising. Below is a simple example of how to go about constructing a denoiser in Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import basic mnist dataset\n",
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "n = 11  # how many digits we will display\n",
    "plt.figure(figsize=(20, 4))\n",
    "\n",
    "#Plot our original clean training images for analysis\n",
    "(x_train, _), (x_test, _) = mnist.load_data()\n",
    "for i in range(1,n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.title(\"MNIST Digits Dataset\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalise all the inputs so they are between 0-1 as per last week\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "# Reshape the train images for our network if they aren't already\n",
    "x_train = np.reshape(x_train, (len(x_train), 28, 28, 1))  # adapt this if using `channels_first` image data format\n",
    "x_test = np.reshape(x_test, (len(x_test), 28, 28, 1))  # adapt this if using `channels_first` image data format\n",
    "\n",
    "#Adjust how much noise we add to each training image\n",
    "noise_factor = 0.5\n",
    "\n",
    "#Multiply the noise to each pixel\n",
    "x_train_noisy = x_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_train.shape) \n",
    "x_test_noisy = x_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_test.shape) \n",
    "\n",
    "x_train_noisy = np.clip(x_train_noisy, 0., 1.)\n",
    "x_test_noisy = np.clip(x_test_noisy, 0., 1.)\n",
    "\n",
    "#Plot some sample images that have had noise added to them\n",
    "n = 10\n",
    "plt.figure(figsize=(20, 2))\n",
    "for i in range(1,n):\n",
    "    ax = plt.subplot(1, n, i)\n",
    "    plt.imshow(x_test_noisy[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "#Input image with dimensions 28*28\n",
    "input_img = Input(shape=(28, 28, 1))\n",
    "\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(input_img)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "encoded = MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "# at this point the representation is (7, 7, 32) and at it's most compressed stage\n",
    "\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(encoded)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "#Create the model representation\n",
    "autoencoder = Model(input_img, decoded)\n",
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the autoencoder with our noisy data versus our clean data for 1 epoch\n",
    "autoencoder.fit(x_train_noisy, x_train,\n",
    "                epochs=1,\n",
    "                batch_size=128,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test_noisy, x_test),\n",
    "                callbacks=[TensorBoard(log_dir='/tmp/tb', histogram_freq=0, write_graph=False)])\n",
    "# Set tensorboard dir to /tmp/tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict on our entire test set\n",
    "decoded_imgs = autoencoder.predict(x_test)\n",
    "\n",
    "n = 11\n",
    "#Show 11 images for sample output to see how the network did\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(1,n):\n",
    "    # display input noisy data\n",
    "    ax = plt.subplot(2, n, i)\n",
    "    plt.imshow(x_test_noisy[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstructed (hopefully) cleaned up data\n",
    "    ax2 = plt.subplot(2, n, i + n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax2.get_xaxis().set_visible(False)\n",
    "    ax2.get_yaxis().set_visible(False)\n",
    "    if i==1:\n",
    "        ax.set_title(\"Noisy Images\")\n",
    "        ax2.set_title(\"Decoded Images\")\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "What happens if we crank the noise up even more? Try and modify the code such that the data is distorted even further and report your findings\n",
    "\n",
    "## Thinking Time\n",
    "\n",
    "Can you think of any other uses for autoencoders?\n",
    "Why do the images look blurry when we decode them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard (Optional) (Advanced)\n",
    "\n",
    "Tensorboard is a tool for TensorFlow that allows us to visualise the intricacies of our neural networks. It's beyond the scope of this short course but if you're doing any other deep learning stuff it's always good to be in the know about tools that are used by academics and professionals. To visualise the training of the previous autoencoder we can use the command\n",
    "\n",
    "`tensorboard --logdir=/tmp/tb`\n",
    "\n",
    "You should get a graph that looks similar to this \n",
    "\n",
    "<img src=\"img/tensorboard.png\" width=\"500px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Convolutional Neural Networks (CNNs) (Optional)\n",
    "\n",
    "CNNs are a fundamental part of deep computer vision. They are a primary contributor to the computer vision revolution that's unfolding. Whilst CNNs are very complex to underestand we will try to get a very very basic intuition between how they work and then later on the lesson we can see the phenomenal things they are able to do. \n",
    "\n",
    "\n",
    "### What does a CNN look like?\n",
    "\n",
    "<img src=\"img/cnn.png\" width=\"1000px\">\n",
    "\n",
    "A typical CNN consists of just a series of convolutional layers and pooling layers stacked on top of each other in a similar way to what we saw last week with a normal neural network. If the network has more than one convolutional layer then we sometimes refer to it as a deep convolutional network (DCNN) but usually a CNN is implied to be deep anyway so CNN is used universally. \n",
    "\n",
    "### Convolutional Intuition\n",
    "\n",
    "The intuition behind using convolutions is rooted in how animal visual cortexes operate. The basic premise is that data which is spatially closer to a single point is more relevant to that point then data which is spatially further away. The image below shows that with a sunflower, the two sliding \"windows\" over the sunflower head are more relevant to each other then the other random window which sees the sky.\n",
    "\n",
    "<img src=\"img/cnn_intuition.png\" width=\"700px\">\n",
    "\n",
    "### Convolutional Layers (Advanced)\n",
    "\n",
    "This is a rather complicated concept to understand so don't worry at all if you have no clue what's going on. In essence a convolutional layer is just a collection of filters which can be learned by the network to decipher images. \n",
    "\n",
    "<img src=\"img/cnn_example.png\" width=\"700px\">\n",
    "\n",
    "Whenever an image is passed to a layer the layer computes its \"convolution\" by sliding over the whole image with each image filter as seen above.\n",
    "\n",
    "These image filters are how the network learns to see. At the start of the network we hope to learn primitive features like edges and blobs and then by the end we try to learn highly complex features like textures, patterns and whole object representations.\n",
    "\n",
    "<img src=\"img/filters_better.png\" width=\"1000px\">\n",
    "\n",
    "### How Do We Train CNNs?\n",
    "\n",
    "To train a CNN we need a lot data. One of the most famous datasets is the ImageNet dataset which contains 14 million classification labels and 1 million bounding box annotations. There are 20 000 classes in total but we usually work with a cut down set of the most popular 1000 classes of object. You can view and explore all the classes at the [Imagenet Website](http://image-net.org/explore). As described last week we simply feed in an imput image to the network and try and see if the network can learn to predict its class label. The process is identical to the non convolutional example we did last week the only differences lie in the internals of the network.\n",
    "\n",
    "### Pretrained Models\n",
    "\n",
    "Often the resources needed to train a neural network from scratch are excessive. Luckily for us neural network trained models are shareable just like any other computer data. Researchers will often post what we call pretrained models on file hosting services so other researchers and users can immediately start using the model. These pretrained models are often trained on large public datasets like [imagenet](http://www.image-net.org/) and [COCO](http://cocodataset.org/).\n",
    "\n",
    "### Retraining Models and Transfer Learning (Advanced)\n",
    "\n",
    "One little quirk about neural network models is that once they've been shared, it's possible to unfreeze them and resume training. It's also possible to add onto them and undergo what we call \"network retraining\". One major advantage of doing this is we are able to do transfer learning. Transfer learning allows us to take knowledge learned in one domain and apply it to another. \n",
    "\n",
    "The image below shows illustratively how this works. \n",
    "\n",
    "<img src=\"img/transferlearning.png\" width=\"600px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Using a Pretrained CNN\n",
    "\n",
    "The next exercise will be using a pretrained model trained on imagenet to classify some images. This model is the saved weights of a convolutional neural network that has been uploaded by another ML enthusiast for us to use. The default implementation below uses mobilenetV2 and you can read a gentle introduction to its advantages on [Google's blog](https://ai.googleblog.com/2018/04/mobilenetv2-next-generation-of-on.html).\n",
    "\n",
    "### Exercise\n",
    "\n",
    "See if you can switch out the pretrained model to another one by using my commented code. If you understand the code see if you can then switch it to an even deeper network using a [keras applications network](https://keras.io/applications/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import and use the mobilenetv2 backbone\n",
    "!pip install requests\n",
    "from keras.applications.mobilenet_v2 import MobileNetV2\n",
    "from keras.applications.mobilenet_v2 import decode_predictions\n",
    "from keras.applications.mobilenet_v2 import preprocess_input\n",
    "from keras.preprocessing.image import img_to_array\n",
    "\n",
    "#initialise the mobilenetv2 model\n",
    "model = MobileNetV2()\n",
    "\n",
    "#This is a deeper and larger network, resnet50\n",
    "#Uncomment the code block below to explore how a bigger brain tackles this problem\n",
    "\n",
    "\n",
    "# from keras.applications.resnet50 import ResNet50\n",
    "# from keras.applications.resnet50 import decode_predictions\n",
    "# from keras.applications.resnet50 import preprocess_input\n",
    "# from keras.preprocessing.image import img_to_array\n",
    "\n",
    "# model = ResNet50()\n",
    "\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "#Substitute with your own image URL, make sure its a jpeg file and roughly square\n",
    "#Images that are already size 250x250 or there abouts work well \n",
    "\n",
    "#Change the url variable\n",
    "url = 'http://www.grmicek.si/images/Banana.jpg'\n",
    "\n",
    "#Test with a cat later by renaming variable\n",
    "url_cat = 'https://upload.wikimedia.org/wikipedia/commons/thumb/1/14/Gatto_europeo4.jpg/250px-Gatto_europeo4.jpg'\n",
    "\n",
    "\n",
    "#Serialise the internet url into a PIL image\n",
    "response = requests.get(url)\n",
    "image = Image.open(BytesIO(response.content))\n",
    "#Crop the image to our network dimensions\n",
    "image = image.crop((0, 0, 224, 224))\n",
    "image = img_to_array(image)\n",
    "import numpy as np\n",
    "arr_image = np.array(image)\n",
    "image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "image = preprocess_input(image)\n",
    "\n",
    "#Run the prediction on our image\n",
    "yhat = model.predict(image)\n",
    "\n",
    "#Get our output prediction and confidence\n",
    "label = decode_predictions(yhat)\n",
    "label = label[0][0]\n",
    "\n",
    "#Output that confidence\n",
    "print('The network has predicted the image is a {} with {}% confidence'.format(label[1], label[2] * 100))\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dissecting a Convolutional Neural Network\n",
    "Now that we have made our prediction on the network we want to try and see inside to know what the networks up to. A common misconception about neural networks is that they are a complete \"black box\". Whilst it can be difficult to actually understand why networks make the predictions they do, when using image data we can visualise each activation at each layer as an image itself. \n",
    "\n",
    "Below we do that using the [keract module](https://github.com/philipperemy/keract) which automatically extracts the feature activations from each layer and plots the results using a nice colour map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install keract\n",
    "\n",
    "import keract\n",
    "activations = keract.get_activations(model, image)\n",
    "\n",
    "#banana_lit =activations.get('block_2_expand_BN_9/cond/Merge:0')\n",
    "#print(activations.keys()) #prints keys\n",
    "\n",
    "#Just a note this function takes a very long time to output all the plots.\n",
    "#I recommend running it only for a little while to see the first few outputs\n",
    "keract.display_activations(activations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another cool thing we can look at using keract is a heatmap. The heatmap output shows us where the layers are showing the strongest activations. This lets us see what it's paying attention to in each image, on each layer. Visualising this can help us computer scientists decode what the network neurons have learned about each particular class throughout each layer. It can also help us compare how an ANN sees objects vs humans which is interesting in it's own right!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keract import display_heatmaps\n",
    "\n",
    "#Just a note this function takes a very long time to output all the plots.\n",
    "#I recommend running it only for a little while to see the first few outputs\n",
    "display_heatmaps(activations, arr_image, save=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thought Exercise\n",
    "\n",
    "Is there anything in the feature filters that you find interesting?\n",
    "In your opinion does the network pick out similar features to your own brain?\n",
    "Why do you think we need to have so many filters and representations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 7: Applications of Deep Computer Vision\n",
    "\n",
    "Deep computer vision applications are almost endless. With the rate the technology is advancing hundreds of new applications are being discovered, turned into companies and incorporated into business processes. This section will touch on a few of the big categories in computer vision and what it might mean for the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fundamental Problems\n",
    "\n",
    "### Image Classification\n",
    "\n",
    "[Image classification](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks) is the the problem of learning to discriminate between images. We teach a network to label each image with a single class. We did this before with the imagenet classifier and the banana.\n",
    "\n",
    "### Image Captioning\n",
    "\n",
    "[Image captioning](https://arxiv.org/pdf/1810.04020.pdf) is the problem of trying to explain what's happening in an image with natural language.\n",
    "\n",
    "<img src=\"img/captioning.png\" width=\"600px\">\n",
    "\n",
    "\n",
    "### Object Detection\n",
    "\n",
    "[Object detection](https://arxiv.org/abs/1506.01497) is the problem of trying to locate all the objects of our classes in an image and put a bounding rectangle around them.\n",
    "\n",
    "<img src=\"img/detection.jpg\" width=\"600px\">\n",
    "\n",
    "\n",
    "### Segmentation\n",
    "\n",
    "[Segmentation](https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf) is where we locate all the objects of our classes then assign relevant pixels in the image a class. [Semantic instance segmentation](https://arxiv.org/abs/1703.06870) can also differentiate between instances of the same class!\n",
    "\n",
    "<img src=\"img/segmentation.png\" width=\"600px\">\n",
    "\n",
    "Panoptic segmentation is a special case that combines both normal segmentation and instance segmentation to ensure every single pixel is labeled.\n",
    "\n",
    "<img src=\"img/panoptic.jpg\" width=\"600px\">\n",
    "\n",
    "### Pose Estimation\n",
    "\n",
    "Pose estimation is the broad task of predicting an objects 3D pose (that is its location and orientation). Typically when we refer to pose estimation we are talking about human pose estimation that can be [2D](https://arxiv.org/abs/1611.08050), [3D](https://arxiv.org/abs/1903.10153), [Dense](https://arxiv.org/abs/1802.00434) and a few other variations. The other side of pose estimation is for [general object 6D pose](https://arxiv.org/abs/1711.00199v3) (x,y,z,pitch,roll,yaw) \n",
    "\n",
    "<img src=\"img/densepose.jpg\" width=\"600px\">\n",
    "\n",
    "<img src=\"img/6dpose.png\" width=\"600px\">\n",
    "\n",
    "\n",
    "## Real World Implementations\n",
    "\n",
    "### Cancer detection\n",
    "\n",
    "<img src=\"img/cancer.png\" width=\"600px\">\n",
    "\n",
    "### Agricultural Monitoring\n",
    "\n",
    "<img src=\"img/hyperspectral.jpg\" width=\"600px\">\n",
    "\n",
    "\n",
    "### Surveillance\n",
    "<img src=\"img/cctv.png\" width=\"600px\">\n",
    "\n",
    "<img src=\"img/cctvcaught.png\" width=\"600px\">\n",
    "\n",
    "\n",
    "### Robotic Vision \n",
    "\n",
    "Self driving cars\n",
    "\n",
    "<img src=\"img/sdc.jpg\" width=\"600px\">\n",
    "\n",
    "Humanoid Robots\n",
    "\n",
    "[Watch the boston dynamics video with Atlas](https://www.youtube.com/watch?v=LikxFZZO2sk)\n",
    "\n",
    "if that doesn't scare you maybe [petman will](https://www.youtube.com/watch?v=tFrjrgBV8K0)\n",
    "\n",
    "## Exercise\n",
    "\n",
    "Can you think of any other applications of AI you have seen in the wild?\n",
    "\n",
    "Can you think of any new applications that use AI to solve a real world problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Game Playing (Optional)\n",
    "\n",
    "Recent advances in neural networks have given computers the leg up in difficult to solve games versus humans. Whilst chess and other smaller search space games have been solved by traditional algorithms many years ago, more complex games like Go, or DOTA2 have never had AI that could compete with humans.\n",
    "\n",
    "You can learn about how Deepmind (Google AI researchers) conquered Go [here](https://deepmind.com/research/alphago/) and learn about how OpenAI managed to beat top teams at Dota2 [here](https://openai.com/five/).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cheers\n",
    "Congrats on getting this far in machine learning and artificial intelligence! This is a fast moving and exciting field to be a part of. Even if you don't understand a lot technically, a little knowledge in this area may prove to be useful in the future and always makes for good conversations over food and drink!\n",
    "\n",
    "Feel free to ask me any questions about anything! \n",
    "\n",
    "[Top](#ML-Week-6---Unsupervised-Learning-and-More-Vision) | [Previous week](#ML-Week-5---Artificial-Neural-Networks) | [Start of Lesson](#Part-1:-Intro-To-Unsupervised-Learning) | [Bottom](#Cheers)\n",
    "\n",
    "* [Part 1: Intro To Unsupervised Learning](#Intro-To-Unsupervised-Learning)\n",
    "* [Part 2: K-Means Clustering](#K-Means-Clustering)\n",
    "* [Part 3: Non Linear K-Means](#Part-3:-Non-Linear-K-Means)\n",
    "* [Part 4: Autoencoders](#Part-4:-Autoencoders)\n",
    "* [Part 5: Convolutional Neural Networks (CNNs) (Optional)](#Part-5:-Convolutional-Neural-Networks-(CNNs)-(Optional))\n",
    "* [Part 6: Using a Pretrained CNN](#Part-6:-Using-a-Pretrained-CNN)\n",
    "* [Part 7: Applications of Deep Computer Vision](#Part-7:-Applications-of-Deep-Computer-Vision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
