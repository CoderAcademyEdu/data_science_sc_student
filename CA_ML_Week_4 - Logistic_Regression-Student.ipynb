{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Week 4 - Logistic Regression\n",
    "\n",
    "---\n",
    "\n",
    "[Top](#ML-Week-4---Logistic-Regression) | [Previous section](#ML-Week-3---Cross-Validation) | [Next section](#Part-0:-Quick-review) | [Bottom](#Thank-you)\n",
    "\n",
    "This notebook has the following sections:\n",
    "\n",
    "* [Part 0: Quick review!](#Part-0:-Quick-review)\n",
    "* [Part 1: Linear to Logistic Regression](#Part-1:-Linear-to-Logistic-Regression)\n",
    "* [Part 2: Assessing the model fit](#Part-2:-Assessing-the-model-fit)\n",
    "* [Part 3: Receiver Operating Curves (Optional)](#Part-3:-Receiver-Operating-Curves-(Optional))\n",
    "* [Part 4: Cross-validating logistic regression](#Part-4:-Cross-validating-logistic-regression)\n",
    "* [Part 5: Another dataset with spam detection](#Part-5:-Another-dataset-with-spam-detection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Quick review\n",
    "---\n",
    "\n",
    "[Top](#ML-Week-4---Logistic-Regression) | [Previous section](#ML-Week-4---Logistic-Regression) | [Next section](#Part-1:-Linear-to-Logistic-Regression) | [Bottom](#Thank-you)\n",
    "\n",
    "Today we're going to primarily focus on a section of the [framingham heart study](https://www.kaggle.com/amanajmera1/framingham-heart-study-dataset) dataset, from Kaggle. For those who don't know the [Framingham heart disease study](https://www.framinghamheartstudy.org/) is a study that began back in 1948 to assess common factors or characteristics that contribute to heart disease. It is a _prospective_, meaning it assessed individuals over time to see how current factors contributed to heart coronary heart disease that developed _in the future_.\n",
    "\n",
    "The dataset contains the following variables...\n",
    "\n",
    "| Column | Description |\n",
    "|--------|-------------|\n",
    "| age | The age of the patient |\n",
    "| male | 1 if the patient is male, 0 if female |\n",
    "| cigsPerDay| Average number of reported cigarettes smoked per day |\n",
    "| totChol | Total cholesterol level |\n",
    "| sysBP | Systolic blood pressure reading |\n",
    "| diaBP | Diastolic blood pressure reading |\n",
    "| BMI | Body mass index |\n",
    "| glucose | glucose level in the blood stream |\n",
    "| heartRate | Heart rate when surveyed |\n",
    "| TenYearCHD | Did someoned develop coronary heart disease (CHD) within the next 10 years? |\n",
    "\n",
    "Our goal is to use these attributes to predict their 10 year risk of CHD. Once the model is developed, we could assess which features contribute the most to the likelihood of heart disease.\n",
    "\n",
    "Let's import in our dataset and describe it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "framingham_data = pd.read_csv('data/framingham.csv')\n",
    "\n",
    "# Describe\n",
    "framingham_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using linear regression to predict coronary heart disease risk\n",
    "\n",
    "Last week we talked about predicting variables using linear regression, and cross-validating the best model using K-Fold cross validation. Let's import our models to run linear regression and visualise the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import plotting modules\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# Import linear regression and needed modules\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a similar function from last week that will run linear regression with cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_regress_w_full_cross_val(\n",
    "    data, \n",
    "    cols=['temp'],\n",
    "    max_powers=[1],\n",
    "    regression_type=None,\n",
    "    target='cnt',\n",
    "    cv=10,\n",
    "):\n",
    "    \"\"\"\n",
    "    Run linear regression with kfold cross validation to analyse error. We will be able to choose\n",
    "    the type of regression, the columns we want within our model, and the maximum power.\n",
    "    \n",
    "    :param data: <pd.DataFrame>, the data for our model\n",
    "    :param cols: list<str>, a list of columns to use in our model\n",
    "    :param max_powers: list<int>, the max power to use for a corresponding column\n",
    "    :param regression_type: <str>, either None (regular), 'lasso', or 'ridge'\n",
    "    :param target: <str>, the target variable, defaults to 'cnt' assuming the\n",
    "                   correct preprocessing\n",
    "    :param cv: <int>, the number of folds\n",
    "    \"\"\"\n",
    "    # Create necessary columns\n",
    "    model_data = data.copy()\n",
    "    \n",
    "    # Create columns dict\n",
    "    all_cols = dict()\n",
    "    all_cols[1] = cols[:]\n",
    "    \n",
    "    # Go through each column and add the correct powers needed\n",
    "    for i in range(len(cols)):\n",
    "        all_cols\n",
    "        if max_powers[i] > 1:\n",
    "            for p in range(2, max_powers[i] + 1):\n",
    "                if p not in all_cols.keys():\n",
    "                    all_cols[p] = []\n",
    "                model_data[cols[i] + '_' + str(p)] = model_data[cols[i]] ** p\n",
    "                # Append to columns list\n",
    "                all_cols[p].append(cols[i] + '_' + str(p))\n",
    "                \n",
    "    # Add on all keys\n",
    "    for i in range(2, len(all_cols.keys()) + 1):\n",
    "        all_cols[i] += all_cols[i - 1]\n",
    "                    \n",
    "    # Fit data towards each power, and calculate MSE\n",
    "    powers = []\n",
    "    mse = []\n",
    "    \n",
    "    # Get linear regression\n",
    "    if regression_type == 'lasso':\n",
    "        lr = Lasso(alpha=1.0)\n",
    "    elif regression_type == 'ridge':\n",
    "        lr = Ridge(alpha=0.1)\n",
    "    else:\n",
    "        lr = LinearRegression()\n",
    "    \n",
    "    for i in range(1, max(max_powers) + 1):\n",
    "        # Run linear regression and cross validation\n",
    "        mse += (cross_val_score(\n",
    "            lr, model_data[all_cols[i]], y=model_data[target], scoring='neg_mean_squared_error', cv=cv\n",
    "        ) * -1).tolist()\n",
    "        # Get metrics\n",
    "        powers += [i] * cv\n",
    "        \n",
    "    # Graph\n",
    "    mse_df = pd.DataFrame({'Max Power': powers, 'MSE': mse})\n",
    "    sns.pointplot(x='Max Power', y='MSE', data=mse_df, ci=68)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Like we did last week, use the inputs below to run cross-validation, and decide on a final model to predict heart disease. Reminder....\n",
    "\n",
    "* `cols` is a list of columns, for example `['age', 'cigsPerDay', 'totChol']`\n",
    "* `max_powers` is a list of maximum powers to use within the model for each columns, for example `[3, 1, 2]`\n",
    "* `regression_type` can be `None` for regular regression, `'lasso'`, or `'ridge'`\n",
    "\n",
    "The MSE will be plotted across the different models used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust columns\n",
    "cols = ['age', 'cigsPerDay', 'totChol']\n",
    "max_powers = [3, 1, 2]\n",
    "regression_type = None\n",
    "\n",
    "# Run regression\n",
    "lin_regress_w_full_cross_val(\n",
    "    framingham_data, \n",
    "    cols=cols,\n",
    "    max_powers=max_powers,\n",
    "    regression_type=regression_type,\n",
    "    target='TenYearCHD',\n",
    "    cv=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Linear to Logistic Regression\n",
    "\n",
    "---\n",
    "\n",
    "[Top](#ML-Week-4---Logistic-Regression) | [Previous section](#Part-0:-Quick-review) | [Next section](#Part-2:-Assessing-the-model-fit) | [Bottom](#Thank-you)\n",
    "\n",
    "Let's actually analyse a resulting regression. We'll use...\n",
    "\n",
    "* regular linear regression\n",
    "* complexity of order 1 on variables\n",
    "* we'll use the [`sns.regplot()`](https://seaborn.pydata.org/generated/seaborn.regplot.html) function, which automatically draws a best fit line\n",
    "\n",
    "What we'll do is we'll graph the relationship between someone's total cholesterol (within the column `glucose`) by the CHD risk (`TenYearCHD`). Let's only do this for a subset of cases for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw regplot on a couple of variables\n",
    "samp_data = framingham_data.loc[\n",
    "        (framingham_data['glucose'] > 175) & (framingham_data['glucose'] < 225), :\n",
    "    ].sample(n=10, random_state=20)\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "sns.regplot(\n",
    "    x='glucose', \n",
    "    y='TenYearCHD', \n",
    "    data=samp_data, \n",
    "    ci=None)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fit here is...not great. Our MSE was low, but that's because the distance between the point and the line happens to be a small magnitude. \n",
    "\n",
    "The issue is that `TenYearCHD` is not a continuous variable, it's a **categorical variable**. It takes on...\n",
    "\n",
    "* 0, if someone was _not_ diagnosed with CHD in the past 10 years\n",
    "* 1, if someone was diagnosed with CHD in the past 10 years\n",
    "\n",
    "### Thought exercise\n",
    "\n",
    "What would you do to take this line and make a **categorical** prediction. Is there a certain threshold, where _if_ someone's glucose is above a certain level, we would start predicting that `TenYearCHD = 1`?\n",
    "\n",
    "### Exercise\n",
    "\n",
    "The following code runs linear regression using the sample points above. Add an extra step to turn your linear regression into a classification based upon your designated threshold. Here's an example of indexing an array, based upon a threshold (`1`) using an arbitrary numpy array called `a`.\n",
    "\n",
    "```python\n",
    "a[a > 1] = 22\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run linear regression\n",
    "lr = LinearRegression()\n",
    "\n",
    "# Fit data\n",
    "lr.fit(samp_data[['glucose']], samp_data['TenYearCHD'])\n",
    "\n",
    "# Predict data\n",
    "y_pred = lr.predict(samp_data[['glucose']])\n",
    "\n",
    "# INSERT CODE HERE TO THRESHOLD THE ARRAY\n",
    "\n",
    "\n",
    "# Graph line\n",
    "plt.figure(figsize=(15, 5))\n",
    "sns.regplot(x=samp_data['glucose'], y=samp_data['TenYearCHD'], ci=None, fit_reg=None, color='blue')\n",
    "sns.regplot(x=samp_data['glucose'], y=y_pred, ci=None, fit_reg=None, color='red', marker='+')\n",
    "\n",
    "# Add code to turn y_pred into a categorical variable\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now look at the entire dataset..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw regplot on a couple of variables\n",
    "plt.figure(figsize=(15, 5))\n",
    "sns.regplot(x='glucose',y='TenYearCHD', data=framingham_data, ci=None)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not really easy to figure out where the threshold should be...is it? Imagine also if we get glucose values that are higher and lower than our current range...we're going to get predictions that are above 1 and below 0. Let's change things up a little bit. What we're going to do is the following...\n",
    "\n",
    "* Run **univariate** linear regression, using just **glucose** to predict heart disease\n",
    "* Use the resulting coefficient to transform the variable (I'll explain the transformation later)\n",
    "* Graph the new fitted curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import numpy\n",
    "import numpy as np\n",
    "\n",
    "# Add coefficients\n",
    "m = 0.01000811\n",
    "b = -2.54022054\n",
    "x = np.array(range(-100, 700))\n",
    "\n",
    "y_log = 1 / (1 + np.exp(-1 * (m*x + b))) # 1 / (1 + e^(-(mx + b)))\n",
    "\n",
    "# Graph\n",
    "plt.figure(figsize=(15, 5))\n",
    "sns.regplot(x=framingham_data['glucose'], y=framingham_data['TenYearCHD'], fit_reg=False)\n",
    "sns.regplot(x=x, y=y_log, fit_reg=False, marker='.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thought exercise\n",
    "\n",
    "* What is the maximum of the orange curve?\n",
    "* What is the minimum of the orange curve?\n",
    "* What do you think the orange curve represents?\n",
    "\n",
    "The function we graphed in orange is called a **logistic function**. It's represented by the following form:\n",
    "\n",
    "$$ f(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "For those who do not know, in mathematics, $e$ is a special mathematical constant. \n",
    "\n",
    "$$e = \\sum_{i=0}^{\\infty}\\frac{1}{n!} = 1 + \\frac{1}{1} + \\frac{1}{2 * 1} + \\frac{1}{3 * 2 * 1} + ... \\approx 2.71828$$\n",
    "\n",
    "The number was oddly discovered by Jacob Bernoulli while [studying compound interest](https://www-history.mcs.st-and.ac.uk/HistTopics/e.html).\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Graph the logistic function. The code initialises $z$ for you.\n",
    "\n",
    "**HINT:** You can use the `np.exp(z)` function to code for $e$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise z\n",
    "z = np.arange(-10, 10, 0.1)\n",
    "\n",
    "# INSERT CODE: Create f_z\n",
    "f_z = 0\n",
    "\n",
    "# INSERT CODE: Graph using sns.scatter\n",
    "sns.scatterplot(z, f_z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, this special curve (a logistic function where, $f(0) = 0.5$) is called a **sigmoid function**. Logistic functions predict a **probability** that an event is going to occur. As you can see\n",
    "\n",
    "* as the curve gets lower, $f(z) \\to 0$\n",
    "* as the curve gets higher, $f(z) \\to 1$\n",
    "\n",
    "Let's make one other alteration to the function. Reminder that in **univariate linear regression**, we tried to find an $m$ and $b$ within the equation...\n",
    "\n",
    "$ g(x) = mx + b$\n",
    "\n",
    "Let's pretend that $z = g(x)$ and\n",
    "\n",
    "$$ f(z) = \\frac{1}{1 + e^{-z}} = \\frac{1}{1 + e^{-g(x)}} = \\frac{1}{1 + e^{-(mx + b)}}$$\n",
    "\n",
    "$$ f(x) = \\frac{1}{1 + e^{-(mx + b)}}$$\n",
    "\n",
    "What we are doing is taking the **output of linear regression** and **squeezing it into a probability**. Thus, we can interpret the output of the logistic function within this example as...\n",
    "\n",
    "> What is the **likelihood that someone has developed CHD within the next 10 years**? \n",
    "\n",
    "We call the **learning algorithm** that finds the specific $m$ and $b$ and that fit the logistic curve, **logistic regression**. Here's a picture to help with this interpretation...\n",
    "\n",
    "---\n",
    "\n",
    "<img src=\"img/Linear_to_Logistic_Regression.png\" width=\"700\">\n",
    "\n",
    "---\n",
    "\n",
    "#### MATH ALERT: What is a _log odds_?\n",
    "\n",
    "For those interested, **a log odds ratio** is the logarithmic ratio of the probability someone developed CHD divided by the ratio some _did not_ develop CHD. It looks something like this...\n",
    "\n",
    "$$ log \\Bigg (\\frac{P(TenYearCHD = 1)}{P(TenYearCHD = 0)} \\Bigg ) = mx + b $$\n",
    "\n",
    "Thus, when $x$ increases by 1, what we are saying is that the _log odds increases by 1_. We can have some fun and rearrange this equation to make our logistic function.\n",
    "\n",
    "$$ \\Bigg (\\frac{P(TenYearCHD = 1)}{P(TenYearCHD = 0)} \\Bigg ) = e^{mx + b} $$\n",
    "$$ \\Bigg (\\frac{P(TenYearCHD = 1)}{1 - P(TenYearCHD = 1)} \\Bigg ) = e^{mx + b} $$\n",
    "\n",
    "and rearranging...\n",
    "\n",
    "$$ P(TenYearCHD = 1) = \\frac{e^{mx + b}}{1 + e^{mx + b}} = \\frac{1}{1 + e^{-(mx + b)}} $$\n",
    "\n",
    "\n",
    "### Classification\n",
    "\n",
    "We're missing one extra step. Sometimes, we do not want the likelihood that someone will develop a disease, we want a definitive answer..._will I get CHD in the next ten years_?. What we can then do is **threshold our likelihood by a specific probability p**. We then conclude by saying...\n",
    "\n",
    "* If $f(x) >= p$, TenYearCHD = 1,\n",
    "* If $f(x) < p$, TenYearCHD = 0\n",
    "\n",
    "This provides a final **classification** based upon our likelihood. Thus normally in machine learning, despite having the word \"regression\" in it.\n",
    "\n",
    "> A logistic regression that outputs a categorical value is considered a **classification** algorithm, since it predicts a **categorical variable** once thresholded (e.g. will someone get CHD, yes or no?). If you are using a logistic regression algorithm that outputs a likelihood instead of a final classification, it is considered a regression algorithm, since the output is continuously valued. Don't get too hung up on this.\n",
    "\n",
    "---\n",
    "\n",
    "<img src=\"img/Logistic_Regression_Threshold.png\" width=\"700\">\n",
    "\n",
    "---\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Run logistic regression using the imported `LogisticRegression` library, fitting with just the `glucose` column. You can assess the fit using the `accuracy_score` function (instead of `mean_squared_error`). Remember the four steps for running a learning algortihm...\n",
    "\n",
    "1. Create a LogisticRegression() object\n",
    "2. Fit the model using the `framingham_data['glucose']`, and `framingham_data['TenYearCHD']` columns\n",
    "3. Predict on the training data\n",
    "4. Assess the model's performance\n",
    "\n",
    "You can run these steps just like you did previously with `LinearRegression`, but replacing the `LinearRegression` object with a `LogisticRegression` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Create a LogisticRegression object\n",
    "log_ress = 0\n",
    "\n",
    "# Fit model\n",
    "\n",
    "# Predict on the input\n",
    "\n",
    "# Find the accuracy_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Assessing the model fit\n",
    "\n",
    "---\n",
    "\n",
    "[Top](#ML-Week-4---Logistic-Regression) | [Previous section](#Part-1:-Linear-to-Logistic-Regression) | [Next section](#Part-3:-Receiver-Operating-Curves-(Optional)) | [Bottom](#Thank-you)\n",
    "\n",
    "In the past example we used a metric called **accuracy** to assess the model fit...you might be asking. What is accuracy?? What other **metrics can we use to assess how well our classification model works**? Let's break this down a little bit further...\n",
    "\n",
    "### Confusion Matrics\n",
    "\n",
    "Let's say we're predicting how many people have a `TenYearCHD = 1`. We predict this for 10 people and we have the following data and predictions.\n",
    "\n",
    "```python\n",
    "\n",
    "y_true = [0, 0, 0, 0, 0, 0, 0, 0, 1, 1]\n",
    "\n",
    "y_pred = [0, 0, 1, 0, 1, 0, 0, 1, 1, 0]\n",
    "\n",
    "```\n",
    "\n",
    "We often call a `1` a **positive** score, and `0` a **negative** score.\n",
    "\n",
    "### Thought exercise\n",
    "\n",
    "* How many values were positive, and predicted as positive? These values are called **True Positives (TP)**.\n",
    "* How many values were negative, and predicted as negative? These values are called **True Negatives (TN)**.\n",
    "* How many values were negative, and predicted as positive? These values are called **False Positives (FP)**.\n",
    "* How many values were positive, and predicted as negative? These values are called **False Negatives (FN)**.\n",
    "\n",
    "As you see, there are **four possible** combinations of correct and incorrect combinations we can make between a true vector and the predicted results. We can provide some definitions on these. We can summarise these values in a **confusion matrix**, which tells us how well a classification model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define vectors\n",
    "y_true = [0, 0, 0, 0, 0, 0, 0, 0, 1, 1]\n",
    "y_pred = [0, 0, 1, 0, 1, 0, 0, 1, 1, 1]\n",
    "\n",
    "# Import confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Make confusion matrix\n",
    "conf = confusion_matrix(y_true, y_pred, labels=[1, 0])\n",
    "sns.heatmap(conf, annot=True, annot_kws={'size': 16})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's breakdown the general version of this confusion matrix.\n",
    "\n",
    "<img src=\"img/Confusion_Matrix.png\" width=\"500\">\n",
    "\n",
    "<br>\n",
    "\n",
    "Ideally, we want to get the **<span style=\"color:green;\">green</span>** areas of the matrix (the diaganol) as high as possible, and the **<span style=\"color:red;\">red</span>** areas of the matrix as low as possible. Let's review some metrics we can calculate using this matrix.\n",
    "\n",
    "### Exercise\n",
    "\n",
    "There are metrics we can calculate using a confusion matrix to **assess the efficacy of the model**. Our confusion matrix is currently stored in a variable called `conf`. Let's define each metric, and then we'll run a quick exercise to calculate each metric.\n",
    "\n",
    "**Reminder:** To index a matrix, we can use the following notation: `conf[row_ind, col_ind]`. For example, `conf[0, 1]` would output the number `0`.\n",
    "\n",
    "#### Accuracy\n",
    "\n",
    "Accuracy is the total amount of correct predictions, divided by the total amount of predictions, in other words it is...\n",
    "\n",
    "$$ Accuracy = \\frac{TP + TN}{TP + TN + FP + FN} $$\n",
    "\n",
    "Calculate the accuracy in the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracay\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision\n",
    "\n",
    "Precision is the correct positive predictions divided by the total overall number of predicted positive predictions.\n",
    "\n",
    "$$ Precision = \\frac{TP}{TP + FP} $$\n",
    "\n",
    "Calculate the precision in the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate precision\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recall\n",
    "\n",
    "Recall is the correct positive predictions divided by the total overall number of actual positive predictions.\n",
    "\n",
    "$$ Recall = \\frac{TP}{TP + FN} $$\n",
    "\n",
    "Calculate the recall in the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate recall\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I forget these terms a lot. But here's something from [stack overflow](https://stats.stackexchange.com/questions/122225/what-is-the-best-way-to-remember-the-difference-between-sensitivity-specificity/122228) that is helpful for remembering.\n",
    "\n",
    "* <strong>P</strong>recision: TP / <strong>P</strong>redicted positive\n",
    "* <strong>R</strong>ecall: TP / <strong>R</strong>eal positive\n",
    "\n",
    "Sklearn has all of these metrics ready for you that you can import. Let's do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using different scores\n",
    "\n",
    "\n",
    "#### Accuracy issues\n",
    "\n",
    "Let's assess the accuracy, precision, and recall of our own logistic regression model on the actual Framingham data. Let's be proper and use K-Fold cross validation with the [`cross_val_score`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html#sklearn.model_selection.cross_val_score)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise model\n",
    "log_ress = LogisticRegression(solver='lbfgs')\n",
    "\n",
    "# Cross val with accuracy\n",
    "acc = cross_val_score(\n",
    "    estimator=log_ress, \n",
    "    X=framingham_data[['glucose']], \n",
    "    y=framingham_data['TenYearCHD'], \n",
    "    scoring='accuracy',\n",
    "    cv=10\n",
    ")\n",
    "print('Mean +/- std of accuracy: %.2f +/- %.2f' % (np.mean(acc), np.std(acc)))\n",
    "\n",
    "# Cross val with precision\n",
    "prec = cross_val_score(\n",
    "    estimator=log_ress, \n",
    "    X=framingham_data[['glucose']], \n",
    "    y=framingham_data['TenYearCHD'], \n",
    "    scoring='precision',\n",
    "    cv=10\n",
    ")\n",
    "print('Mean +/- std of precision: %.2f +/- %.2f' % (np.mean(prec), np.std(prec)))\n",
    "\n",
    "# Cross val with recall\n",
    "rec = cross_val_score(\n",
    "    estimator=log_ress, \n",
    "    X=framingham_data[['glucose']], \n",
    "    y=framingham_data['TenYearCHD'], \n",
    "    scoring='recall',\n",
    "    cv=10\n",
    ")\n",
    "print('Mean +/- std of recall: %.2f +/- %.2f' % (np.mean(rec), np.std(rec)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thought exercise\n",
    "\n",
    "Why is accuracy _so much higher_ than precision and recall?\n",
    "\n",
    "The reason is that the classes in our model are _imbalanced_, meaning that there are many more people with `TenYearCHD=1`, then `TenYearCHD=0`. Let's use the `series.value_counts()` to print out the percentage of people with specific values for `TenYearCHD` metric in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print value counts\n",
    "framingham_data.TenYearCHD.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine if we classified **every single oberservation in our model** as 0? We would get ~85% accuracy...and our classifier isn't doing very much. So though classifier will looks \"good\" since the accuracy is high, it's not really affective.\n",
    "\n",
    "> A **general rule** is that when you are trying to predict a variable with **imbalanced classes**, it's better to use **precision and recall** as evaluation metrics for a model.\n",
    "\n",
    "Other thoughts...if our model is mainly predicting all 0 values...\n",
    "\n",
    "* Why is **precision ~0.5?** (Think about low TP, but low FP)\n",
    "* Why is **recall ~0.0?** (Think about 0 TP, and 0 FN)\n",
    "\n",
    "#### Precision or recall?\n",
    "\n",
    "So, should we use precision or recall? It really depends on whether you care more about the FP rate, or the FN on being low...which is often context dependent. Here are a few examples.\n",
    "\n",
    "### Thought exercise\n",
    "\n",
    "* If you were developing a classifier for disease diagnosis, and treatment was inexpensive and non-dangerous, what are the trade-offs between a high FP or FN? How about a more circumstantial disease?\n",
    "* If you were analysing the potential of a person being innocent or guilty, what are the trade-offs between high FP or FN?\n",
    "\n",
    "If your answer is...I'd rather have both be reasonably low, then there is a metric for you, called a **F1-score**. The F1-score is a type of average between the precision and recall. Mathematically it looks a little funky...\n",
    "\n",
    "$$ F1 = \\frac{2}{\\frac{1}{precision} + \\frac{1}{recall}} = \\frac{2* precision * recall}{precision + recall} $$\n",
    "\n",
    "Let's calculate F1 using our cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross val with recall\n",
    "f1 = cross_val_score(\n",
    "    estimator=log_ress, \n",
    "    X=framingham_data[['glucose']], \n",
    "    y=framingham_data['TenYearCHD'], \n",
    "    scoring='f1',\n",
    "    cv=10\n",
    ")\n",
    "print('Mean +/- std of f1: %.2f +/- %.2f' % (np.mean(f1), np.std(f1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Receiver Operating Curves (Optional)\n",
    "\n",
    "---\n",
    "\n",
    "[Top](#ML-Week-4---Logistic-Regression) | [Previous section](#Part-2:-Assessing-the-model-fit) | [Next section](#Part-4:-Cross-validating-logistic-regression) | [Bottom](#Thank-you)\n",
    "\n",
    "Remember that technically logistic regression does not output a classification, it outputs the **probability that an example is the positive class**. By default, sklearn says, if `probability >= 0.5, predict a True value`. We can use the `predict_proba` method to get the probability, and **choose our own threshold**.\n",
    "\n",
    "Let's define a function that runs logistic regression over a specific train/test set. We'll include more variables than **count**. The function also graphs two metrics on an x and y axis...\n",
    "\n",
    "* y-axis: the TP rate, which is $recall = \\frac{TP}{TP + FN}$\n",
    "* x-axis: the FP rate, which is $FPR = \\frac{FP}{FP + TN}$\n",
    "\n",
    "Note how _all_ the sections of the confusion matrix are used within these two metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_w_thresh(\n",
    "    data,\n",
    "    cols=['glucose'],\n",
    "    target='TenYearCHD',\n",
    "    thresholds=[0.5]\n",
    "):\n",
    "    \"\"\"\n",
    "    Run logistic regression for a set of columns and graph ROC curve\n",
    "    \n",
    "    :param data: pd.DataFrame, the data\n",
    "    :param cols: list<str>, the list of columns to predict\n",
    "    :param target: str, the target column to predict\n",
    "    :param thresholds: list<float>, list of thresholds to check against\n",
    "    \"\"\"\n",
    "    # Get train/test\n",
    "    train, test = train_test_split(data, random_state=42)\n",
    "    \n",
    "    # Get data\n",
    "    log_ress = LogisticRegression(solver='lbfgs')\n",
    "    \n",
    "    # Get train/test\n",
    "    log_ress.fit(train[cols], train[target])\n",
    "    \n",
    "    # Generate predictions across tresholds\n",
    "    probs = log_ress.predict_proba(test[cols])[:, 1]\n",
    "    \n",
    "    # Tresholds\n",
    "    fp_rate = []\n",
    "    tp_rate = []\n",
    "    \n",
    "    # Calculate\n",
    "    for t in thresholds:\n",
    "        y_pred = np.copy(probs)\n",
    "        y_pred[y_pred >= t] = 1\n",
    "        y_pred[y_pred < t] = 0\n",
    "        conf = confusion_matrix(test[target], y_pred, labels=[1, 0])\n",
    "        tp_rate.append(conf[0, 0] / (conf[0, 0] + conf[0, 1]))\n",
    "        fp_rate.append(conf[1, 0] / (conf[1, 0] + conf[1, 1]))\n",
    "        \n",
    "    # Now graph\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.plot(fp_rate, tp_rate)\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.xlim([0.0, 1.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Add thresholds (between 0 and 1) to the curve below. Think about...\n",
    "\n",
    "* What would this curve look like if we guessed completely random between classes?\n",
    "* If our guesses were perfect, what would the resulting curve look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define thresholds (between 0 and 1)\n",
    "thresh = [0, 0.5, 1.0]\n",
    "thresh = np.arange(0, 1.1, 0.05)\n",
    "cols = ['glucose']\n",
    "\n",
    "# Run function\n",
    "logistic_regression_w_thresh(\n",
    "    framingham_data,\n",
    "    cols=cols,\n",
    "    target='TenYearCHD',\n",
    "    thresholds=thresh\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This type of curve has a special name, called a **receiver operating curve**, or **ROC** curve. It tells us how capable our model is of distinguishing between classes, and it is a robust metric as it covers how our model would respond over different thresholds. Let's look through a few examples of ROC curves with simpler data. The following function will help us input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the roc curve metric\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "# Create roc_curve function\n",
    "def create_roc_curve(y_true, y_prob):\n",
    "    \"\"\"\n",
    "    Plot an ROC curve given a vector of probabilities and a true set of classification labels.\n",
    "    \n",
    "    :param y_true: np.array<float>, the true values\n",
    "    :param y_prob: np.array<float>, the probability values\n",
    "    :parma title: str, the title of the curve\n",
    "    \"\"\"\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_prob)\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.0])\n",
    "    plt.xlabel('False positive rate')\n",
    "    plt.ylabel('True positive rate')\n",
    "    plt.title('ROC AUC: %.2f' % roc_auc_score(y_true, y_prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll plot three examples below, each time initialising a _probability_ vector, and a vector of true classification examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# 0.5 area\n",
    "plt.subplot(1, 3, 1)\n",
    "y_true = [0, 0, 0, 0, 1, 1, 1, 1]\n",
    "y_prob = [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]\n",
    "create_roc_curve(y_true, y_prob)\n",
    "\n",
    "# 0.88 area\n",
    "plt.subplot(1, 3, 2)\n",
    "y_true = [0, 0, 0, 0, 1, 1, 1, 1]\n",
    "y_prob = [0.1, 0.1, 0.7, 0.7, 0.7, 0.7, 0.9, 0.9]\n",
    "create_roc_curve(y_true, y_prob)\n",
    "\n",
    "# 1.0 area\n",
    "plt.subplot(1, 3, 3)\n",
    "y_true = [0, 0, 0, 0, 1, 1, 1, 1]\n",
    "y_prob = [0, 0, 0, 0, 1, 1, 1, 1]\n",
    "create_roc_curve(y_true, y_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's breakdown what these pictures mean.\n",
    "\n",
    "<img src=\"img/ROC_Curve.png\" width=\"800\">\n",
    "\n",
    "You'll notice that from left-to-right, the prediction curves get better. One thing I haven't pointed out is the title, which has the words **AUC** or **Area Under the Curve**. The `AUC` will range between 0 and 1, and measures **how good our model performs**. A model with AUC = 1.0 performs well among any threshold.\n",
    "\n",
    "![](https://media.springernature.com/original/springer-static/image/art%3A10.1007%2Fs10115-017-1022-8/MediaObjects/10115_2017_1022_Fig1_HTML.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Cross-validating logistic regression\n",
    "\n",
    "---\n",
    "\n",
    "[Top](#ML-Week-4---Logistic-Regression) | [Previous section](#Part-3:-Receiver-Operating-Curves-(Optional)) | [Next section](#Part-5:-Another-dataset-with-spam-detection) | [Bottom](#Thank-you)\n",
    "\n",
    "Let's talk about the hyperparameters on the [logistic regression model](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) in sklearn that can be fine-tuned.\n",
    "\n",
    "* The features. We could fine tune what columns we use for the model. Thus far, we've only been using `glucose`\n",
    "* `penalty`: whether to use `l1` or `l2` regularisation for the model\n",
    "* `class_weight`: A **reweighting for classes**. This is **extremely useful for unbalanced classes**. The input is a dictionary. For example, if 75% of our observations have people with `TenYearCHD = 0`, and 25% of the observations have `TenYearCHD = 1`, we could use `class_weight='balanced'` so that the algorithm weights the data to balance the classes to be equal in the optimisation\n",
    "* `solver`, which has multiple values that are work well for specific datasets. Different solvers are good for different sizes of data, speed, etc.\n",
    "\n",
    "There are a ton of other hyperparameters that you can checkout on the documentation [link](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html).\n",
    "\n",
    "Let's make a function that will cross-validate using K-Fold cross-validation and an inputted set of parameters. It will also graph the results on an ROC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import auc\n",
    "from scipy import interp\n",
    "\n",
    "def logistic_regression_w_cross_val(\n",
    "    data,\n",
    "    cols=['glucose'],\n",
    "    target='TenYearCHD',\n",
    "    penalty='l2',\n",
    "    class_weight='balanced',\n",
    "    solver='lbfgs'\n",
    "\n",
    "):\n",
    "    \"\"\"\n",
    "    Run logistic regression for a set of columns and graph ROC curve\n",
    "    \n",
    "    :param data: pd.DataFrame, the data\n",
    "    :param cols: list<str>, the list of columns to predict\n",
    "    :param target: str, the target column to predict\n",
    "    :param penalty: str, the penalty for the model\n",
    "    :param class_weight: str or dict<int:float>, the balancing scheme for the training\n",
    "    :param solver: str, the type of optimisation to use in the model\n",
    "    \n",
    "    :return log_ress_final: fully trained final model\n",
    "    :return coef: the model coefficients\n",
    "    \"\"\"\n",
    "    # Create splits\n",
    "    cv = KFold(n_splits=10)\n",
    "    classifier = LogisticRegression(\n",
    "        penalty=penalty, \n",
    "        class_weight=class_weight, \n",
    "        solver=solver,\n",
    "        max_iter=1000\n",
    "    )\n",
    "    \n",
    "    # Run MinMaxScalar for model interpretability\n",
    "    X = pd.DataFrame(MinMaxScaler().fit_transform(data[cols]), columns=cols)\n",
    "\n",
    "    tprs = []\n",
    "    aucs = []\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    f1s = []\n",
    "\n",
    "    i = 0\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Classify on different splits and create curves\n",
    "    for train, test in cv.split(X[cols], data[target]):\n",
    "        # Run classifier\n",
    "        probas_ = classifier.fit(\n",
    "            X.loc[train, cols], data.loc[train, target]\n",
    "        ).predict_proba(X.loc[test, cols])\n",
    "        # Compute ROC curve and area the curve\n",
    "        fpr, tpr, thresholds = roc_curve(data.loc[test, target], probas_[:, 1])\n",
    "        tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "        tprs[-1][0] = 0.0\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        aucs.append(roc_auc)\n",
    "        # Plot the specific line with label\n",
    "        plt.plot(fpr, tpr, lw=1, alpha=0.3,\n",
    "                 label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc))\n",
    "\n",
    "        i += 1\n",
    "    # Plot AUC = 0.5 line\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
    "             label='Chance', alpha=.8)\n",
    "\n",
    "    # Plot mean\n",
    "    mean_tpr = np.mean(tprs, axis=0)\n",
    "    mean_tpr[-1] = 1.0\n",
    "    mean_auc = auc(mean_fpr, mean_tpr)\n",
    "    std_auc = np.std(aucs)\n",
    "    plt.plot(mean_fpr, mean_tpr, color='b',\n",
    "             label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "             lw=2, alpha=.8)\n",
    "\n",
    "    std_tpr = np.std(tprs, axis=0)\n",
    "    tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "    tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "    plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
    "                     label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC: Framingham Heart Disease Study')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Run final model on all data\n",
    "    classifier.fit(X[cols], data[target])\n",
    "    coef = pd.DataFrame({\n",
    "        'Columns': cols + ['Intercept'], \n",
    "        'Coefficients': classifier.coef_[0].tolist() + [classifier.intercept_[0]]\n",
    "    })\n",
    "\n",
    "    return classifier, coef[['Columns', 'Coefficients']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "The following code will run logistic regression and allow you to input different parameters on our dataset. It will grade the resulting fit using the `F1 score` and `ROC AUC`, graph the `ROC` curves, and then output the `feature importance`. The feature importance are the model weight coefficients, and like linear regression, the more impactful features have a higher magnitude weight.\n",
    "\n",
    "Try to tune the following...\n",
    "\n",
    "* Change the `penalty` variable between `'l1'` and `'l2'`\n",
    "* Change the `class_weight` between `None` and `'balanced'`\n",
    "* Change the `'solver'` between `'liblinear'` and `'lbfgs'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT PARAMETERS\n",
    "cols = ['age', 'male', 'cigsPerDay', 'totChol', 'sysBP', 'diaBP', 'BMI', 'glucose', 'heartRate']\n",
    "penalty = 'l2'\n",
    "class_weight = 'balanced'\n",
    "solver = 'lbfgs'\n",
    "\n",
    "# Run logistic regression\n",
    "model, coef = logistic_regression_w_cross_val(\n",
    "    framingham_data,\n",
    "    cols=cols,\n",
    "    target='TenYearCHD',\n",
    "    penalty=penalty,\n",
    "    class_weight=class_weight,\n",
    "    solver=solver\n",
    "\n",
    ")\n",
    "\n",
    "print(coef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it's possible that our model is much more _nonlinear_ than we think. Logistic Regression, though not linear regression, is part of a family of models called [**generalised linear models**](https://medium.com/@yongddeng/regression-analysis-generalised-linear-model-2f03c7e4cecb). We'll come back to this the next lesson.\n",
    "\n",
    "![](https://sebastianraschka.com/images/blog/2014/kernel_pca/linear_vs_nonlinear.png)\n",
    "\n",
    "## Part 5: Another dataset with spam detection\n",
    "\n",
    "---\n",
    "\n",
    "[Top](#ML-Week-4---Logistic-Regression) | [Previous section](#Part-4:-Cross-validating-logistic-regression) | [Next section](#Thank-you) | [Bottom](#Thank-you)\n",
    "\n",
    "Let's take a look at another dataset for fun. We'll use a [spam detection](https://www.kaggle.com/uciml/sms-spam-collection-dataset/data) dataset that composes SMS's that were labeled as spam vs. not spam. We need to do some transformations of our data before using logistic regression, as computers are not very great at reading text messages.\n",
    "\n",
    "**Natural langauges processing** is a complex field that works with translating human language into a form that computers can read, comprehend and make decisions off of.\n",
    "\n",
    "Let's load-up our dataset, and compute transformations using something called a [Term frequency inverse document frequency (or TFIDF) Vectoriser](https://medium.freecodecamp.org/how-to-process-textual-data-using-tf-idf-in-python-cd2bbc0a94a3). We've also already processed the data using the [nltk](https://www.nltk.org/) library, by doing things like...\n",
    "\n",
    "* Removing punctuation\n",
    "* Removing commonly used _stop words_, like \"the\", \"a\", etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload spam dataset\n",
    "spam_data = pd.read_csv('data/spam.csv')\n",
    "spam_data.dropna(axis=0, inplace=True)\n",
    "\n",
    "# Run TFIDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "message_mat = vectorizer.fit_transform(spam_data['Text_Clean'])\n",
    "category = spam_data[['Class']]\n",
    "new_mat = pd.concat(\n",
    "    (pd.DataFrame(message_mat.todense(), columns=['V' + str(i) for i in range(message_mat.shape[1])]), category), \n",
    "    axis=1\n",
    ")\n",
    "new_mat.dropna(axis=0, inplace=True)\n",
    "new_mat.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# Print data head\n",
    "spam_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_data['Class'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now train the classifier using our function from before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# INPUT PARAMETERS\n",
    "cols=['V' + str(i) for i in range(message_mat.shape[1])]\n",
    "penalty = 'l2'\n",
    "class_weight = 'balanced'\n",
    "solver = 'lbfgs'\n",
    "\n",
    "# Run logistic regression\n",
    "model, coef = logistic_regression_w_cross_val(\n",
    "    new_mat,\n",
    "    cols=cols,\n",
    "    target='Class',\n",
    "    penalty=penalty,\n",
    "    class_weight=class_weight,\n",
    "    solver=solver\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge...until next time\n",
    "\n",
    "Can you improve the spam model? Take a look at what [other people have done on Kaggle](https://www.kaggle.com/uciml/sms-spam-collection-dataset/kernels) and post your results in slack if you improve the ROC AUC! It might be helpful to do some **exploratory data analysis** and find out...\n",
    "\n",
    "* What words are mostly in spam/not spam emails?\n",
    "* Is TFIDF the best feature creating process?\n",
    "* Are there other algorithms that can be used? Here are a few we won't cover in the course that might be fun to checkout. The syntax for creating and predicting models are the same, but the hyperparameter set will be slightly different.\n",
    "    * [Naive Bayes](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB). This is used often for simple spam problems.\n",
    "    * [RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n",
    "    * [SupportVectorMachine](https://scikit-learn.org/stable/modules/svm.html)\n",
    "    * [MultilayerPerceptron](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier). We will cover this more next lesson, but with a different library!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thank you\n",
    "\n",
    "[Top](#ML-Week-4---Logistic-Regression) | [Previous section](#Part-5:-Another-dataset-with-spam-detection) | [Next section](#Thank-you) | [Bottom](#Thank-you)\n",
    "\n",
    "That concludes our week 4 lesson. Hopefully you enjoyed :)\n",
    "\n",
    "### Downloading the notebook\n",
    "\n",
    "If you would like to retain your work, please follow the following directions:\n",
    "* On the top of this screen, in the header menu, click \"File\", then \"Download .ipynb\".\n",
    "* You will need to download [Python 3.7 with Anaconda](https://www.anaconda.com/distribution/#download-section) to use this in the future"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
